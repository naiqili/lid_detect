{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data.dataset import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0cc292bc30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Split: train\n",
      "    Root Location: ./data/\n",
      "    Transforms (if any): Compose(\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=(0.1307,), std=(0.3081,))\n",
      "                         )\n",
      "    Target Transforms (if any): None\n"
     ]
    }
   ],
   "source": [
    "ds_train10 = torchvision.datasets.MNIST('./data/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]));\n",
    "print(ds_train10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(len(ds_train10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54051\n"
     ]
    }
   ],
   "source": [
    "ln = len(ds_train10);\n",
    "train_y = np.zeros(ln);\n",
    "for idx, (data, target) in enumerate(ds_train10):\n",
    "    train_y[idx] = target.numpy()\n",
    "\n",
    "idx = np.where(train_y <= 8)[0]\n",
    "ds_train9 = Subset(ds_train10, idx)\n",
    "print(len(ds_train9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48200\n"
     ]
    }
   ],
   "source": [
    "idx = np.where(train_y <= 7)[0]\n",
    "ds_train8 = Subset(ds_train10, idx)\n",
    "print(len(ds_train8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test10 = torchvision.datasets.MNIST('./data/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(ds_test10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8991\n"
     ]
    }
   ],
   "source": [
    "ln = len(ds_test10);\n",
    "test_y = np.zeros(ln);\n",
    "for idx, (data, target) in enumerate(ds_test10):\n",
    "    test_y[idx] = target.numpy()\n",
    "\n",
    "idx = np.where(test_y <= 8)[0]\n",
    "ds_test9 = Subset(ds_test10, idx)\n",
    "print(len(ds_test9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8017\n"
     ]
    }
   ],
   "source": [
    "idx = np.where(test_y <= 7)[0]\n",
    "ds_test8 = Subset(ds_test10, idx)\n",
    "print(len(ds_test8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader10 = torch.utils.data.DataLoader(\n",
    "    ds_train10,\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader10 = torch.utils.data.DataLoader(\n",
    "    ds_test10,\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 49 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples = enumerate(test_loader10)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "example_data.shape\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(50):\n",
    "    plt.subplot(5,10,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"{}\".format(example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader8 = torch.utils.data.DataLoader(\n",
    "    ds_train8,\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader8 = torch.utils.data.DataLoader(\n",
    "    ds_test8,\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 49 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples = enumerate(test_loader8)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(example_data.shape)\n",
    "\n",
    "example_data.shape\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(50):\n",
    "    plt.subplot(5,10,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"{}\".format(example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30596\n"
     ]
    }
   ],
   "source": [
    "idx = np.where(train_y <= 4)[0]\n",
    "ds_train5 = Subset(ds_train10, idx)\n",
    "print(len(ds_train5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5139\n"
     ]
    }
   ],
   "source": [
    "idx = np.where(test_y <= 4)[0]\n",
    "ds_test5 = Subset(ds_test10, idx)\n",
    "print(len(ds_test5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader5 = torch.utils.data.DataLoader(\n",
    "    ds_train5,\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader5 = torch.utils.data.DataLoader(\n",
    "    ds_test5,\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12665\n"
     ]
    }
   ],
   "source": [
    "idx = np.where(train_y <= 1)[0]\n",
    "ds_train2 = Subset(ds_train10, idx)\n",
    "print(len(ds_train2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2115\n"
     ]
    }
   ],
   "source": [
    "idx = np.where(test_y <= 1)[0]\n",
    "ds_test2 = Subset(ds_test10, idx)\n",
    "print(len(ds_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader2 = torch.utils.data.DataLoader(\n",
    "    ds_train2,\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader2 = torch.utils.data.DataLoader(\n",
    "    ds_test2,\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network using 0-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        self.last = x\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, train_loader, epoch):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()            \n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append((batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            torch.save(network.state_dict(), './results/model.pth')\n",
    "            torch.save(optimizer.state_dict(), './results/optimizer.pth')\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(network, test_loader):\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()            \n",
    "            output = network(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1 = Net().cuda()\n",
    "optimizer = optim.SGD(net1.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader10.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naiqi/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/naiqi/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3111, Accuracy: 956/10000 (9%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.296428\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.275135\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.290798\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.282064\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.258089\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.213377\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.210812\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.159625\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.076725\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.050290\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.919861\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.947154\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.758680\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.462732\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.574901\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.556828\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.358212\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.291479\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.280410\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.277229\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.308324\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.015913\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.025623\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 1.074849\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.741426\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.801734\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.763562\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.916458\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.944048\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.814832\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.818485\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.978244\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.760019\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.577024\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.723613\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.797211\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.686379\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.661267\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.644604\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.647441\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.648140\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.662356\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.589510\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.551239\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.830040\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.618606\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.534584\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.682513\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.686321\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.583150\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.912355\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.542294\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.393084\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.395951\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.600859\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.605380\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.439086\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.464384\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.795138\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.467232\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.379787\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.379007\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.508491\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.447111\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.521136\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.695111\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.476410\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.556339\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.517720\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.527631\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.339316\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.516312\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.560687\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.337115\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.443773\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.508009\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.650681\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.752692\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.691802\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.592376\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.506953\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.463697\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.400113\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.419800\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.371261\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.391785\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.636782\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.514687\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.326159\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.285463\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.414964\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.355005\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.439178\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.273592\n",
      "\n",
      "Test set: Avg. loss: 0.1765, Accuracy: 9472/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.520088\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.253862\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.267326\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.352786\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.453673\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.486581\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.485173\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.318915\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.366181\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.529433\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.423549\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.601731\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.344543\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.388404\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.456412\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.412187\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.619169\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.513397\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.250908\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.392966\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.243193\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.246909\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.455916\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.208360\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.334172\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.319938\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.236887\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.238457\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.501826\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.382048\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.498156\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.433150\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.288271\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.356007\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.583433\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.331204\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.229191\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.402938\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.326276\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.367282\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.415168\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.422994\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.406749\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.420138\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.213106\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.516101\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.298303\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.354013\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.198423\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.387806\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.319410\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.448150\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.414148\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.301019\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.320064\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.289879\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.417323\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.376329\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.226505\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.376860\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.198500\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.346880\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.312764\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.431920\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.233393\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.339553\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.331647\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.289751\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.431618\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.350002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.278168\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.439769\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.217291\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.258742\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.265625\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.213628\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.313699\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.251113\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.186378\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.455516\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.274573\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.302203\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.222525\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.245448\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.493234\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.495537\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.373417\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.215059\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.200566\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.423534\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.332534\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.269204\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.309091\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.192906\n",
      "\n",
      "Test set: Avg. loss: 0.1161, Accuracy: 9642/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.314261\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.324782\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.355188\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.246325\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.263608\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.276694\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.350033\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.416056\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.209097\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.165673\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.250029\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.404920\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.227504\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.361764\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.426461\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.313792\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.247018\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.239493\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.408884\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.640995\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.076577\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.247619\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.267913\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.418921\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.147289\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.289240\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.245274\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.285021\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.362075\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.277580\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.239292\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.229244\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.349333\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.410480\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.305458\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.179228\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.205975\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.365922\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.316004\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.267440\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.264206\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.228817\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.505664\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.211607\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.327224\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.267466\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.236977\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.292757\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.335345\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.115842\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.239779\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.161688\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.340200\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.300434\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.186593\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.347181\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.324403\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.518653\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.151185\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.224485\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.387928\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.262151\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.130385\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.133013\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.334191\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.157135\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.193581\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.336871\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.193506\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.246527\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.443405\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.304672\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.327669\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.303423\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.178482\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.228087\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.102686\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.334732\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.206721\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.215812\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.219619\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.344315\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.392475\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.312541\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.279461\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.412644\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.272314\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.279230\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.149919\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.275839\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.358075\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.369055\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.339248\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.165161\n",
      "\n",
      "Test set: Avg. loss: 0.0902, Accuracy: 9710/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.339799\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.110301\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.173920\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.208128\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.170454\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.543395\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.130080\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.207226\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.278699\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.158952\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.250982\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.377914\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.466713\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.224756\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.378635\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.193292\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.156525\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.246255\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.556860\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.416481\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.286166\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.400160\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.288908\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.111409\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.225853\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.293735\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.247169\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.189516\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.122194\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.109866\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.310687\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.238842\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.619609\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.334187\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.314447\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.289722\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.401514\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.361206\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.398268\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.243558\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.146588\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.315016\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.225920\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.286327\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.269631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.110679\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.380068\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.149398\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.274498\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.291689\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.339421\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.226408\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.271932\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.328961\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.332856\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.337463\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.212005\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.345667\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.058675\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.305580\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.159506\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.321072\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.261939\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.378035\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.208678\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.431637\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.199144\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.289297\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.233961\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.159530\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.358694\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.094580\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.203900\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.298008\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.222242\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.248070\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.355300\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.317884\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.228633\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.184910\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.252473\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.078049\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.259585\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.222341\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.279097\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.339799\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.115093\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.174757\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.160893\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.296402\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.333116\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.082894\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.359358\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.228086\n",
      "\n",
      "Test set: Avg. loss: 0.0749, Accuracy: 9760/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.155105\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.084926\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.346387\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.109063\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.130008\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.108213\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.236642\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.360335\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.380174\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.258226\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.096693\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.257113\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.296871\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.181286\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.384628\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.217443\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.110507\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.192498\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.208078\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.098305\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.214016\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.222483\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.307751\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.173320\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.164605\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.148095\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.186393\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.247289\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.238618\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.232561\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.278122\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.209725\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.165740\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.224075\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.106937\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.197660\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.312363\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.392146\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.226711\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.226655\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.270365\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.109277\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.221682\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.230825\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.231832\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.365822\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.289398\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.133185\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.218713\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.304694\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.199429\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.100663\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.269360\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.300014\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.113563\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.159535\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.330515\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.248865\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.275554\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.099238\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.149538\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.283332\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.069060\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.177534\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.331812\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.116129\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.188362\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.432145\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.307836\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.121785\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.269419\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.088228\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.382377\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.130285\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.152708\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.327428\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.177863\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.279767\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.354753\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.221903\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.135490\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.259834\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.173927\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.165129\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.322619\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.412343\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.124356\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.282746\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.306904\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.077965\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.208386\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.080176\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.170409\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.197491\n",
      "\n",
      "Test set: Avg. loss: 0.0681, Accuracy: 9777/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.247709\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.172963\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.294730\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.297724\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.250866\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.369711\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.203204\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.134014\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.098832\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.093354\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.327953\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.207318\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.139937\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.232598\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.215772\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.228195\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.102535\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.191091\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.223165\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.339698\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.227317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.300516\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.465941\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.252868\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.120264\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.134791\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.137490\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.254618\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.502740\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.134832\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.412872\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.099467\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.129690\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.062403\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.307494\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.153802\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.324872\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.196463\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.264732\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.133931\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.470937\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.344596\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.258461\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.088655\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.132008\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.224545\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.213638\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.283065\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.199064\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.123062\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.382903\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.235748\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.097440\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.178233\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.226602\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.437706\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.114407\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.229368\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.439491\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.145852\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.170325\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.170918\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.485234\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.242301\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.281718\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.230453\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.228435\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.221079\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.253199\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.178526\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.148581\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.098097\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.184719\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.158918\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.255133\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.311121\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.182967\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.400957\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.166704\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.170060\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.363521\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.159741\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.186561\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.088625\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.207078\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.109036\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.077976\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.246726\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.318050\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.365802\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.111556\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.230885\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.162511\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.287442\n",
      "\n",
      "Test set: Avg. loss: 0.0644, Accuracy: 9786/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.100236\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.087230\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.112681\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.171713\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.338018\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.397210\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.206801\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.099104\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.331196\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.140717\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.311067\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.143551\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.300724\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.233251\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.181721\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.180541\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.174773\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.174981\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.128126\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.247511\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.165777\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.323999\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.149340\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.107689\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.268362\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.315149\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.146562\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.126739\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.224899\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.116158\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.356423\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.108004\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.202193\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.259755\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.217746\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.188172\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.228621\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.235728\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.174059\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.247285\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.282636\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.247217\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.169541\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.262255\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.262788\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.201123\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.224056\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.097368\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.115538\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.159964\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.258379\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.128878\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.063205\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.102465\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.199139\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.097689\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.201792\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.234906\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.231686\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.176893\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.108012\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.245990\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.418916\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.226459\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.285442\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.192649\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.076898\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.235320\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.375144\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.111469\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.126083\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.100263\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.044297\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.215176\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.125817\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.311701\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.216516\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.225914\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.167234\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.106902\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.195077\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.110363\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.196489\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.148717\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.099121\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.212708\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.382192\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.142022\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.098383\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.173268\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.279564\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.152868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.205250\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.074272\n",
      "\n",
      "Test set: Avg. loss: 0.0593, Accuracy: 9816/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.195859\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.183627\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.131834\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.069188\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.254059\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.077695\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.283479\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.224602\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.148367\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.232872\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.252813\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.219346\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.188102\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.243118\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.243987\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.075515\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.217166\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.137308\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.122748\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.131824\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.112568\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.191841\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.478123\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.336827\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.218344\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.125975\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.151924\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.177826\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.164730\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.081578\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.175606\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.196779\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.192868\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.130051\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.119773\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.210665\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.118113\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.362806\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.051317\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.131481\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.231524\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.296596\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.141739\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.232905\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.038360\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.115911\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.302940\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.138168\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.063589\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.137951\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.165648\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.188700\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.181345\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.205023\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.133612\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.082785\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.187446\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.135976\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.218381\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.128075\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.129748\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.288460\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.188007\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.349514\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.266871\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.229377\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.349697\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.131740\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.101804\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.149093\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.082566\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.091057\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.163399\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.217538\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.146057\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.128090\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.120640\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.130083\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.122641\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.122239\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.176304\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.109321\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.341900\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.278768\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.285733\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.107691\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.164375\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.166051\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.067364\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.189297\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.124508\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.262848\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.330982\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.172160\n",
      "\n",
      "Test set: Avg. loss: 0.0569, Accuracy: 9819/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.185907\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.194102\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.211391\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.062013\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.248813\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.129168\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.131990\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.343215\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.101587\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.103351\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.305406\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.106514\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.255825\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.306508\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.216042\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.290034\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.117371\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.154095\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.083285\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.229213\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.143439\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.153410\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.292036\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.149533\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.222718\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.069049\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.200355\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.150314\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.152647\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.082392\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.166856\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.215561\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.104828\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.121373\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.086103\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.092052\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.077594\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.322882\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.076481\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.168969\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.223827\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.024388\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.130262\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.222876\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.156030\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.368086\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.166711\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.278211\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.248268\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.155037\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.064788\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.231738\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.131868\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.149723\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.099343\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.212119\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.252037\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.267782\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.365772\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.111552\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.188080\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.219074\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.134249\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.125930\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.224080\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.233634\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.082370\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.191483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.127651\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.119514\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.280890\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.174676\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.312178\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.087110\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.134723\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.242130\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.133048\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.094104\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.305479\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.239999\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.050261\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.062300\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.149544\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.219416\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.134464\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.140804\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.276580\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.130013\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.162159\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.241778\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.236493\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.217472\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.199458\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.166163\n",
      "\n",
      "Test set: Avg. loss: 0.0543, Accuracy: 9837/10000 (98%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.106044\n",
      "Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.086618\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.180702\n",
      "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.258715\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.110176\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.091619\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.252277\n",
      "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.116751\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.203112\n",
      "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.193405\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.215201\n",
      "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.176474\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.180864\n",
      "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.200961\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.098066\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.258470\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.161597\n",
      "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.037561\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.166566\n",
      "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.306151\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.137730\n",
      "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.314837\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.101308\n",
      "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.415811\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.177253\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.215319\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.134949\n",
      "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.186911\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.144259\n",
      "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.082886\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.112218\n",
      "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.086370\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.308134\n",
      "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.115812\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.087683\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.197324\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.218039\n",
      "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.201315\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.020183\n",
      "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.072819\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.250605\n",
      "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.119559\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.134688\n",
      "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.117126\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.095703\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.141184\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.147965\n",
      "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.097925\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.146482\n",
      "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.448839\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.044592\n",
      "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.235677\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.252823\n",
      "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.125418\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.134680\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.102097\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.106036\n",
      "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.213301\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.034393\n",
      "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.145279\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.263406\n",
      "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.243519\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.204465\n",
      "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.111620\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.211984\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.139259\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.103239\n",
      "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.086146\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.251986\n",
      "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.067047\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.270114\n",
      "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.114828\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.172130\n",
      "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.181764\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.052484\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.158272\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.106488\n",
      "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.056998\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.487811\n",
      "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.278297\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.219924\n",
      "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.095738\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.125215\n",
      "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.182189\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.078810\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.248977\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.209920\n",
      "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.128940\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.221385\n",
      "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.059084\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.077744\n",
      "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.218377\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.208406\n",
      "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.077406\n",
      "\n",
      "Test set: Avg. loss: 0.0515, Accuracy: 9844/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(net1, test_loader10)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(net1, train_loader10, epoch)\n",
    "    test(net1, test_loader10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-2.6191e-01,  1.6000e+00,  1.2026e-01,  ...,  5.0821e-01,\n",
      "            1.5822e+00, -2.8444e-01],\n",
      "          [-2.5215e-01,  3.0717e-01,  5.8217e-01,  ..., -3.7917e-01,\n",
      "            1.1804e+00,  2.1767e-01],\n",
      "          [-1.0980e+00, -2.9605e-01, -3.7059e-01,  ..., -7.1855e-01,\n",
      "           -9.7577e-01,  4.2951e-01],\n",
      "          ...,\n",
      "          [ 1.6117e+00,  1.8910e-01,  5.3194e-01,  ...,  2.9000e+00,\n",
      "           -5.5116e-01,  1.3791e+00],\n",
      "          [-5.1855e-01,  8.8223e-01,  1.4188e+00,  ..., -9.4058e-01,\n",
      "            3.5701e-01, -1.4769e-02],\n",
      "          [-4.3208e-01, -4.6863e-01,  3.0607e-01,  ...,  3.7375e-02,\n",
      "            2.2970e-01, -4.8209e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 8.8264e-02,  8.0247e-01,  9.9029e-02,  ..., -2.8812e-03,\n",
      "           -1.2020e+00, -4.9668e-02],\n",
      "          [ 1.9805e-01, -2.1863e-01, -7.2834e-01,  ...,  3.2264e-01,\n",
      "            4.4164e-01,  7.3346e-01],\n",
      "          [-2.0377e+00,  1.2917e+00, -4.6451e-01,  ..., -6.4024e-02,\n",
      "            1.0503e+00, -1.1456e+00],\n",
      "          ...,\n",
      "          [-5.6015e-01, -4.5634e-01,  1.2833e-01,  ..., -1.3356e+00,\n",
      "           -1.2550e-01,  7.4293e-02],\n",
      "          [-7.8833e-01,  1.1097e-01,  6.0521e-01,  ..., -3.7422e-01,\n",
      "           -6.1590e-01,  1.2721e+00],\n",
      "          [-1.8313e+00, -2.2796e-01, -3.7456e-01,  ...,  1.1237e+00,\n",
      "            9.1953e-02,  6.9260e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.7787e+00,  5.9405e-02,  7.1008e-01,  ...,  6.9277e-01,\n",
      "            3.9631e-01, -6.3885e-01],\n",
      "          [ 3.0279e-02, -8.4916e-01, -2.3768e-01,  ...,  1.5866e+00,\n",
      "           -1.2055e+00, -6.2208e-01],\n",
      "          [-2.0711e+00,  3.3518e-01,  1.2181e+00,  ...,  5.2159e-02,\n",
      "           -4.7900e-01, -7.4373e-01],\n",
      "          ...,\n",
      "          [ 6.2043e-01,  4.7788e-01,  1.6402e-01,  ..., -1.0176e+00,\n",
      "           -4.3046e-01, -8.8365e-01],\n",
      "          [-8.6499e-01, -2.7575e+00, -6.4236e-01,  ..., -3.7131e-01,\n",
      "            5.4275e-01,  7.8251e-01],\n",
      "          [ 6.6845e-01,  3.3965e-01,  5.3889e-01,  ...,  1.4623e-01,\n",
      "            1.3819e-03,  5.1589e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.5961e+00, -2.5281e-02, -1.0742e-03,  ...,  2.4205e-01,\n",
      "           -2.8552e-01, -7.1839e-01],\n",
      "          [-2.2387e-01, -1.6849e-01, -1.4037e-01,  ...,  5.8037e-01,\n",
      "           -8.6958e-01,  2.9515e-01],\n",
      "          [ 1.2215e-02,  2.5648e-01,  2.0446e-01,  ...,  1.4023e+00,\n",
      "           -2.8346e+00, -2.1468e+00],\n",
      "          ...,\n",
      "          [ 2.9811e-01, -1.5608e+00, -2.8686e-01,  ...,  1.1647e-01,\n",
      "           -3.9236e-01,  7.6014e-01],\n",
      "          [ 2.1795e-01, -5.3230e-01,  7.8460e-01,  ...,  1.1774e+00,\n",
      "           -5.1508e-01,  1.6990e+00],\n",
      "          [ 7.9113e-01,  5.5034e-01,  5.6917e-01,  ...,  5.1052e-01,\n",
      "            1.5044e+00,  1.0190e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1907e+00,  1.6010e+00,  4.5727e-01,  ...,  1.6415e-01,\n",
      "           -2.5902e-01,  1.3203e+00],\n",
      "          [ 9.6612e-02, -1.2823e-01,  1.2228e+00,  ..., -1.0238e+00,\n",
      "           -5.6302e-01, -1.8172e+00],\n",
      "          [ 1.3846e-01, -1.0607e-01, -3.6603e-01,  ...,  4.8812e-01,\n",
      "           -3.5042e-01,  7.6698e-01],\n",
      "          ...,\n",
      "          [-2.3129e+00,  2.3762e-01, -1.0266e-01,  ...,  7.0537e-01,\n",
      "           -8.2272e-01, -1.4142e+00],\n",
      "          [ 2.7644e-01,  4.4708e-01,  2.2987e+00,  ..., -3.8705e-01,\n",
      "            4.2852e-01, -3.9038e-01],\n",
      "          [-9.2727e-01,  3.9866e-01, -2.8041e-01,  ...,  1.1367e+00,\n",
      "           -1.8702e+00, -1.8206e-02]]],\n",
      "\n",
      "\n",
      "        [[[-5.1291e-01, -5.1096e-01,  7.2220e-01,  ...,  1.4819e-01,\n",
      "           -5.9410e-01,  5.7120e-01],\n",
      "          [-4.1722e-02,  6.6410e-01, -8.0418e-01,  ...,  1.1099e+00,\n",
      "            1.6638e+00,  8.9088e-01],\n",
      "          [ 2.2200e+00,  8.0709e-01, -1.9177e+00,  ..., -3.5906e-01,\n",
      "           -8.7681e-01, -1.2604e+00],\n",
      "          ...,\n",
      "          [-8.6773e-01, -7.2826e-01,  5.1101e-01,  ...,  1.6832e+00,\n",
      "            9.8020e-01, -9.1955e-01],\n",
      "          [-2.1962e-01, -4.6693e-01, -1.5700e+00,  ...,  1.9686e-01,\n",
      "            1.7409e+00, -7.6590e-03],\n",
      "          [ 1.2739e+00, -8.7460e-01, -1.3923e+00,  ...,  2.8880e+00,\n",
      "           -1.4006e+00,  1.0820e+00]]]])\n"
     ]
    }
   ],
   "source": [
    "b = 2000;\n",
    "noise = torch.randn(b, 1, 28, 28)\n",
    "print(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72.564896 72.02741  71.821785 71.39236  71.20723  71.03239  70.89848\n",
      " 70.71081  70.527596 70.294136 70.03065  69.96297  69.82146  69.67461\n",
      " 69.50367  69.299385 69.17481  68.94679  68.83895  68.775856 68.683044\n",
      " 68.42211  68.35752  68.25724  68.207825 68.00232  67.8506   67.7941\n",
      " 67.61232  67.551254 67.29059  67.17794  67.10745  67.02427  66.935\n",
      " 66.77299  66.71027  66.61384  66.541756 66.38652  66.25177  66.14398\n",
      " 66.0609   65.926056 65.81205  65.70636  65.6203   65.53505  65.429085\n",
      " 65.26066  65.23882  65.15011  64.98914  64.93331  64.91308  64.79178\n",
      " 64.638176 64.582016 64.49468  64.35157  64.24058  64.21586  64.03889\n",
      " 63.914597 63.812458 63.78243  63.753395 63.580963 63.531845 63.428234\n",
      " 63.342968 63.188034 63.146896 63.038284 62.98759  62.877308 62.80342\n",
      " 62.60198  62.5365   62.486435 62.452354 62.39067  62.31934  62.193577\n",
      " 62.052067 61.9578   61.929737 61.884182 61.83291  61.659546 61.59147\n",
      " 61.426918 61.42095  61.339073 61.3266   61.129658 61.124012 61.034714\n",
      " 60.962086 60.86324  60.793533 60.688553 60.61319  60.3955   60.258667\n",
      " 60.176643 60.115147 60.047916 59.967907 59.923832 59.793285 59.76104\n",
      " 59.720417 59.635315 59.565216 59.55952  59.41651  59.379932 59.19169\n",
      " 59.1425   59.08121  58.98273  58.918438 58.87781  58.735325 58.69573\n",
      " 58.607605 58.520832 58.467545 58.373013 58.314255 58.260616 58.187405\n",
      " 58.075047 57.963627 57.88374  57.850395 57.777145 57.724247 57.69007\n",
      " 57.56764  57.445896 57.39443  57.242275 57.212696 57.147213 57.058712\n",
      " 57.036293 56.95902  56.921787 56.819298 56.764355 56.68097  56.616455\n",
      " 56.489304 56.4641   56.347504 56.298275 56.251244 56.165943 56.121517\n",
      " 56.009495 55.979908 55.928474 55.870644 55.80589  55.71859  55.62222\n",
      " 55.57939  55.461025 55.400856 55.37521  55.365646 55.28201  55.201527\n",
      " 55.021286 55.00122  54.940205 54.882614 54.694504 54.659954 54.560314\n",
      " 54.554455 54.485695 54.329044 54.305557 54.213917 54.194717 54.129772\n",
      " 54.084846 54.039352 53.992466 53.95733  53.821056 53.786438 53.7121\n",
      " 53.61991  53.551826 53.474945 53.431355 53.361332 53.310436 53.267094\n",
      " 53.144825 53.07413  53.025074 52.87715  52.832516 52.76851  52.733475\n",
      " 52.68758  52.64443  52.605213 52.545044 52.4764   52.347607 52.29006\n",
      " 52.252277 52.15656  52.10184  52.068935 51.9997   51.93989  51.86946\n",
      " 51.79253  51.73323  51.674057 51.62143  51.574203 51.52508  51.427025\n",
      " 51.388336 51.262497 51.192604 51.14776  51.08951  51.00246  50.893967\n",
      " 50.872276 50.81981  50.71773  50.699673 50.632942 50.583923 50.473568\n",
      " 50.448338 50.357307 50.296318 50.205673 50.15272  50.130367 50.085003\n",
      " 50.01382  49.98844  49.961353 49.884705 49.84996  49.772617 49.66457\n",
      " 49.56099  49.542454 49.39598  49.384136 49.295593 49.25162  49.232307\n",
      " 49.19783  49.13033  49.055576 48.92099  48.88416  48.816406 48.741535\n",
      " 48.71319  48.629776 48.614132 48.4874   48.442112 48.36343  48.310265\n",
      " 48.268284 48.212894 48.172676 48.06526  47.953896 47.932125 47.887478\n",
      " 47.859913 47.791103 47.67708  47.659508 47.57679  47.514576 47.45331\n",
      " 47.426563 47.372562 47.33921  47.317505 47.262337 47.177372 47.06168\n",
      " 47.00738  46.935486 46.87259  46.852215 46.773766 46.713196 46.61783\n",
      " 46.560444 46.52719  46.460217 46.432037 46.32125  46.28225  46.192364\n",
      " 46.143913 46.083107 46.06925  45.90616  45.866985 45.83416  45.734367\n",
      " 45.674377 45.630993 45.527683 45.490757 45.454678 45.41369  45.36067\n",
      " 45.252174 45.166096 45.126125 45.084343 45.029156 45.016174 44.951828\n",
      " 44.90774  44.838146 44.78821  44.67709  44.652283 44.62076  44.56104\n",
      " 44.505344 44.417442 44.357334 44.327602 44.267235 44.24048  44.157967\n",
      " 44.12397  44.064312 43.984737 43.941902 43.87403  43.765488 43.744663\n",
      " 43.608868 43.59275  43.55105  43.477116 43.403053 43.357243 43.31074\n",
      " 43.28779  43.23858  43.121075 43.10937  43.05073  42.97844  42.95733\n",
      " 42.880497 42.805046 42.78578  42.74179  42.649197 42.63584  42.55469\n",
      " 42.53212  42.36758  42.255566 42.18462  42.163513 42.116127 42.05878\n",
      " 42.01072  41.989353 41.909664 41.840656 41.773758 41.749393 41.719955\n",
      " 41.642838 41.608074 41.593586 41.51438  41.467663 41.393578 41.331882\n",
      " 41.301033 41.24014  41.177048 41.111794 41.039265 40.980762 40.896095\n",
      " 40.858913 40.833267 40.727737 40.68947  40.58091  40.5369   40.492558\n",
      " 40.44692  40.409805 40.316124 40.269203 40.206493 40.143772 40.11804\n",
      " 40.046783 40.015022 39.924217 39.86146  39.81836  39.791214 39.72571\n",
      " 39.680256 39.59581  39.467545 39.39812  39.36288  39.32534  39.28102\n",
      " 39.22195  39.163727 39.047314 39.036186 38.98958  38.90141  38.86404\n",
      " 38.83881  38.756847 38.732018 38.661552 38.60782  38.541058 38.52047\n",
      " 38.43911  38.39082  38.332558 38.20026  38.171806 38.15089  38.09559\n",
      " 38.048244 37.951374 37.933723 37.875885 37.790524 37.71967  37.675575\n",
      " 37.56935  37.499516 37.452423 37.419632 37.397907 37.335754 37.30849\n",
      " 37.23281  37.17925  37.137978 37.076874 37.00879  36.983425 36.869972\n",
      " 36.81235  36.76868  36.684456 36.651608 36.597878 36.52617  36.44599\n",
      " 36.39802  36.342754 36.329933 36.2711   36.131874 36.100098 36.038643\n",
      " 35.99963  35.979908 35.90348  35.889828 35.83733  35.70162  35.633797\n",
      " 35.611443 35.50248  35.497765 35.38888  35.33187  35.240314 35.21727\n",
      " 35.19491  35.1627   35.07816  34.97959  34.930153 34.919254 34.834488\n",
      " 34.81105  34.755806 34.68509  34.63438  34.56795  34.54464  34.53042\n",
      " 34.44481  34.393425 34.32711  34.285103 34.21483  34.17638  34.137135\n",
      " 34.103508 34.065517 33.984642 33.939976 33.850098 33.79234  33.774254\n",
      " 33.73438  33.656715 33.614605 33.57894  33.479588 33.442245 33.367474\n",
      " 33.30455  33.26962  33.237904 33.161694 33.122013 33.053513 32.98046\n",
      " 32.952374 32.864597 32.82152  32.782993 32.69864  32.669876 32.56339\n",
      " 32.498566 32.47408  32.466763 32.393234 32.285954 32.24326  32.171703\n",
      " 32.145645 32.056828 32.01562  31.955845 31.951557 31.913464 31.810354\n",
      " 31.754467 31.730143 31.601349 31.57227  31.526031 31.458387 31.386438\n",
      " 31.338617 31.323618 31.197195 31.173214 31.123598 31.063915 31.019733\n",
      " 30.958984 30.941788 30.893946 30.804625 30.73092  30.700153 30.670744\n",
      " 30.614685 30.56135  30.461151 30.435223 30.372253 30.298481 30.226744\n",
      " 30.185284 30.114353 30.095392 29.951576 29.93276  29.876543 29.823965\n",
      " 29.806885 29.76385  29.739027 29.600107 29.575037 29.530237 29.463833\n",
      " 29.41249  29.336224 29.233803 29.144598 29.113287 29.071413 28.939684\n",
      " 28.885475 28.860262 28.812096 28.763685 28.690876 28.612606 28.590076\n",
      " 28.550566 28.488913 28.400574 28.300406 28.262812 28.233206 28.198101\n",
      " 28.116537 28.055489 28.000216 27.94011  27.86111  27.833265 27.745533\n",
      " 27.686817 27.655563 27.573925 27.532978 27.484701 27.443794 27.385725\n",
      " 27.340555 27.253122 27.213312 27.104923 27.059402 27.022682 26.934393\n",
      " 26.863983 26.775398 26.7243   26.659243 26.614117 26.5884   26.5485\n",
      " 26.491299 26.418034 26.35771  26.33858  26.247116 26.138788 26.11478\n",
      " 26.050282 26.020037 25.9661   25.927004 25.847061 25.83051  25.735409\n",
      " 25.612993 25.601696 25.563826 25.451448 25.409155 25.260086 25.242535\n",
      " 25.184614 25.117498 25.021132 24.982134 24.925259 24.90446  24.820541\n",
      " 24.800192 24.649857 24.606728 24.50095  24.462217 24.424362 24.354515\n",
      " 24.27325  24.210356 24.187876 24.092249 24.055801 23.993338 23.91394\n",
      " 23.830198 23.816658 23.772749 23.693222 23.617823 23.55323  23.509863\n",
      " 23.41489  23.346495 23.299091 23.159262 23.113302 23.037985 23.01948\n",
      " 23.001286 22.936327 22.878426 22.787449 22.743841 22.695606 22.579449\n",
      " 22.507973 22.420258 22.406042 22.325377 22.216455 22.182291 22.137054\n",
      " 21.966566 21.944298 21.876194 21.787441 21.745972 21.719995 21.647297\n",
      " 21.602846 21.537012 21.45855  21.288754 21.23535  21.174736 21.035007\n",
      " 21.023888 20.978117 20.816122 20.747515 20.697874 20.56588  20.477455\n",
      " 20.402285 20.350985 20.288143 20.248291 20.199497 19.996689 19.905724\n",
      " 19.87934  19.848482 19.786888 19.675846 19.53394  19.480757 19.418472\n",
      " 19.35656  19.241776 19.209179 19.07991  18.953468 18.901949 18.777954\n",
      " 18.651436 18.603287 18.431341 18.377588 18.236252 18.174992 18.076027\n",
      " 17.953812 17.894424 17.845423 17.661518 17.574364 17.156248 16.85364 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAEiVJREFUeJzt3X+s3XV9x/Hna62tygRmKYZRWGvaOQEnyg3DOJcNBpaIlESIJUzIQlJjaKJxZoElkI1gIv+MzchMUHBIxgrrRrzRav0BZtMY7K2wQcHGC9ZxrZMSENEFSNl7f5xP8Xi8t/d7f/X+8PlITs73+/l+vp/z+fSe29f9fs/5fj+pKiRJ+o357oAkaWEwECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqVk+3x2YiuOOO67Wrl07392QpEVl9+7dT1XV6snqLapAWLt2LSMjI/PdDUlaVJL8oEs9TxlJkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAjoGQpKNSfYmGU1y9TjbVya5q22/P8naVn5ukt1JHmrPZ/ft8/XW5oPtcfxsDUqSNHWTXpiWZBlwM3AuMAbsSjJcVY/0VbsSeKaq1ifZDNwIvBd4Cnh3Ve1PchqwEzixb7/LqsorzSRpAehypfKZwGhVPQ6QZBuwCegPhE3AX7fl7cAnkqSqHuirswd4ZZKVVfXCjHv+a27t1V94eXnfx961oPrQXz6Rieofrp25GGeXf8cu/Zuvn4E0m7qcMjoReKJvfYxf/iv/l+pU1UHgWWDVQJ33AA8MhMFn2umia5NkvBdPsiXJSJKRAwcOdOiuJGk6ugTCeP9R11TqJDmV3mmk9/dtv6yq3gS8oz3eN96LV9UtVTVUVUOrV096byZJ0jR1CYQx4KS+9TXA/onqJFkOHAM83dbXAPcAl1fVY4d2qKoftufngDvpnZqSJM2TLoGwC9iQZF2SFcBmYHigzjBwRVu+GLi3qirJscAXgGuq6puHKidZnuS4tvwK4ALg4ZkNRZI0E5MGQvtMYCu9bwg9CtxdVXuSXJ/kwlbtVmBVklHgw8Chr6ZuBdYD1w58vXQlsDPJfwEPAj8EPjWbA5MkTU2n+RCqagewY6Dsur7l54FLxtnvBuCGCZo9o3s3JUlzzSuVJUmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJKaTje30+ybq+kXJ5q+ssv0kBOZ6vSYR0KXcc5kis+pjnMmU3FO1ZGYWlS/njxCkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAc6YNmUzmRlrolm4us6ANZPZyuZ639maSW2m7XT9Nz5Sptqf6cxO1+X91qXdLvt2eV0tXh4hSJKAjoGQZGOSvUlGk1w9zvaVSe5q2+9PsraVn5tkd5KH2vPZffuc0cpHk3w8SWZrUJKkqZs0EJIsA24GzgdOAS5NcspAtSuBZ6pqPXATcGMrfwp4d1W9CbgCuKNvn08CW4AN7bFxBuOQJM1QlyOEM4HRqnq8ql4EtgGbBupsAm5vy9uBc5Kkqh6oqv2tfA/wynY0cQJwdFV9q6oK+Cxw0YxHI0mati6BcCLwRN/6WCsbt05VHQSeBVYN1HkP8EBVvdDqj03SJgBJtiQZSTJy4MCBDt2VJE1Hl0AY79x+TaVOklPpnUZ6/xTa7BVW3VJVQ1U1tHr16g7dlSRNR5dAGANO6ltfA+yfqE6S5cAxwNNtfQ1wD3B5VT3WV3/NJG1Kko6gLoGwC9iQZF2SFcBmYHigzjC9D40BLgburapKcizwBeCaqvrmocpV9SPguSRntW8XXQ58boZjkSTNwKSB0D4T2ArsBB4F7q6qPUmuT3Jhq3YrsCrJKPBh4NBXU7cC64FrkzzYHse3bR8APg2MAo8BX5ytQUmSpq7TlcpVtQPYMVB2Xd/y88Al4+x3A3DDBG2OAKdNpbOSpLnjlcqSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktSkd/fpxWFoaKhGRkbmtQ9zMYWmtJBN9T081Wk8pzp1p6Yuye6qGpqsnkcIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDXL57sDi1mX6QSdNlO/biZ6z8/kd2GqU3dOp12n7PQIQZLUGAiSJMBAkCQ1BoIkCegYCEk2JtmbZDTJ1eNsX5nkrrb9/iRrW/mqJPcl+VmSTwzs8/XW5oPtcfxsDEiSND2TfssoyTLgZuBcYAzYlWS4qh7pq3Yl8ExVrU+yGbgReC/wPHAtcFp7DLqsqkZmOAZJ0izocoRwJjBaVY9X1YvANmDTQJ1NwO1teTtwTpJU1c+r6hv0gkGStIB1CYQTgSf61sda2bh1quog8CywqkPbn2mni65NkvEqJNmSZCTJyIEDBzo0KUmaji6BMN5/1DWNOoMuq6o3Ae9oj/eNV6mqbqmqoaoaWr169aSdlSRNT5dAGANO6ltfA+yfqE6S5cAxwNOHa7SqftienwPupHdqSpI0T7oEwi5gQ5J1SVYAm4HhgTrDwBVt+WLg3qqa8AghyfIkx7XlVwAXAA9PtfOSpNkz6beMqupgkq3ATmAZcFtV7UlyPTBSVcPArcAdSUbpHRlsPrR/kn3A0cCKJBcB5wE/AHa2MFgGfBX41KyOTJI0JZ1ubldVO4AdA2XX9S0/D1wywb5rJ2j2jG5dlCQdCV6pLEkCDARJUmMgSJIAA0GS1BgIkiTAKTR/idNdSr9qrn8v5qr9iabH9Pd8Yh4hSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktSkqua7D50NDQ3VyMjIjNtxCj1J0zHRVJxdyudTkt1VNTRZPY8QJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEdAyHJxiR7k4wmuXqc7SuT3NW2359kbStfleS+JD9L8omBfc5I8lDb5+NJMhsDkiRNz6SBkGQZcDNwPnAKcGmSUwaqXQk8U1XrgZuAG1v588C1wEfGafqTwBZgQ3tsnM4AJEmzo8sRwpnAaFU9XlUvAtuATQN1NgG3t+XtwDlJUlU/r6pv0AuGlyU5ATi6qr5VvUulPwtcNJOBSJJmpksgnAg80bc+1srGrVNVB4FngVWTtDk2SZuSpCOoSyCMd25/8AZIXepMq36SLUlGkowcOHDgME1KkmaiSyCMASf1ra8B9k9UJ8ly4Bjg6UnaXDNJmwBU1S1VNVRVQ6tXr+7QXUnSdHQJhF3AhiTrkqwANgPDA3WGgSva8sXAvXWY26hW1Y+A55Kc1b5ddDnwuSn3XpI0a5ZPVqGqDibZCuwElgG3VdWeJNcDI1U1DNwK3JFklN6RweZD+yfZBxwNrEhyEXBeVT0CfAD4R+BVwBfbQ5I0TyYNBICq2gHsGCi7rm/5eeCSCfZdO0H5CHBa145KkuaWVypLkoCORwiSpIlnW+wyC+NimGHNIwRJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGqfQlKQ50mVqzYXEIwRJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEdAyEJBuT7E0ymuTqcbavTHJX235/krV9265p5XuTvLOvfF+Sh5I8mGRkNgYjSZq+SW9ul2QZcDNwLjAG7EoyXFWP9FW7EnimqtYn2QzcCLw3ySnAZuBU4LeBryb53ap6qe33J1X11CyOR5I0TV2OEM4ERqvq8ap6EdgGbBqoswm4vS1vB85Jkla+rapeqKrvA6OtPUnSAtMlEE4EnuhbH2tl49apqoPAs8CqSfYt4MtJdifZMvWuS5JmU5f5EDJOWXWsc7h9315V+5McD3wlyXer6t9/5cV7YbEF4OSTT+7QXUnSdHQ5QhgDTupbXwPsn6hOkuXAMcDTh9u3qg49PwncwwSnkqrqlqoaqqqh1atXd+iuJGk6ugTCLmBDknVJVtD7kHh4oM4wcEVbvhi4t6qqlW9u30JaB2wAvp3kqCSvAUhyFHAe8PDMhyNJmq5JTxlV1cEkW4GdwDLgtqrak+R6YKSqhoFbgTuSjNI7Mtjc9t2T5G7gEeAgcFVVvZTkdcA9vc+dWQ7cWVVfmoPxvWyxTWUnaenq8v9Rf519H3vXXHbnZZ3mVK6qHcCOgbLr+pafBy6ZYN+PAh8dKHscePNUOytJmjteqSxJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQI6zpgmSZp78z3Vr0cIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVLTKRCSbEyyN8lokqvH2b4yyV1t+/1J1vZtu6aV703yzq5tSpKOrEkDIcky4GbgfOAU4NIkpwxUuxJ4pqrWAzcBN7Z9TwE2A6cCG4F/SLKsY5uSpCOoyxHCmcBoVT1eVS8C24BNA3U2Abe35e3AOUnSyrdV1QtV9X1gtLXXpU1J0hHUJRBOBJ7oWx9rZePWqaqDwLPAqsPs26VNSdIR1GUKzYxTVh3rTFQ+XhANttlrONkCbGmrP0uyd4J+HgnHAU/N4+vPpaU6tqU6LnBsi9G0xpUbZ/y6v9OlUpdAGANO6ltfA+yfoM5YkuXAMcDTk+w7WZsAVNUtwC0d+jnnkoxU1dB892MuLNWxLdVxgWNbjBb6uLqcMtoFbEiyLskKeh8SDw/UGQauaMsXA/dWVbXyze1bSOuADcC3O7YpSTqCJj1CqKqDSbYCO4FlwG1VtSfJ9cBIVQ0DtwJ3JBmld2Swue27J8ndwCPAQeCqqnoJYLw2Z394kqSu0vtDXl0k2dJOYS05S3VsS3Vc4NgWo4U+LgNBkgR46wpJUmMgTCDJSUnuS/Jokj1JPtjKX5vkK0m+155/a777OhVJXpnk20n+s43rb1r5unbbke+125CsmO++Tle7Gv6BJJ9v60tibEn2JXkoyYNJRlrZon4/AiQ5Nsn2JN9tv29vWyLjekP7WR16/DTJhxby2AyEiR0E/qKq3gicBVzVbq9xNfC1qtoAfK2tLyYvAGdX1ZuB04GNSc6id7uRm9q4nqF3O5LF6oPAo33rS2lsf1JVp/d9dXGxvx8B/h74UlX9HvBmej+7RT+uqtrbflanA2cA/wvcw0IeW1X56PAAPgecC+wFTmhlJwB757tvMxjTq4HvAH9A72KZ5a38bcDO+e7fNMe0ht4v2dnA5+ldHLlUxrYPOG6gbFG/H4Gjge/TPs9cKuMaZ5znAd9c6GPzCKGDdvfWtwD3A6+rqh8BtOfj569n09NOqTwIPAl8BXgM+En1bjsCi/tWIn8H/CXwf219FUtnbAV8OcnudgU/LP734+uBA8Bn2mm+Tyc5isU/rkGbgX9uywt2bAbCJJL8JvCvwIeq6qfz3Z/ZUFUvVe8wdg29Gw2+cbxqR7ZXM5fkAuDJqtrdXzxO1UU3tubtVfVWencJvirJH813h2bBcuCtwCer6i3Az1lIp1BmQfvM6kLgX+a7L5MxEA4jySvohcE/VdW/teIfJzmhbT+B3l/Zi1JV/QT4Or3PSI5ttx2Bw9xKZIF7O3Bhkn307qB7Nr0jhqUwNqpqf3t+kt656DNZ/O/HMWCsqu5v69vpBcRiH1e/84HvVNWP2/qCHZuBMIF2++5bgUer6m/7NvXfpuMKep8tLBpJVic5ti2/CvhTeh/i3UfvtiOwCMcFUFXXVNWaqlpL7xD93qq6jCUwtiRHJXnNoWV656QfZpG/H6vqf4AnkryhFZ1D784Gi3pcAy7lF6eLYAGPzQvTJpDkD4H/AB7iF+ej/4re5wh3AycD/w1cUlVPz0snpyHJ79Obu2IZvT8I7q6q65O8nt5f1a8FHgD+rKpemL+ezkySPwY+UlUXLIWxtTHc01aXA3dW1UeTrGIRvx8BkpwOfBpYATwO/DntvckiHhdAklfTu9X/66vq2Va2YH9mBoIkCfCUkSSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAfD/5rb+ca7emZwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from numpy.linalg import svd\n",
    "ll = noise.view(b, -1).cpu().detach().numpy() \n",
    "_, svd_noise, _ = svd(ll)\n",
    "_ = plt.hist(svd_noise, bins=100, normed=True)\n",
    "print(svd_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naiqi/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "output = net1(noise.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 50])\n"
     ]
    }
   ],
   "source": [
    "ll = net1.last\n",
    "print(ll.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = ll.cpu().detach().numpy() \n",
    "_, svd_net1_c10, _ = svd(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naiqi/anaconda3/envs/py36/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6521: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  alternative=\"'density'\", removal=\"3.1\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.13621958, 0.        , 0.08513724, 0.08513724, 0.06810981,\n",
       "        0.06810978, 0.06810978, 0.06810981, 0.0340549 , 0.0340549 ,\n",
       "        0.03405488, 0.        , 0.01702745, 0.        , 0.01702745,\n",
       "        0.01702745, 0.        , 0.01702745, 0.        , 0.        ,\n",
       "        0.01702742, 0.        , 0.01702745, 0.        , 0.        ,\n",
       "        0.01702745, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01702748, 0.        , 0.        ,\n",
       "        0.        , 0.01702742, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.01702742]),\n",
       " array([8.10913260e-15, 1.17457414e+00, 2.34914827e+00, 3.52372241e+00,\n",
       "        4.69829655e+00, 5.87287045e+00, 7.04744482e+00, 8.22201920e+00,\n",
       "        9.39659309e+00, 1.05711670e+01, 1.17457409e+01, 1.29203157e+01,\n",
       "        1.40948896e+01, 1.52694635e+01, 1.64440384e+01, 1.76186123e+01,\n",
       "        1.87931862e+01, 1.99677601e+01, 2.11423340e+01, 2.23169079e+01,\n",
       "        2.34914818e+01, 2.46660576e+01, 2.58406315e+01, 2.70152054e+01,\n",
       "        2.81897793e+01, 2.93643532e+01, 3.05389271e+01, 3.17135010e+01,\n",
       "        3.28880768e+01, 3.40626488e+01, 3.52372246e+01, 3.64117966e+01,\n",
       "        3.75863724e+01, 3.87609444e+01, 3.99355202e+01, 4.11100960e+01,\n",
       "        4.22846680e+01, 4.34592438e+01, 4.46338158e+01, 4.58083916e+01,\n",
       "        4.69829636e+01, 4.81575394e+01, 4.93321152e+01, 5.05066872e+01,\n",
       "        5.16812630e+01, 5.28558350e+01, 5.40304108e+01, 5.52049828e+01,\n",
       "        5.63795586e+01, 5.75541306e+01, 5.87287064e+01, 5.99032822e+01,\n",
       "        6.10778542e+01, 6.22524300e+01, 6.34270020e+01, 6.46015778e+01,\n",
       "        6.57761536e+01, 6.69507217e+01, 6.81252975e+01, 6.92998734e+01,\n",
       "        7.04744492e+01, 7.16490250e+01, 7.28235931e+01, 7.39981689e+01,\n",
       "        7.51727448e+01, 7.63473206e+01, 7.75218887e+01, 7.86964645e+01,\n",
       "        7.98710403e+01, 8.10456161e+01, 8.22201920e+01, 8.33947601e+01,\n",
       "        8.45693359e+01, 8.57439117e+01, 8.69184875e+01, 8.80930634e+01,\n",
       "        8.92676315e+01, 9.04422073e+01, 9.16167831e+01, 9.27913589e+01,\n",
       "        9.39659271e+01, 9.51405029e+01, 9.63150787e+01, 9.74896545e+01,\n",
       "        9.86642303e+01, 9.98387985e+01, 1.01013374e+02, 1.02187950e+02,\n",
       "        1.03362526e+02, 1.04537094e+02, 1.05711670e+02, 1.06886246e+02,\n",
       "        1.08060822e+02, 1.09235397e+02, 1.10409966e+02, 1.11584541e+02,\n",
       "        1.12759117e+02, 1.13933693e+02, 1.15108261e+02, 1.16282837e+02,\n",
       "        1.17457413e+02], dtype=float32),\n",
       " <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(svd_net1_c10, bins=100, normed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.17457413e+02 4.91695213e+01 4.39079323e+01 3.04415607e+01\n",
      " 2.66073036e+01 2.43308563e+01 2.09312172e+01 1.83306732e+01\n",
      " 1.70965862e+01 1.46694355e+01 1.28857641e+01 1.19768610e+01\n",
      " 1.14052038e+01 1.11284990e+01 1.01634130e+01 9.88846111e+00\n",
      " 9.26074505e+00 9.09348679e+00 8.78005695e+00 8.39824390e+00\n",
      " 8.08632469e+00 8.00556278e+00 7.73175907e+00 7.18629885e+00\n",
      " 6.78189182e+00 6.52420998e+00 6.36985445e+00 6.11617136e+00\n",
      " 5.86494064e+00 5.65963888e+00 4.98579502e+00 4.94813776e+00\n",
      " 4.65096569e+00 4.45912600e+00 3.95222735e+00 3.65528131e+00\n",
      " 3.54803348e+00 3.24312544e+00 2.99603581e+00 2.74233174e+00\n",
      " 2.69110823e+00 2.36616898e+00 8.05393815e-01 7.39224195e-01\n",
      " 6.93635583e-01 6.04064167e-01 4.02935237e-01 3.90441000e-01\n",
      " 8.10913260e-15 8.10913260e-15]\n"
     ]
    }
   ],
   "source": [
    "print(svd_net1_c10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net2 similar to net, but last layer dim=100, instead of 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        self.last = x\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = Net2().cuda()\n",
    "n_epochs = 10\n",
    "optimizer = optim.SGD(net2.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader10.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naiqi/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3074, Accuracy: 1114/10000 (11%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.288522\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.310102\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.290101\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.284409\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.290248\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.298102\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.282577\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.248449\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.245279\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.243803\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.204534\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.181953\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.168794\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.102715\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.055047\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.916580\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.738268\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.468500\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.407731\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.161059\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.293398\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.281642\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.271985\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 1.238611\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.015480\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.026598\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.866228\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.855466\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.787713\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.832617\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.718624\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.794481\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.938590\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.575992\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.704798\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.701243\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.709272\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.764580\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.774855\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.576155\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.734603\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.690096\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.719757\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.631400\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.578059\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.485465\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.661133\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.572866\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.532655\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.510595\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.519701\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.702215\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.566760\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.469520\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.457977\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.484039\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.603955\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.662566\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.341123\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.557903\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.461643\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.394537\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.393837\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.451749\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.751732\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.440589\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.555500\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.307684\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.577361\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.343328\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.408429\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.337049\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.414562\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.614529\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.615332\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.406876\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.477274\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.633530\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.595147\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.533283\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.485747\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.556611\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.288065\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.335823\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.285699\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.345907\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.348869\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.357434\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.668119\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.588712\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.472577\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.271303\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.329271\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.370872\n",
      "\n",
      "Test set: Avg. loss: 0.1792, Accuracy: 9432/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.342679\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.268852\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.546361\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.444367\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.379766\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.429833\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.558894\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.549987\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.240424\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.394991\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.502497\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.480638\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.232534\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.369290\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.405829\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.198329\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.421872\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.281621\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.301701\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.415444\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.399012\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.387244\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.462777\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.153916\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.277027\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.275905\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.160257\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.383216\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.358128\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.252735\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.370450\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.382694\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.163671\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.463101\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.263565\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.246080\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.391558\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.407885\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.373332\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.348258\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.300217\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.328426\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.265519\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.407420\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.269475\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.246340\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.462775\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.378574\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.272579\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.248954\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.258854\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.193592\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.405590\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.201548\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.126069\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.422749\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.227252\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.249479\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.284311\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.205016\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.226236\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.346501\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.234416\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.345918\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.287128\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.096277\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.380114\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.308756\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.291520\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.290873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.266035\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.185744\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.290547\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.134534\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.217128\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.278396\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.333344\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.431278\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.349911\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.414125\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.141687\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.490166\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.198963\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.294182\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.166205\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.180784\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.164786\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.362507\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.146096\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.353935\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.107746\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.392046\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.267754\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.455978\n",
      "\n",
      "Test set: Avg. loss: 0.1144, Accuracy: 9646/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.131153\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.220945\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.326146\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.283036\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.519229\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.284807\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.167652\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.276154\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.204479\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.266616\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.297174\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.200008\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.189196\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.388018\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.324218\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.296662\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.358194\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.234079\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.181031\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.199123\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.171761\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.164580\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.182809\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.158946\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.272131\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.360539\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.132209\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.296968\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.414779\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.208581\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.256989\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.314158\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.339934\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.202713\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.160159\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.401370\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.422194\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.265530\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.340238\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.213185\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.197781\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.319736\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.298211\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.420726\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.320387\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.264285\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.236857\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.240864\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.093426\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.172005\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.217057\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.149232\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.207177\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.146429\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.194459\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.267292\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.109748\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.167625\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.338198\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.155214\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.217537\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.301398\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.132401\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.149388\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.347787\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.360835\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.128134\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.198966\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.103232\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.178607\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.274727\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.266384\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.137243\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.166823\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.298777\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.157988\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.100879\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.163900\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.112765\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.129706\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.322465\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.218685\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.088348\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.347473\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.418906\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.284851\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.281062\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.202119\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.175036\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.167292\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.140143\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.237633\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.391964\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.172945\n",
      "\n",
      "Test set: Avg. loss: 0.0889, Accuracy: 9715/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.231703\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.207697\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.226841\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.321845\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.197487\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.116468\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.065151\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.250991\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.326699\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.159443\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.258220\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.077106\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.229460\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.135477\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.174408\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.151180\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.419914\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.375968\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.178017\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.219951\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.255620\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.065985\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.161741\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.199038\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.092484\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.177335\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.329349\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.146385\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.336088\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.289323\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.120261\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.246353\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.296761\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.121508\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.320479\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.171068\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.394675\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.236904\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.261805\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.332138\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.124045\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.278183\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.315706\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.122140\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.279608\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.267076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.185238\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.215423\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.212814\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.171681\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.141145\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.223571\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.242784\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.164667\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.154460\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.121669\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.105415\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.346812\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.083585\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.210093\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.315057\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.188002\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.228438\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.259277\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.108278\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.251671\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.235873\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.098594\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.214171\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.261477\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.122151\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.131696\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.291153\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.289226\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.088141\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.174997\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.193926\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.389670\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.089783\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.216568\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.295568\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.218533\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.123509\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.106021\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.074689\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.139322\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.120331\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.150938\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.136014\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.137258\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.338032\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.084888\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.083454\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.090653\n",
      "\n",
      "Test set: Avg. loss: 0.0760, Accuracy: 9777/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.212010\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.217759\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.148232\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.174532\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.382034\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.343651\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.163677\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.174227\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.073923\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.245766\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.215862\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.297836\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.313294\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.539596\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.226558\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.165823\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.124406\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.182269\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.293863\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.195125\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.125484\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.117681\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.129663\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.120547\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.153791\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.315063\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.190021\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.103474\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.310491\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.064730\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.165186\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.380720\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.363018\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.156172\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.255880\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.213146\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.150611\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.027940\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.098065\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.254185\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.185539\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.110902\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.082526\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.329837\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.174864\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.229194\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.173224\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.240568\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.148907\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.049039\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.304763\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.192217\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.219635\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.211864\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.232112\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.553201\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.066706\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.163184\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.133452\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.202536\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.177879\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.356270\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.133210\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.477797\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.125129\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.325026\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.258034\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.328672\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.260562\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.128254\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.222784\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.056136\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.143717\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.195972\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.269506\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.148769\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.095561\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.120888\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.210355\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.214182\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.374667\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.065459\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.153439\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.060002\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.069007\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.213789\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.302755\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.093510\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.102349\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.246184\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.121676\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.303903\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.168998\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.143556\n",
      "\n",
      "Test set: Avg. loss: 0.0639, Accuracy: 9793/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.282795\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.142586\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.113331\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.182874\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.102047\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.182508\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.071632\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.076944\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.063511\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.305380\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.131402\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.206086\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.068685\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.195714\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.150783\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.151186\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.140513\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.195304\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.218983\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.370050\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.065811\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.238116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.062279\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.130699\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.156980\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.072087\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.415923\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.124832\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.155165\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.079360\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.099484\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.182737\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.079778\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.218099\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.089105\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.347106\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.331802\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.161071\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.161372\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.083050\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.171177\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.170312\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.336992\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.119907\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.113547\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.176269\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.155449\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.274576\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.198062\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.384422\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.152453\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.037802\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.225124\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.115552\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.221857\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.228443\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.156024\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.198955\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.068494\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.043719\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.179609\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.335173\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.168000\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.199224\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.105378\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.140199\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.084311\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.179303\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.269614\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.313901\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.192933\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.194595\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.094968\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.166143\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.086010\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.245066\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.253073\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.148517\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.212320\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.085866\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.130669\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.197019\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.244779\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.209872\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.226254\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.080128\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.083574\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.181790\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.103834\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.372419\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.111401\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.258236\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.080992\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.145454\n",
      "\n",
      "Test set: Avg. loss: 0.0616, Accuracy: 9794/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.065979\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.065541\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.144098\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.260547\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.140615\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.173717\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.118437\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.159670\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.072010\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.195943\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.091353\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.271468\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.113387\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.268001\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.058767\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.079435\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.065650\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.117181\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.232857\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.095385\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.152165\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.056765\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.141396\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.106021\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.034685\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.095037\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.060369\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.224189\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.026003\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.075423\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.264563\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.103104\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.123012\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.193152\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.068799\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.251091\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.156354\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.032852\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.122945\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.150184\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.080349\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.141568\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.187810\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.253123\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.212254\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.090035\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.213838\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.206091\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.099450\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.194975\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.151781\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.074805\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.202028\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.021833\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.109911\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.297938\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.078894\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.087521\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.139354\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.133652\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.090486\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.126045\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.058004\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.152313\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.045579\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.139856\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.137787\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.134079\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.364610\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.044907\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.072487\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.100737\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.054676\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.256913\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.321576\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.234826\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.104475\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.088780\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.356877\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.209787\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.151416\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.215372\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.133927\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.074263\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.133255\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.598783\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.041246\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.196130\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.162063\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.137502\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.140638\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.100875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.355349\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.142604\n",
      "\n",
      "Test set: Avg. loss: 0.0548, Accuracy: 9816/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.180897\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.145571\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.110002\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.200543\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.070035\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.122384\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.130428\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.116895\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.261097\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.044600\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.322890\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.134576\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.183788\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.143282\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.251352\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.147351\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.169182\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.112782\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.121219\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.103687\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.054102\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.296658\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.323050\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.064130\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.066696\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.105078\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.024018\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.150885\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.187238\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.070873\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.113169\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.163975\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.222608\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.262392\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.294466\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.147432\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.170930\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.277714\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.063146\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.128004\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.141426\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.180511\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.288497\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.132421\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.203281\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.142563\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.085769\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.114293\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.100549\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.069877\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.047186\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.098498\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.368697\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.074885\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.169938\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.146004\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.122365\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.267758\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.104632\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.156691\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.064149\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.296381\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.143002\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.138555\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.106523\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.125369\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.208544\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.127539\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.126785\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.131348\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.084110\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.141282\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.063675\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.323660\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.135820\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.091707\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.115181\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.216416\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.125370\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.147412\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.121539\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.175995\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.181353\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.293193\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.095159\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.069371\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.218824\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.239165\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.212230\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.118504\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.041654\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.203279\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.047041\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.017618\n",
      "\n",
      "Test set: Avg. loss: 0.0505, Accuracy: 9842/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.187108\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.125337\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.128048\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.060149\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.209189\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.246746\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.152608\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.145046\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.113958\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.045681\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.061448\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.048516\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.018875\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.173067\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.096038\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.038696\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.154322\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.089375\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.149595\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.112069\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.147411\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.093643\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.073996\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.155883\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.054148\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.210848\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.179299\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.095025\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.050721\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.165297\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.125213\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.183725\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.071859\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.145517\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.108061\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.295551\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.032037\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.073237\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.278625\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.036648\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.140369\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.029489\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.215886\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.039430\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.204814\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.124988\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.227841\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.096467\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.159006\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.377949\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.178709\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.053348\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.307841\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.056264\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.052181\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.125374\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.122742\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.196298\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.124268\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.050685\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.216248\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.177049\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.232448\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.131400\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.156701\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.254224\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.065545\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.210642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.202216\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.132746\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.026898\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.169453\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.062813\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.172081\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.061824\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.076804\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.079331\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.210631\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.272324\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.082527\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.134004\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.303418\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.263496\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.276479\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.217985\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.075055\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.149603\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.197337\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.108593\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.127134\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.131723\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.160803\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.183179\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.186272\n",
      "\n",
      "Test set: Avg. loss: 0.0486, Accuracy: 9838/10000 (98%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.196507\n",
      "Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.107099\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.109600\n",
      "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.091113\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.095558\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.067502\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.106971\n",
      "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.119843\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.268273\n",
      "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.120204\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.083696\n",
      "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.164477\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.106628\n",
      "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.106191\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.137071\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.113498\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.098168\n",
      "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.092670\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.179690\n",
      "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.228036\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.151130\n",
      "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.165983\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.135680\n",
      "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.077538\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.145524\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.147709\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.076788\n",
      "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.026759\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.207293\n",
      "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.040573\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.096800\n",
      "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.143378\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.367429\n",
      "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.099214\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.123246\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.074780\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.095917\n",
      "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.125101\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.144202\n",
      "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.128289\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.126386\n",
      "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.151758\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.217391\n",
      "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.110123\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.048660\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.253804\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.236451\n",
      "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.131928\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.076387\n",
      "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.267570\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.104347\n",
      "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.066666\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.151793\n",
      "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.123996\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.136182\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.050652\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.148421\n",
      "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.121306\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.099430\n",
      "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.070108\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.233411\n",
      "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.132381\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.063762\n",
      "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.038431\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.037873\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.087237\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.066626\n",
      "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.167992\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.224697\n",
      "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.106675\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.388288\n",
      "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.066496\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.097025\n",
      "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.134619\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.105142\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.066374\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.130813\n",
      "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.196506\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.350736\n",
      "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.040093\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.154051\n",
      "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.170698\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.037101\n",
      "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.145317\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.093465\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.070514\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.063253\n",
      "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.144636\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.054651\n",
      "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.126861\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.082149\n",
      "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.059705\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.045258\n",
      "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.031973\n",
      "\n",
      "Test set: Avg. loss: 0.0478, Accuracy: 9848/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(net2, test_loader10)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(net2, train_loader10, epoch)\n",
    "    test(net2, test_loader10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naiqi/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.00301022, 0.        , 0.00602043, 0.00903065, 0.00602043,\n",
       "        0.00602043, 0.02709195, 0.02107152, 0.03010217, 0.0361226 ,\n",
       "        0.02709195, 0.02709195, 0.04214304, 0.05719412, 0.06020434,\n",
       "        0.09632694, 0.06923499, 0.11137803, 0.10836781, 0.07826564,\n",
       "        0.1053576 , 0.10836781, 0.12040868, 0.13846998, 0.1414802 ,\n",
       "        0.09933716, 0.13846998, 0.20469476, 0.14449042, 0.18663346,\n",
       "        0.15352107, 0.19265389, 0.16255172, 0.1595415 , 0.13545977,\n",
       "        0.1595415 , 0.16255172, 0.14750063, 0.13846998, 0.21974584,\n",
       "        0.13244955, 0.13846998, 0.13846998, 0.1234189 , 0.10836781,\n",
       "        0.09933716, 0.09933716, 0.06923499, 0.08127586, 0.07826564,\n",
       "        0.09933716, 0.09933716, 0.09632694, 0.06923499, 0.06622477,\n",
       "        0.07525543, 0.04214304, 0.03913282, 0.0361226 , 0.04816347,\n",
       "        0.04816347, 0.03010217, 0.03913282, 0.0361226 , 0.02107152,\n",
       "        0.01505109, 0.02709195, 0.02107152, 0.02408174, 0.02408174,\n",
       "        0.03311239, 0.01505109, 0.00602043, 0.00602043, 0.00903065,\n",
       "        0.01204087, 0.00301022, 0.00301022, 0.01204087, 0.00301022,\n",
       "        0.00903065, 0.        , 0.00602043, 0.00301022, 0.00602043,\n",
       "        0.00903065, 0.00301022, 0.00301022, 0.00301022, 0.00301022,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00301022,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00301022]),\n",
       " array([ 7.34908815,  7.51518913,  7.68129011,  7.84739109,  8.01349207,\n",
       "         8.17959305,  8.34569404,  8.51179502,  8.677896  ,  8.84399698,\n",
       "         9.01009796,  9.17619894,  9.34229992,  9.5084009 ,  9.67450188,\n",
       "         9.84060286, 10.00670384, 10.17280482, 10.3389058 , 10.50500678,\n",
       "        10.67110776, 10.83720874, 11.00330972, 11.1694107 , 11.33551168,\n",
       "        11.50161266, 11.66771364, 11.83381462, 11.9999156 , 12.16601658,\n",
       "        12.33211757, 12.49821855, 12.66431953, 12.83042051, 12.99652149,\n",
       "        13.16262247, 13.32872345, 13.49482443, 13.66092541, 13.82702639,\n",
       "        13.99312737, 14.15922835, 14.32532933, 14.49143031, 14.65753129,\n",
       "        14.82363227, 14.98973325, 15.15583423, 15.32193521, 15.48803619,\n",
       "        15.65413717, 15.82023815, 15.98633913, 16.15244011, 16.31854109,\n",
       "        16.48464208, 16.65074306, 16.81684404, 16.98294502, 17.149046  ,\n",
       "        17.31514698, 17.48124796, 17.64734894, 17.81344992, 17.9795509 ,\n",
       "        18.14565188, 18.31175286, 18.47785384, 18.64395482, 18.8100558 ,\n",
       "        18.97615678, 19.14225776, 19.30835874, 19.47445972, 19.6405607 ,\n",
       "        19.80666168, 19.97276266, 20.13886364, 20.30496462, 20.47106561,\n",
       "        20.63716659, 20.80326757, 20.96936855, 21.13546953, 21.30157051,\n",
       "        21.46767149, 21.63377247, 21.79987345, 21.96597443, 22.13207541,\n",
       "        22.29817639, 22.46427737, 22.63037835, 22.79647933, 22.96258031,\n",
       "        23.12868129, 23.29478227, 23.46088325, 23.62698423, 23.79308521,\n",
       "        23.95918619]),\n",
       " <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = net2(noise.cuda())\n",
    "ll = net2.last\n",
    "print(ll.shape)\n",
    "ll = ll.cpu().detach().numpy() \n",
    "lid_net2_c10 = LID(ll, ll, k=50)\n",
    "plt.hist(lid_net2_c10, bins=100, normed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try 8 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_c8(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_c8, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        self.last = x\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naiqi/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.0851, Accuracy: 953/8017 (11%)\n",
      "\n",
      "Train Epoch: 1 [0/48200 (0%)]\tLoss: 2.080665\n",
      "Train Epoch: 1 [640/48200 (1%)]\tLoss: 2.077751\n",
      "Train Epoch: 1 [1280/48200 (3%)]\tLoss: 2.054353\n",
      "Train Epoch: 1 [1920/48200 (4%)]\tLoss: 2.033965\n",
      "Train Epoch: 1 [2560/48200 (5%)]\tLoss: 1.990703\n",
      "Train Epoch: 1 [3200/48200 (7%)]\tLoss: 1.951077\n",
      "Train Epoch: 1 [3840/48200 (8%)]\tLoss: 1.842593\n",
      "Train Epoch: 1 [4480/48200 (9%)]\tLoss: 1.821124\n",
      "Train Epoch: 1 [5120/48200 (11%)]\tLoss: 1.542811\n",
      "Train Epoch: 1 [5760/48200 (12%)]\tLoss: 1.499018\n",
      "Train Epoch: 1 [6400/48200 (13%)]\tLoss: 1.317794\n",
      "Train Epoch: 1 [7040/48200 (15%)]\tLoss: 1.324623\n",
      "Train Epoch: 1 [7680/48200 (16%)]\tLoss: 1.246471\n",
      "Train Epoch: 1 [8320/48200 (17%)]\tLoss: 0.994980\n",
      "Train Epoch: 1 [8960/48200 (19%)]\tLoss: 0.791617\n",
      "Train Epoch: 1 [9600/48200 (20%)]\tLoss: 0.807380\n",
      "Train Epoch: 1 [10240/48200 (21%)]\tLoss: 0.614277\n",
      "Train Epoch: 1 [10880/48200 (23%)]\tLoss: 0.647308\n",
      "Train Epoch: 1 [11520/48200 (24%)]\tLoss: 0.749157\n",
      "Train Epoch: 1 [12160/48200 (25%)]\tLoss: 0.598286\n",
      "Train Epoch: 1 [12800/48200 (27%)]\tLoss: 0.710374\n",
      "Train Epoch: 1 [13440/48200 (28%)]\tLoss: 0.649362\n",
      "Train Epoch: 1 [14080/48200 (29%)]\tLoss: 0.523132\n",
      "Train Epoch: 1 [14720/48200 (31%)]\tLoss: 0.824831\n",
      "Train Epoch: 1 [15360/48200 (32%)]\tLoss: 0.616160\n",
      "Train Epoch: 1 [16000/48200 (33%)]\tLoss: 0.520115\n",
      "Train Epoch: 1 [16640/48200 (34%)]\tLoss: 0.514463\n",
      "Train Epoch: 1 [17280/48200 (36%)]\tLoss: 0.575170\n",
      "Train Epoch: 1 [17920/48200 (37%)]\tLoss: 0.569719\n",
      "Train Epoch: 1 [18560/48200 (38%)]\tLoss: 0.738421\n",
      "Train Epoch: 1 [19200/48200 (40%)]\tLoss: 0.591270\n",
      "Train Epoch: 1 [19840/48200 (41%)]\tLoss: 0.603958\n",
      "Train Epoch: 1 [20480/48200 (42%)]\tLoss: 0.432337\n",
      "Train Epoch: 1 [21120/48200 (44%)]\tLoss: 0.499276\n",
      "Train Epoch: 1 [21760/48200 (45%)]\tLoss: 0.574944\n",
      "Train Epoch: 1 [22400/48200 (46%)]\tLoss: 0.585039\n",
      "Train Epoch: 1 [23040/48200 (48%)]\tLoss: 0.510873\n",
      "Train Epoch: 1 [23680/48200 (49%)]\tLoss: 0.355442\n",
      "Train Epoch: 1 [24320/48200 (50%)]\tLoss: 0.604939\n",
      "Train Epoch: 1 [24960/48200 (52%)]\tLoss: 0.544235\n",
      "Train Epoch: 1 [25600/48200 (53%)]\tLoss: 0.466225\n",
      "Train Epoch: 1 [26240/48200 (54%)]\tLoss: 0.614204\n",
      "Train Epoch: 1 [26880/48200 (56%)]\tLoss: 0.506875\n",
      "Train Epoch: 1 [27520/48200 (57%)]\tLoss: 0.401381\n",
      "Train Epoch: 1 [28160/48200 (58%)]\tLoss: 0.428894\n",
      "Train Epoch: 1 [28800/48200 (60%)]\tLoss: 0.424197\n",
      "Train Epoch: 1 [29440/48200 (61%)]\tLoss: 0.545836\n",
      "Train Epoch: 1 [30080/48200 (62%)]\tLoss: 0.444747\n",
      "Train Epoch: 1 [30720/48200 (64%)]\tLoss: 0.755102\n",
      "Train Epoch: 1 [31360/48200 (65%)]\tLoss: 0.493412\n",
      "Train Epoch: 1 [32000/48200 (66%)]\tLoss: 0.507261\n",
      "Train Epoch: 1 [32640/48200 (68%)]\tLoss: 0.272132\n",
      "Train Epoch: 1 [33280/48200 (69%)]\tLoss: 0.384716\n",
      "Train Epoch: 1 [33920/48200 (70%)]\tLoss: 0.568271\n",
      "Train Epoch: 1 [34560/48200 (72%)]\tLoss: 0.297456\n",
      "Train Epoch: 1 [35200/48200 (73%)]\tLoss: 0.559548\n",
      "Train Epoch: 1 [35840/48200 (74%)]\tLoss: 0.439964\n",
      "Train Epoch: 1 [36480/48200 (76%)]\tLoss: 0.360048\n",
      "Train Epoch: 1 [37120/48200 (77%)]\tLoss: 0.339736\n",
      "Train Epoch: 1 [37760/48200 (78%)]\tLoss: 0.435746\n",
      "Train Epoch: 1 [38400/48200 (80%)]\tLoss: 0.228631\n",
      "Train Epoch: 1 [39040/48200 (81%)]\tLoss: 0.457109\n",
      "Train Epoch: 1 [39680/48200 (82%)]\tLoss: 0.377885\n",
      "Train Epoch: 1 [40320/48200 (84%)]\tLoss: 0.371244\n",
      "Train Epoch: 1 [40960/48200 (85%)]\tLoss: 0.232714\n",
      "Train Epoch: 1 [41600/48200 (86%)]\tLoss: 0.248536\n",
      "Train Epoch: 1 [42240/48200 (88%)]\tLoss: 0.302662\n",
      "Train Epoch: 1 [42880/48200 (89%)]\tLoss: 0.216697\n",
      "Train Epoch: 1 [43520/48200 (90%)]\tLoss: 0.330641\n",
      "Train Epoch: 1 [44160/48200 (92%)]\tLoss: 0.301485\n",
      "Train Epoch: 1 [44800/48200 (93%)]\tLoss: 0.222462\n",
      "Train Epoch: 1 [45440/48200 (94%)]\tLoss: 0.410987\n",
      "Train Epoch: 1 [46080/48200 (95%)]\tLoss: 0.351986\n",
      "Train Epoch: 1 [46720/48200 (97%)]\tLoss: 0.253272\n",
      "Train Epoch: 1 [47360/48200 (98%)]\tLoss: 0.338484\n",
      "Train Epoch: 1 [48000/48200 (99%)]\tLoss: 0.447061\n",
      "\n",
      "Test set: Avg. loss: 0.1489, Accuracy: 7641/8017 (95%)\n",
      "\n",
      "Train Epoch: 2 [0/48200 (0%)]\tLoss: 0.323174\n",
      "Train Epoch: 2 [640/48200 (1%)]\tLoss: 0.354278\n",
      "Train Epoch: 2 [1280/48200 (3%)]\tLoss: 0.248883\n",
      "Train Epoch: 2 [1920/48200 (4%)]\tLoss: 0.166388\n",
      "Train Epoch: 2 [2560/48200 (5%)]\tLoss: 0.397040\n",
      "Train Epoch: 2 [3200/48200 (7%)]\tLoss: 0.218510\n",
      "Train Epoch: 2 [3840/48200 (8%)]\tLoss: 0.291416\n",
      "Train Epoch: 2 [4480/48200 (9%)]\tLoss: 0.398899\n",
      "Train Epoch: 2 [5120/48200 (11%)]\tLoss: 0.317256\n",
      "Train Epoch: 2 [5760/48200 (12%)]\tLoss: 0.236407\n",
      "Train Epoch: 2 [6400/48200 (13%)]\tLoss: 0.210772\n",
      "Train Epoch: 2 [7040/48200 (15%)]\tLoss: 0.453019\n",
      "Train Epoch: 2 [7680/48200 (16%)]\tLoss: 0.385828\n",
      "Train Epoch: 2 [8320/48200 (17%)]\tLoss: 0.132379\n",
      "Train Epoch: 2 [8960/48200 (19%)]\tLoss: 0.339033\n",
      "Train Epoch: 2 [9600/48200 (20%)]\tLoss: 0.320277\n",
      "Train Epoch: 2 [10240/48200 (21%)]\tLoss: 0.300362\n",
      "Train Epoch: 2 [10880/48200 (23%)]\tLoss: 0.436732\n",
      "Train Epoch: 2 [11520/48200 (24%)]\tLoss: 0.154764\n",
      "Train Epoch: 2 [12160/48200 (25%)]\tLoss: 0.415711\n",
      "Train Epoch: 2 [12800/48200 (27%)]\tLoss: 0.322257\n",
      "Train Epoch: 2 [13440/48200 (28%)]\tLoss: 0.134885\n",
      "Train Epoch: 2 [14080/48200 (29%)]\tLoss: 0.168188\n",
      "Train Epoch: 2 [14720/48200 (31%)]\tLoss: 0.390150\n",
      "Train Epoch: 2 [15360/48200 (32%)]\tLoss: 0.218124\n",
      "Train Epoch: 2 [16000/48200 (33%)]\tLoss: 0.366782\n",
      "Train Epoch: 2 [16640/48200 (34%)]\tLoss: 0.294357\n",
      "Train Epoch: 2 [17280/48200 (36%)]\tLoss: 0.350128\n",
      "Train Epoch: 2 [17920/48200 (37%)]\tLoss: 0.195998\n",
      "Train Epoch: 2 [18560/48200 (38%)]\tLoss: 0.256879\n",
      "Train Epoch: 2 [19200/48200 (40%)]\tLoss: 0.355263\n",
      "Train Epoch: 2 [19840/48200 (41%)]\tLoss: 0.345637\n",
      "Train Epoch: 2 [20480/48200 (42%)]\tLoss: 0.546193\n",
      "Train Epoch: 2 [21120/48200 (44%)]\tLoss: 0.317940\n",
      "Train Epoch: 2 [21760/48200 (45%)]\tLoss: 0.203955\n",
      "Train Epoch: 2 [22400/48200 (46%)]\tLoss: 0.427803\n",
      "Train Epoch: 2 [23040/48200 (48%)]\tLoss: 0.355619\n",
      "Train Epoch: 2 [23680/48200 (49%)]\tLoss: 0.182983\n",
      "Train Epoch: 2 [24320/48200 (50%)]\tLoss: 0.247618\n",
      "Train Epoch: 2 [24960/48200 (52%)]\tLoss: 0.484696\n",
      "Train Epoch: 2 [25600/48200 (53%)]\tLoss: 0.455800\n",
      "Train Epoch: 2 [26240/48200 (54%)]\tLoss: 0.234724\n",
      "Train Epoch: 2 [26880/48200 (56%)]\tLoss: 0.353174\n",
      "Train Epoch: 2 [27520/48200 (57%)]\tLoss: 0.147362\n",
      "Train Epoch: 2 [28160/48200 (58%)]\tLoss: 0.385863\n",
      "Train Epoch: 2 [28800/48200 (60%)]\tLoss: 0.215982\n",
      "Train Epoch: 2 [29440/48200 (61%)]\tLoss: 0.174910\n",
      "Train Epoch: 2 [30080/48200 (62%)]\tLoss: 0.187421\n",
      "Train Epoch: 2 [30720/48200 (64%)]\tLoss: 0.353949\n",
      "Train Epoch: 2 [31360/48200 (65%)]\tLoss: 0.216616\n",
      "Train Epoch: 2 [32000/48200 (66%)]\tLoss: 0.200897\n",
      "Train Epoch: 2 [32640/48200 (68%)]\tLoss: 0.162755\n",
      "Train Epoch: 2 [33280/48200 (69%)]\tLoss: 0.254369\n",
      "Train Epoch: 2 [33920/48200 (70%)]\tLoss: 0.403007\n",
      "Train Epoch: 2 [34560/48200 (72%)]\tLoss: 0.269047\n",
      "Train Epoch: 2 [35200/48200 (73%)]\tLoss: 0.221838\n",
      "Train Epoch: 2 [35840/48200 (74%)]\tLoss: 0.409778\n",
      "Train Epoch: 2 [36480/48200 (76%)]\tLoss: 0.407832\n",
      "Train Epoch: 2 [37120/48200 (77%)]\tLoss: 0.100093\n",
      "Train Epoch: 2 [37760/48200 (78%)]\tLoss: 0.233083\n",
      "Train Epoch: 2 [38400/48200 (80%)]\tLoss: 0.361730\n",
      "Train Epoch: 2 [39040/48200 (81%)]\tLoss: 0.200932\n",
      "Train Epoch: 2 [39680/48200 (82%)]\tLoss: 0.326824\n",
      "Train Epoch: 2 [40320/48200 (84%)]\tLoss: 0.236133\n",
      "Train Epoch: 2 [40960/48200 (85%)]\tLoss: 0.258823\n",
      "Train Epoch: 2 [41600/48200 (86%)]\tLoss: 0.212682\n",
      "Train Epoch: 2 [42240/48200 (88%)]\tLoss: 0.149424\n",
      "Train Epoch: 2 [42880/48200 (89%)]\tLoss: 0.272535\n",
      "Train Epoch: 2 [43520/48200 (90%)]\tLoss: 0.323660\n",
      "Train Epoch: 2 [44160/48200 (92%)]\tLoss: 0.340315\n",
      "Train Epoch: 2 [44800/48200 (93%)]\tLoss: 0.429923\n",
      "Train Epoch: 2 [45440/48200 (94%)]\tLoss: 0.121006\n",
      "Train Epoch: 2 [46080/48200 (95%)]\tLoss: 0.303956\n",
      "Train Epoch: 2 [46720/48200 (97%)]\tLoss: 0.163855\n",
      "Train Epoch: 2 [47360/48200 (98%)]\tLoss: 0.253763\n",
      "Train Epoch: 2 [48000/48200 (99%)]\tLoss: 0.205139\n",
      "\n",
      "Test set: Avg. loss: 0.0981, Accuracy: 7778/8017 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/48200 (0%)]\tLoss: 0.399992\n",
      "Train Epoch: 3 [640/48200 (1%)]\tLoss: 0.272651\n",
      "Train Epoch: 3 [1280/48200 (3%)]\tLoss: 0.229360\n",
      "Train Epoch: 3 [1920/48200 (4%)]\tLoss: 0.306168\n",
      "Train Epoch: 3 [2560/48200 (5%)]\tLoss: 0.129741\n",
      "Train Epoch: 3 [3200/48200 (7%)]\tLoss: 0.206076\n",
      "Train Epoch: 3 [3840/48200 (8%)]\tLoss: 0.415719\n",
      "Train Epoch: 3 [4480/48200 (9%)]\tLoss: 0.420944\n",
      "Train Epoch: 3 [5120/48200 (11%)]\tLoss: 0.177933\n",
      "Train Epoch: 3 [5760/48200 (12%)]\tLoss: 0.305710\n",
      "Train Epoch: 3 [6400/48200 (13%)]\tLoss: 0.272305\n",
      "Train Epoch: 3 [7040/48200 (15%)]\tLoss: 0.237585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [7680/48200 (16%)]\tLoss: 0.238202\n",
      "Train Epoch: 3 [8320/48200 (17%)]\tLoss: 0.204273\n",
      "Train Epoch: 3 [8960/48200 (19%)]\tLoss: 0.112339\n",
      "Train Epoch: 3 [9600/48200 (20%)]\tLoss: 0.372577\n",
      "Train Epoch: 3 [10240/48200 (21%)]\tLoss: 0.226432\n",
      "Train Epoch: 3 [10880/48200 (23%)]\tLoss: 0.115228\n",
      "Train Epoch: 3 [11520/48200 (24%)]\tLoss: 0.150101\n",
      "Train Epoch: 3 [12160/48200 (25%)]\tLoss: 0.263199\n",
      "Train Epoch: 3 [12800/48200 (27%)]\tLoss: 0.168046\n",
      "Train Epoch: 3 [13440/48200 (28%)]\tLoss: 0.157311\n",
      "Train Epoch: 3 [14080/48200 (29%)]\tLoss: 0.290823\n",
      "Train Epoch: 3 [14720/48200 (31%)]\tLoss: 0.270473\n",
      "Train Epoch: 3 [15360/48200 (32%)]\tLoss: 0.340872\n",
      "Train Epoch: 3 [16000/48200 (33%)]\tLoss: 0.186803\n",
      "Train Epoch: 3 [16640/48200 (34%)]\tLoss: 0.129556\n",
      "Train Epoch: 3 [17280/48200 (36%)]\tLoss: 0.213797\n",
      "Train Epoch: 3 [17920/48200 (37%)]\tLoss: 0.264308\n",
      "Train Epoch: 3 [18560/48200 (38%)]\tLoss: 0.169477\n",
      "Train Epoch: 3 [19200/48200 (40%)]\tLoss: 0.143088\n",
      "Train Epoch: 3 [19840/48200 (41%)]\tLoss: 0.320433\n",
      "Train Epoch: 3 [20480/48200 (42%)]\tLoss: 0.273388\n",
      "Train Epoch: 3 [21120/48200 (44%)]\tLoss: 0.147539\n",
      "Train Epoch: 3 [21760/48200 (45%)]\tLoss: 0.195806\n",
      "Train Epoch: 3 [22400/48200 (46%)]\tLoss: 0.133376\n",
      "Train Epoch: 3 [23040/48200 (48%)]\tLoss: 0.175974\n",
      "Train Epoch: 3 [23680/48200 (49%)]\tLoss: 0.112603\n",
      "Train Epoch: 3 [24320/48200 (50%)]\tLoss: 0.128946\n",
      "Train Epoch: 3 [24960/48200 (52%)]\tLoss: 0.235991\n",
      "Train Epoch: 3 [25600/48200 (53%)]\tLoss: 0.202287\n",
      "Train Epoch: 3 [26240/48200 (54%)]\tLoss: 0.134250\n",
      "Train Epoch: 3 [26880/48200 (56%)]\tLoss: 0.288794\n",
      "Train Epoch: 3 [27520/48200 (57%)]\tLoss: 0.359154\n",
      "Train Epoch: 3 [28160/48200 (58%)]\tLoss: 0.121038\n",
      "Train Epoch: 3 [28800/48200 (60%)]\tLoss: 0.633391\n",
      "Train Epoch: 3 [29440/48200 (61%)]\tLoss: 0.440531\n",
      "Train Epoch: 3 [30080/48200 (62%)]\tLoss: 0.247978\n",
      "Train Epoch: 3 [30720/48200 (64%)]\tLoss: 0.369785\n",
      "Train Epoch: 3 [31360/48200 (65%)]\tLoss: 0.092017\n",
      "Train Epoch: 3 [32000/48200 (66%)]\tLoss: 0.060308\n",
      "Train Epoch: 3 [32640/48200 (68%)]\tLoss: 0.216089\n",
      "Train Epoch: 3 [33280/48200 (69%)]\tLoss: 0.130905\n",
      "Train Epoch: 3 [33920/48200 (70%)]\tLoss: 0.157219\n",
      "Train Epoch: 3 [34560/48200 (72%)]\tLoss: 0.176096\n",
      "Train Epoch: 3 [35200/48200 (73%)]\tLoss: 0.247343\n",
      "Train Epoch: 3 [35840/48200 (74%)]\tLoss: 0.402680\n",
      "Train Epoch: 3 [36480/48200 (76%)]\tLoss: 0.117362\n",
      "Train Epoch: 3 [37120/48200 (77%)]\tLoss: 0.116862\n",
      "Train Epoch: 3 [37760/48200 (78%)]\tLoss: 0.210143\n",
      "Train Epoch: 3 [38400/48200 (80%)]\tLoss: 0.378541\n",
      "Train Epoch: 3 [39040/48200 (81%)]\tLoss: 0.256883\n",
      "Train Epoch: 3 [39680/48200 (82%)]\tLoss: 0.291593\n",
      "Train Epoch: 3 [40320/48200 (84%)]\tLoss: 0.344430\n",
      "Train Epoch: 3 [40960/48200 (85%)]\tLoss: 0.138233\n",
      "Train Epoch: 3 [41600/48200 (86%)]\tLoss: 0.141287\n",
      "Train Epoch: 3 [42240/48200 (88%)]\tLoss: 0.244725\n",
      "Train Epoch: 3 [42880/48200 (89%)]\tLoss: 0.073449\n",
      "Train Epoch: 3 [43520/48200 (90%)]\tLoss: 0.206679\n",
      "Train Epoch: 3 [44160/48200 (92%)]\tLoss: 0.144227\n",
      "Train Epoch: 3 [44800/48200 (93%)]\tLoss: 0.310460\n",
      "Train Epoch: 3 [45440/48200 (94%)]\tLoss: 0.366151\n",
      "Train Epoch: 3 [46080/48200 (95%)]\tLoss: 0.333815\n",
      "Train Epoch: 3 [46720/48200 (97%)]\tLoss: 0.138183\n",
      "Train Epoch: 3 [47360/48200 (98%)]\tLoss: 0.108869\n",
      "Train Epoch: 3 [48000/48200 (99%)]\tLoss: 0.132182\n",
      "\n",
      "Test set: Avg. loss: 0.0805, Accuracy: 7807/8017 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/48200 (0%)]\tLoss: 0.118352\n",
      "Train Epoch: 4 [640/48200 (1%)]\tLoss: 0.190738\n",
      "Train Epoch: 4 [1280/48200 (3%)]\tLoss: 0.191097\n",
      "Train Epoch: 4 [1920/48200 (4%)]\tLoss: 0.161166\n",
      "Train Epoch: 4 [2560/48200 (5%)]\tLoss: 0.314735\n",
      "Train Epoch: 4 [3200/48200 (7%)]\tLoss: 0.126862\n",
      "Train Epoch: 4 [3840/48200 (8%)]\tLoss: 0.171968\n",
      "Train Epoch: 4 [4480/48200 (9%)]\tLoss: 0.171187\n",
      "Train Epoch: 4 [5120/48200 (11%)]\tLoss: 0.157195\n",
      "Train Epoch: 4 [5760/48200 (12%)]\tLoss: 0.280166\n",
      "Train Epoch: 4 [6400/48200 (13%)]\tLoss: 0.150335\n",
      "Train Epoch: 4 [7040/48200 (15%)]\tLoss: 0.270069\n",
      "Train Epoch: 4 [7680/48200 (16%)]\tLoss: 0.471340\n",
      "Train Epoch: 4 [8320/48200 (17%)]\tLoss: 0.208607\n",
      "Train Epoch: 4 [8960/48200 (19%)]\tLoss: 0.179477\n",
      "Train Epoch: 4 [9600/48200 (20%)]\tLoss: 0.211719\n",
      "Train Epoch: 4 [10240/48200 (21%)]\tLoss: 0.094355\n",
      "Train Epoch: 4 [10880/48200 (23%)]\tLoss: 0.156873\n",
      "Train Epoch: 4 [11520/48200 (24%)]\tLoss: 0.153832\n",
      "Train Epoch: 4 [12160/48200 (25%)]\tLoss: 0.356248\n",
      "Train Epoch: 4 [12800/48200 (27%)]\tLoss: 0.162367\n",
      "Train Epoch: 4 [13440/48200 (28%)]\tLoss: 0.426901\n",
      "Train Epoch: 4 [14080/48200 (29%)]\tLoss: 0.209070\n",
      "Train Epoch: 4 [14720/48200 (31%)]\tLoss: 0.128152\n",
      "Train Epoch: 4 [15360/48200 (32%)]\tLoss: 0.240304\n",
      "Train Epoch: 4 [16000/48200 (33%)]\tLoss: 0.367592\n",
      "Train Epoch: 4 [16640/48200 (34%)]\tLoss: 0.195254\n",
      "Train Epoch: 4 [17280/48200 (36%)]\tLoss: 0.135404\n",
      "Train Epoch: 4 [17920/48200 (37%)]\tLoss: 0.257271\n",
      "Train Epoch: 4 [18560/48200 (38%)]\tLoss: 0.149297\n",
      "Train Epoch: 4 [19200/48200 (40%)]\tLoss: 0.137495\n",
      "Train Epoch: 4 [19840/48200 (41%)]\tLoss: 0.143699\n",
      "Train Epoch: 4 [20480/48200 (42%)]\tLoss: 0.097852\n",
      "Train Epoch: 4 [21120/48200 (44%)]\tLoss: 0.168934\n",
      "Train Epoch: 4 [21760/48200 (45%)]\tLoss: 0.200492\n",
      "Train Epoch: 4 [22400/48200 (46%)]\tLoss: 0.197109\n",
      "Train Epoch: 4 [23040/48200 (48%)]\tLoss: 0.273618\n",
      "Train Epoch: 4 [23680/48200 (49%)]\tLoss: 0.185830\n",
      "Train Epoch: 4 [24320/48200 (50%)]\tLoss: 0.177898\n",
      "Train Epoch: 4 [24960/48200 (52%)]\tLoss: 0.496138\n",
      "Train Epoch: 4 [25600/48200 (53%)]\tLoss: 0.210451\n",
      "Train Epoch: 4 [26240/48200 (54%)]\tLoss: 0.218814\n",
      "Train Epoch: 4 [26880/48200 (56%)]\tLoss: 0.612282\n",
      "Train Epoch: 4 [27520/48200 (57%)]\tLoss: 0.312825\n",
      "Train Epoch: 4 [28160/48200 (58%)]\tLoss: 0.190481\n",
      "Train Epoch: 4 [28800/48200 (60%)]\tLoss: 0.087406\n",
      "Train Epoch: 4 [29440/48200 (61%)]\tLoss: 0.184749\n",
      "Train Epoch: 4 [30080/48200 (62%)]\tLoss: 0.218152\n",
      "Train Epoch: 4 [30720/48200 (64%)]\tLoss: 0.249685\n",
      "Train Epoch: 4 [31360/48200 (65%)]\tLoss: 0.108382\n",
      "Train Epoch: 4 [32000/48200 (66%)]\tLoss: 0.251458\n",
      "Train Epoch: 4 [32640/48200 (68%)]\tLoss: 0.212876\n",
      "Train Epoch: 4 [33280/48200 (69%)]\tLoss: 0.162245\n",
      "Train Epoch: 4 [33920/48200 (70%)]\tLoss: 0.081375\n",
      "Train Epoch: 4 [34560/48200 (72%)]\tLoss: 0.131055\n",
      "Train Epoch: 4 [35200/48200 (73%)]\tLoss: 0.144670\n",
      "Train Epoch: 4 [35840/48200 (74%)]\tLoss: 0.058652\n",
      "Train Epoch: 4 [36480/48200 (76%)]\tLoss: 0.304656\n",
      "Train Epoch: 4 [37120/48200 (77%)]\tLoss: 0.140161\n",
      "Train Epoch: 4 [37760/48200 (78%)]\tLoss: 0.121390\n",
      "Train Epoch: 4 [38400/48200 (80%)]\tLoss: 0.062813\n",
      "Train Epoch: 4 [39040/48200 (81%)]\tLoss: 0.147723\n",
      "Train Epoch: 4 [39680/48200 (82%)]\tLoss: 0.187001\n",
      "Train Epoch: 4 [40320/48200 (84%)]\tLoss: 0.167115\n",
      "Train Epoch: 4 [40960/48200 (85%)]\tLoss: 0.128485\n",
      "Train Epoch: 4 [41600/48200 (86%)]\tLoss: 0.419909\n",
      "Train Epoch: 4 [42240/48200 (88%)]\tLoss: 0.217412\n",
      "Train Epoch: 4 [42880/48200 (89%)]\tLoss: 0.162152\n",
      "Train Epoch: 4 [43520/48200 (90%)]\tLoss: 0.206680\n",
      "Train Epoch: 4 [44160/48200 (92%)]\tLoss: 0.069479\n",
      "Train Epoch: 4 [44800/48200 (93%)]\tLoss: 0.165302\n",
      "Train Epoch: 4 [45440/48200 (94%)]\tLoss: 0.204444\n",
      "Train Epoch: 4 [46080/48200 (95%)]\tLoss: 0.143699\n",
      "Train Epoch: 4 [46720/48200 (97%)]\tLoss: 0.317344\n",
      "Train Epoch: 4 [47360/48200 (98%)]\tLoss: 0.147596\n",
      "Train Epoch: 4 [48000/48200 (99%)]\tLoss: 0.437624\n",
      "\n",
      "Test set: Avg. loss: 0.0661, Accuracy: 7835/8017 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/48200 (0%)]\tLoss: 0.161614\n",
      "Train Epoch: 5 [640/48200 (1%)]\tLoss: 0.149059\n",
      "Train Epoch: 5 [1280/48200 (3%)]\tLoss: 0.298157\n",
      "Train Epoch: 5 [1920/48200 (4%)]\tLoss: 0.145931\n",
      "Train Epoch: 5 [2560/48200 (5%)]\tLoss: 0.152272\n",
      "Train Epoch: 5 [3200/48200 (7%)]\tLoss: 0.370804\n",
      "Train Epoch: 5 [3840/48200 (8%)]\tLoss: 0.481439\n",
      "Train Epoch: 5 [4480/48200 (9%)]\tLoss: 0.150875\n",
      "Train Epoch: 5 [5120/48200 (11%)]\tLoss: 0.175330\n",
      "Train Epoch: 5 [5760/48200 (12%)]\tLoss: 0.134188\n",
      "Train Epoch: 5 [6400/48200 (13%)]\tLoss: 0.074730\n",
      "Train Epoch: 5 [7040/48200 (15%)]\tLoss: 0.103769\n",
      "Train Epoch: 5 [7680/48200 (16%)]\tLoss: 0.120175\n",
      "Train Epoch: 5 [8320/48200 (17%)]\tLoss: 0.505460\n",
      "Train Epoch: 5 [8960/48200 (19%)]\tLoss: 0.270367\n",
      "Train Epoch: 5 [9600/48200 (20%)]\tLoss: 0.107330\n",
      "Train Epoch: 5 [10240/48200 (21%)]\tLoss: 0.115815\n",
      "Train Epoch: 5 [10880/48200 (23%)]\tLoss: 0.141303\n",
      "Train Epoch: 5 [11520/48200 (24%)]\tLoss: 0.119370\n",
      "Train Epoch: 5 [12160/48200 (25%)]\tLoss: 0.300576\n",
      "Train Epoch: 5 [12800/48200 (27%)]\tLoss: 0.239046\n",
      "Train Epoch: 5 [13440/48200 (28%)]\tLoss: 0.185791\n",
      "Train Epoch: 5 [14080/48200 (29%)]\tLoss: 0.172847\n",
      "Train Epoch: 5 [14720/48200 (31%)]\tLoss: 0.239624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [15360/48200 (32%)]\tLoss: 0.130724\n",
      "Train Epoch: 5 [16000/48200 (33%)]\tLoss: 0.279688\n",
      "Train Epoch: 5 [16640/48200 (34%)]\tLoss: 0.179391\n",
      "Train Epoch: 5 [17280/48200 (36%)]\tLoss: 0.171943\n",
      "Train Epoch: 5 [17920/48200 (37%)]\tLoss: 0.193679\n",
      "Train Epoch: 5 [18560/48200 (38%)]\tLoss: 0.211494\n",
      "Train Epoch: 5 [19200/48200 (40%)]\tLoss: 0.176691\n",
      "Train Epoch: 5 [19840/48200 (41%)]\tLoss: 0.144760\n",
      "Train Epoch: 5 [20480/48200 (42%)]\tLoss: 0.235052\n",
      "Train Epoch: 5 [21120/48200 (44%)]\tLoss: 0.251301\n",
      "Train Epoch: 5 [21760/48200 (45%)]\tLoss: 0.065417\n",
      "Train Epoch: 5 [22400/48200 (46%)]\tLoss: 0.150386\n",
      "Train Epoch: 5 [23040/48200 (48%)]\tLoss: 0.189008\n",
      "Train Epoch: 5 [23680/48200 (49%)]\tLoss: 0.109398\n",
      "Train Epoch: 5 [24320/48200 (50%)]\tLoss: 0.247290\n",
      "Train Epoch: 5 [24960/48200 (52%)]\tLoss: 0.209570\n",
      "Train Epoch: 5 [25600/48200 (53%)]\tLoss: 0.096663\n",
      "Train Epoch: 5 [26240/48200 (54%)]\tLoss: 0.224310\n",
      "Train Epoch: 5 [26880/48200 (56%)]\tLoss: 0.101614\n",
      "Train Epoch: 5 [27520/48200 (57%)]\tLoss: 0.118140\n",
      "Train Epoch: 5 [28160/48200 (58%)]\tLoss: 0.104433\n",
      "Train Epoch: 5 [28800/48200 (60%)]\tLoss: 0.289248\n",
      "Train Epoch: 5 [29440/48200 (61%)]\tLoss: 0.296044\n",
      "Train Epoch: 5 [30080/48200 (62%)]\tLoss: 0.241915\n",
      "Train Epoch: 5 [30720/48200 (64%)]\tLoss: 0.423619\n",
      "Train Epoch: 5 [31360/48200 (65%)]\tLoss: 0.259227\n",
      "Train Epoch: 5 [32000/48200 (66%)]\tLoss: 0.058679\n",
      "Train Epoch: 5 [32640/48200 (68%)]\tLoss: 0.142529\n",
      "Train Epoch: 5 [33280/48200 (69%)]\tLoss: 0.149555\n",
      "Train Epoch: 5 [33920/48200 (70%)]\tLoss: 0.177661\n",
      "Train Epoch: 5 [34560/48200 (72%)]\tLoss: 0.310531\n",
      "Train Epoch: 5 [35200/48200 (73%)]\tLoss: 0.071262\n",
      "Train Epoch: 5 [35840/48200 (74%)]\tLoss: 0.136907\n",
      "Train Epoch: 5 [36480/48200 (76%)]\tLoss: 0.152330\n",
      "Train Epoch: 5 [37120/48200 (77%)]\tLoss: 0.091268\n",
      "Train Epoch: 5 [37760/48200 (78%)]\tLoss: 0.228161\n",
      "Train Epoch: 5 [38400/48200 (80%)]\tLoss: 0.239100\n",
      "Train Epoch: 5 [39040/48200 (81%)]\tLoss: 0.224595\n",
      "Train Epoch: 5 [39680/48200 (82%)]\tLoss: 0.126563\n",
      "Train Epoch: 5 [40320/48200 (84%)]\tLoss: 0.293960\n",
      "Train Epoch: 5 [40960/48200 (85%)]\tLoss: 0.122304\n",
      "Train Epoch: 5 [41600/48200 (86%)]\tLoss: 0.148056\n",
      "Train Epoch: 5 [42240/48200 (88%)]\tLoss: 0.194970\n",
      "Train Epoch: 5 [42880/48200 (89%)]\tLoss: 0.163118\n",
      "Train Epoch: 5 [43520/48200 (90%)]\tLoss: 0.146238\n",
      "Train Epoch: 5 [44160/48200 (92%)]\tLoss: 0.441091\n",
      "Train Epoch: 5 [44800/48200 (93%)]\tLoss: 0.070483\n",
      "Train Epoch: 5 [45440/48200 (94%)]\tLoss: 0.085692\n",
      "Train Epoch: 5 [46080/48200 (95%)]\tLoss: 0.274414\n",
      "Train Epoch: 5 [46720/48200 (97%)]\tLoss: 0.101319\n",
      "Train Epoch: 5 [47360/48200 (98%)]\tLoss: 0.191572\n",
      "Train Epoch: 5 [48000/48200 (99%)]\tLoss: 0.240339\n",
      "\n",
      "Test set: Avg. loss: 0.0601, Accuracy: 7859/8017 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/48200 (0%)]\tLoss: 0.045432\n",
      "Train Epoch: 6 [640/48200 (1%)]\tLoss: 0.173006\n",
      "Train Epoch: 6 [1280/48200 (3%)]\tLoss: 0.188601\n",
      "Train Epoch: 6 [1920/48200 (4%)]\tLoss: 0.267546\n",
      "Train Epoch: 6 [2560/48200 (5%)]\tLoss: 0.260779\n",
      "Train Epoch: 6 [3200/48200 (7%)]\tLoss: 0.115071\n",
      "Train Epoch: 6 [3840/48200 (8%)]\tLoss: 0.125421\n",
      "Train Epoch: 6 [4480/48200 (9%)]\tLoss: 0.102285\n",
      "Train Epoch: 6 [5120/48200 (11%)]\tLoss: 0.091904\n",
      "Train Epoch: 6 [5760/48200 (12%)]\tLoss: 0.231138\n",
      "Train Epoch: 6 [6400/48200 (13%)]\tLoss: 0.163605\n",
      "Train Epoch: 6 [7040/48200 (15%)]\tLoss: 0.081967\n",
      "Train Epoch: 6 [7680/48200 (16%)]\tLoss: 0.075602\n",
      "Train Epoch: 6 [8320/48200 (17%)]\tLoss: 0.265837\n",
      "Train Epoch: 6 [8960/48200 (19%)]\tLoss: 0.184473\n",
      "Train Epoch: 6 [9600/48200 (20%)]\tLoss: 0.108854\n",
      "Train Epoch: 6 [10240/48200 (21%)]\tLoss: 0.093712\n",
      "Train Epoch: 6 [10880/48200 (23%)]\tLoss: 0.081324\n",
      "Train Epoch: 6 [11520/48200 (24%)]\tLoss: 0.249854\n",
      "Train Epoch: 6 [12160/48200 (25%)]\tLoss: 0.100561\n",
      "Train Epoch: 6 [12800/48200 (27%)]\tLoss: 0.169328\n",
      "Train Epoch: 6 [13440/48200 (28%)]\tLoss: 0.206953\n",
      "Train Epoch: 6 [14080/48200 (29%)]\tLoss: 0.099078\n",
      "Train Epoch: 6 [14720/48200 (31%)]\tLoss: 0.097455\n",
      "Train Epoch: 6 [15360/48200 (32%)]\tLoss: 0.277362\n",
      "Train Epoch: 6 [16000/48200 (33%)]\tLoss: 0.142702\n",
      "Train Epoch: 6 [16640/48200 (34%)]\tLoss: 0.273207\n",
      "Train Epoch: 6 [17280/48200 (36%)]\tLoss: 0.195240\n",
      "Train Epoch: 6 [17920/48200 (37%)]\tLoss: 0.201494\n",
      "Train Epoch: 6 [18560/48200 (38%)]\tLoss: 0.066876\n",
      "Train Epoch: 6 [19200/48200 (40%)]\tLoss: 0.164911\n",
      "Train Epoch: 6 [19840/48200 (41%)]\tLoss: 0.200599\n",
      "Train Epoch: 6 [20480/48200 (42%)]\tLoss: 0.132437\n",
      "Train Epoch: 6 [21120/48200 (44%)]\tLoss: 0.062237\n",
      "Train Epoch: 6 [21760/48200 (45%)]\tLoss: 0.226228\n",
      "Train Epoch: 6 [22400/48200 (46%)]\tLoss: 0.173204\n",
      "Train Epoch: 6 [23040/48200 (48%)]\tLoss: 0.089752\n",
      "Train Epoch: 6 [23680/48200 (49%)]\tLoss: 0.103038\n",
      "Train Epoch: 6 [24320/48200 (50%)]\tLoss: 0.204897\n",
      "Train Epoch: 6 [24960/48200 (52%)]\tLoss: 0.046894\n",
      "Train Epoch: 6 [25600/48200 (53%)]\tLoss: 0.151488\n",
      "Train Epoch: 6 [26240/48200 (54%)]\tLoss: 0.176735\n",
      "Train Epoch: 6 [26880/48200 (56%)]\tLoss: 0.051804\n",
      "Train Epoch: 6 [27520/48200 (57%)]\tLoss: 0.063404\n",
      "Train Epoch: 6 [28160/48200 (58%)]\tLoss: 0.261634\n",
      "Train Epoch: 6 [28800/48200 (60%)]\tLoss: 0.091264\n",
      "Train Epoch: 6 [29440/48200 (61%)]\tLoss: 0.139442\n",
      "Train Epoch: 6 [30080/48200 (62%)]\tLoss: 0.198085\n",
      "Train Epoch: 6 [30720/48200 (64%)]\tLoss: 0.043056\n",
      "Train Epoch: 6 [31360/48200 (65%)]\tLoss: 0.174546\n",
      "Train Epoch: 6 [32000/48200 (66%)]\tLoss: 0.050334\n",
      "Train Epoch: 6 [32640/48200 (68%)]\tLoss: 0.106554\n",
      "Train Epoch: 6 [33280/48200 (69%)]\tLoss: 0.058265\n",
      "Train Epoch: 6 [33920/48200 (70%)]\tLoss: 0.145727\n",
      "Train Epoch: 6 [34560/48200 (72%)]\tLoss: 0.140916\n",
      "Train Epoch: 6 [35200/48200 (73%)]\tLoss: 0.064469\n",
      "Train Epoch: 6 [35840/48200 (74%)]\tLoss: 0.242116\n",
      "Train Epoch: 6 [36480/48200 (76%)]\tLoss: 0.216709\n",
      "Train Epoch: 6 [37120/48200 (77%)]\tLoss: 0.216465\n",
      "Train Epoch: 6 [37760/48200 (78%)]\tLoss: 0.204660\n",
      "Train Epoch: 6 [38400/48200 (80%)]\tLoss: 0.219785\n",
      "Train Epoch: 6 [39040/48200 (81%)]\tLoss: 0.495064\n",
      "Train Epoch: 6 [39680/48200 (82%)]\tLoss: 0.079773\n",
      "Train Epoch: 6 [40320/48200 (84%)]\tLoss: 0.148731\n",
      "Train Epoch: 6 [40960/48200 (85%)]\tLoss: 0.240375\n",
      "Train Epoch: 6 [41600/48200 (86%)]\tLoss: 0.185146\n",
      "Train Epoch: 6 [42240/48200 (88%)]\tLoss: 0.160939\n",
      "Train Epoch: 6 [42880/48200 (89%)]\tLoss: 0.061481\n",
      "Train Epoch: 6 [43520/48200 (90%)]\tLoss: 0.159520\n",
      "Train Epoch: 6 [44160/48200 (92%)]\tLoss: 0.146491\n",
      "Train Epoch: 6 [44800/48200 (93%)]\tLoss: 0.166413\n",
      "Train Epoch: 6 [45440/48200 (94%)]\tLoss: 0.322764\n",
      "Train Epoch: 6 [46080/48200 (95%)]\tLoss: 0.421789\n",
      "Train Epoch: 6 [46720/48200 (97%)]\tLoss: 0.219724\n",
      "Train Epoch: 6 [47360/48200 (98%)]\tLoss: 0.161654\n",
      "Train Epoch: 6 [48000/48200 (99%)]\tLoss: 0.140478\n",
      "\n",
      "Test set: Avg. loss: 0.0547, Accuracy: 7875/8017 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/48200 (0%)]\tLoss: 0.041839\n",
      "Train Epoch: 7 [640/48200 (1%)]\tLoss: 0.158753\n",
      "Train Epoch: 7 [1280/48200 (3%)]\tLoss: 0.166194\n",
      "Train Epoch: 7 [1920/48200 (4%)]\tLoss: 0.101463\n",
      "Train Epoch: 7 [2560/48200 (5%)]\tLoss: 0.092977\n",
      "Train Epoch: 7 [3200/48200 (7%)]\tLoss: 0.158628\n",
      "Train Epoch: 7 [3840/48200 (8%)]\tLoss: 0.217883\n",
      "Train Epoch: 7 [4480/48200 (9%)]\tLoss: 0.111355\n",
      "Train Epoch: 7 [5120/48200 (11%)]\tLoss: 0.191137\n",
      "Train Epoch: 7 [5760/48200 (12%)]\tLoss: 0.221944\n",
      "Train Epoch: 7 [6400/48200 (13%)]\tLoss: 0.375989\n",
      "Train Epoch: 7 [7040/48200 (15%)]\tLoss: 0.055798\n",
      "Train Epoch: 7 [7680/48200 (16%)]\tLoss: 0.031789\n",
      "Train Epoch: 7 [8320/48200 (17%)]\tLoss: 0.263036\n",
      "Train Epoch: 7 [8960/48200 (19%)]\tLoss: 0.150406\n",
      "Train Epoch: 7 [9600/48200 (20%)]\tLoss: 0.157336\n",
      "Train Epoch: 7 [10240/48200 (21%)]\tLoss: 0.201112\n",
      "Train Epoch: 7 [10880/48200 (23%)]\tLoss: 0.027742\n",
      "Train Epoch: 7 [11520/48200 (24%)]\tLoss: 0.077591\n",
      "Train Epoch: 7 [12160/48200 (25%)]\tLoss: 0.126951\n",
      "Train Epoch: 7 [12800/48200 (27%)]\tLoss: 0.280003\n",
      "Train Epoch: 7 [13440/48200 (28%)]\tLoss: 0.242744\n",
      "Train Epoch: 7 [14080/48200 (29%)]\tLoss: 0.168359\n",
      "Train Epoch: 7 [14720/48200 (31%)]\tLoss: 0.227230\n",
      "Train Epoch: 7 [15360/48200 (32%)]\tLoss: 0.326746\n",
      "Train Epoch: 7 [16000/48200 (33%)]\tLoss: 0.115852\n",
      "Train Epoch: 7 [16640/48200 (34%)]\tLoss: 0.298790\n",
      "Train Epoch: 7 [17280/48200 (36%)]\tLoss: 0.287936\n",
      "Train Epoch: 7 [17920/48200 (37%)]\tLoss: 0.112122\n",
      "Train Epoch: 7 [18560/48200 (38%)]\tLoss: 0.127850\n",
      "Train Epoch: 7 [19200/48200 (40%)]\tLoss: 0.203548\n",
      "Train Epoch: 7 [19840/48200 (41%)]\tLoss: 0.149229\n",
      "Train Epoch: 7 [20480/48200 (42%)]\tLoss: 0.557801\n",
      "Train Epoch: 7 [21120/48200 (44%)]\tLoss: 0.115822\n",
      "Train Epoch: 7 [21760/48200 (45%)]\tLoss: 0.050280\n",
      "Train Epoch: 7 [22400/48200 (46%)]\tLoss: 0.182672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [23040/48200 (48%)]\tLoss: 0.189784\n",
      "Train Epoch: 7 [23680/48200 (49%)]\tLoss: 0.285413\n",
      "Train Epoch: 7 [24320/48200 (50%)]\tLoss: 0.151739\n",
      "Train Epoch: 7 [24960/48200 (52%)]\tLoss: 0.093503\n",
      "Train Epoch: 7 [25600/48200 (53%)]\tLoss: 0.283423\n",
      "Train Epoch: 7 [26240/48200 (54%)]\tLoss: 0.151320\n",
      "Train Epoch: 7 [26880/48200 (56%)]\tLoss: 0.104477\n",
      "Train Epoch: 7 [27520/48200 (57%)]\tLoss: 0.118996\n",
      "Train Epoch: 7 [28160/48200 (58%)]\tLoss: 0.216587\n",
      "Train Epoch: 7 [28800/48200 (60%)]\tLoss: 0.110020\n",
      "Train Epoch: 7 [29440/48200 (61%)]\tLoss: 0.096755\n",
      "Train Epoch: 7 [30080/48200 (62%)]\tLoss: 0.170507\n",
      "Train Epoch: 7 [30720/48200 (64%)]\tLoss: 0.098796\n",
      "Train Epoch: 7 [31360/48200 (65%)]\tLoss: 0.112937\n",
      "Train Epoch: 7 [32000/48200 (66%)]\tLoss: 0.211110\n",
      "Train Epoch: 7 [32640/48200 (68%)]\tLoss: 0.178929\n",
      "Train Epoch: 7 [33280/48200 (69%)]\tLoss: 0.415125\n",
      "Train Epoch: 7 [33920/48200 (70%)]\tLoss: 0.129206\n",
      "Train Epoch: 7 [34560/48200 (72%)]\tLoss: 0.113458\n",
      "Train Epoch: 7 [35200/48200 (73%)]\tLoss: 0.105352\n",
      "Train Epoch: 7 [35840/48200 (74%)]\tLoss: 0.131477\n",
      "Train Epoch: 7 [36480/48200 (76%)]\tLoss: 0.167981\n",
      "Train Epoch: 7 [37120/48200 (77%)]\tLoss: 0.142083\n",
      "Train Epoch: 7 [37760/48200 (78%)]\tLoss: 0.061652\n",
      "Train Epoch: 7 [38400/48200 (80%)]\tLoss: 0.143534\n",
      "Train Epoch: 7 [39040/48200 (81%)]\tLoss: 0.145656\n",
      "Train Epoch: 7 [39680/48200 (82%)]\tLoss: 0.045793\n",
      "Train Epoch: 7 [40320/48200 (84%)]\tLoss: 0.178142\n",
      "Train Epoch: 7 [40960/48200 (85%)]\tLoss: 0.093971\n",
      "Train Epoch: 7 [41600/48200 (86%)]\tLoss: 0.147170\n",
      "Train Epoch: 7 [42240/48200 (88%)]\tLoss: 0.156735\n",
      "Train Epoch: 7 [42880/48200 (89%)]\tLoss: 0.297336\n",
      "Train Epoch: 7 [43520/48200 (90%)]\tLoss: 0.183085\n",
      "Train Epoch: 7 [44160/48200 (92%)]\tLoss: 0.072125\n",
      "Train Epoch: 7 [44800/48200 (93%)]\tLoss: 0.271674\n",
      "Train Epoch: 7 [45440/48200 (94%)]\tLoss: 0.093008\n",
      "Train Epoch: 7 [46080/48200 (95%)]\tLoss: 0.073395\n",
      "Train Epoch: 7 [46720/48200 (97%)]\tLoss: 0.102944\n",
      "Train Epoch: 7 [47360/48200 (98%)]\tLoss: 0.125981\n",
      "Train Epoch: 7 [48000/48200 (99%)]\tLoss: 0.125934\n",
      "\n",
      "Test set: Avg. loss: 0.0488, Accuracy: 7889/8017 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/48200 (0%)]\tLoss: 0.263384\n",
      "Train Epoch: 8 [640/48200 (1%)]\tLoss: 0.143792\n",
      "Train Epoch: 8 [1280/48200 (3%)]\tLoss: 0.044900\n",
      "Train Epoch: 8 [1920/48200 (4%)]\tLoss: 0.114223\n",
      "Train Epoch: 8 [2560/48200 (5%)]\tLoss: 0.103471\n",
      "Train Epoch: 8 [3200/48200 (7%)]\tLoss: 0.152213\n",
      "Train Epoch: 8 [3840/48200 (8%)]\tLoss: 0.074526\n",
      "Train Epoch: 8 [4480/48200 (9%)]\tLoss: 0.149352\n",
      "Train Epoch: 8 [5120/48200 (11%)]\tLoss: 0.345875\n",
      "Train Epoch: 8 [5760/48200 (12%)]\tLoss: 0.130299\n",
      "Train Epoch: 8 [6400/48200 (13%)]\tLoss: 0.121658\n",
      "Train Epoch: 8 [7040/48200 (15%)]\tLoss: 0.118288\n",
      "Train Epoch: 8 [7680/48200 (16%)]\tLoss: 0.097875\n",
      "Train Epoch: 8 [8320/48200 (17%)]\tLoss: 0.050516\n",
      "Train Epoch: 8 [8960/48200 (19%)]\tLoss: 0.194438\n",
      "Train Epoch: 8 [9600/48200 (20%)]\tLoss: 0.086100\n",
      "Train Epoch: 8 [10240/48200 (21%)]\tLoss: 0.100639\n",
      "Train Epoch: 8 [10880/48200 (23%)]\tLoss: 0.151115\n",
      "Train Epoch: 8 [11520/48200 (24%)]\tLoss: 0.144435\n",
      "Train Epoch: 8 [12160/48200 (25%)]\tLoss: 0.127333\n",
      "Train Epoch: 8 [12800/48200 (27%)]\tLoss: 0.174273\n",
      "Train Epoch: 8 [13440/48200 (28%)]\tLoss: 0.086231\n",
      "Train Epoch: 8 [14080/48200 (29%)]\tLoss: 0.167111\n",
      "Train Epoch: 8 [14720/48200 (31%)]\tLoss: 0.118231\n",
      "Train Epoch: 8 [15360/48200 (32%)]\tLoss: 0.111653\n",
      "Train Epoch: 8 [16000/48200 (33%)]\tLoss: 0.477348\n",
      "Train Epoch: 8 [16640/48200 (34%)]\tLoss: 0.084855\n",
      "Train Epoch: 8 [17280/48200 (36%)]\tLoss: 0.233669\n",
      "Train Epoch: 8 [17920/48200 (37%)]\tLoss: 0.235438\n",
      "Train Epoch: 8 [18560/48200 (38%)]\tLoss: 0.078005\n",
      "Train Epoch: 8 [19200/48200 (40%)]\tLoss: 0.179995\n",
      "Train Epoch: 8 [19840/48200 (41%)]\tLoss: 0.144593\n",
      "Train Epoch: 8 [20480/48200 (42%)]\tLoss: 0.245488\n",
      "Train Epoch: 8 [21120/48200 (44%)]\tLoss: 0.148878\n",
      "Train Epoch: 8 [21760/48200 (45%)]\tLoss: 0.150398\n",
      "Train Epoch: 8 [22400/48200 (46%)]\tLoss: 0.220386\n",
      "Train Epoch: 8 [23040/48200 (48%)]\tLoss: 0.096802\n",
      "Train Epoch: 8 [23680/48200 (49%)]\tLoss: 0.201011\n",
      "Train Epoch: 8 [24320/48200 (50%)]\tLoss: 0.156519\n",
      "Train Epoch: 8 [24960/48200 (52%)]\tLoss: 0.093583\n",
      "Train Epoch: 8 [25600/48200 (53%)]\tLoss: 0.065848\n",
      "Train Epoch: 8 [26240/48200 (54%)]\tLoss: 0.130861\n",
      "Train Epoch: 8 [26880/48200 (56%)]\tLoss: 0.215656\n",
      "Train Epoch: 8 [27520/48200 (57%)]\tLoss: 0.058210\n",
      "Train Epoch: 8 [28160/48200 (58%)]\tLoss: 0.134147\n",
      "Train Epoch: 8 [28800/48200 (60%)]\tLoss: 0.207924\n",
      "Train Epoch: 8 [29440/48200 (61%)]\tLoss: 0.139224\n",
      "Train Epoch: 8 [30080/48200 (62%)]\tLoss: 0.123215\n",
      "Train Epoch: 8 [30720/48200 (64%)]\tLoss: 0.127994\n",
      "Train Epoch: 8 [31360/48200 (65%)]\tLoss: 0.126330\n",
      "Train Epoch: 8 [32000/48200 (66%)]\tLoss: 0.093767\n",
      "Train Epoch: 8 [32640/48200 (68%)]\tLoss: 0.199658\n",
      "Train Epoch: 8 [33280/48200 (69%)]\tLoss: 0.062292\n",
      "Train Epoch: 8 [33920/48200 (70%)]\tLoss: 0.067371\n",
      "Train Epoch: 8 [34560/48200 (72%)]\tLoss: 0.075821\n",
      "Train Epoch: 8 [35200/48200 (73%)]\tLoss: 0.086992\n",
      "Train Epoch: 8 [35840/48200 (74%)]\tLoss: 0.199298\n",
      "Train Epoch: 8 [36480/48200 (76%)]\tLoss: 0.072128\n",
      "Train Epoch: 8 [37120/48200 (77%)]\tLoss: 0.116832\n",
      "Train Epoch: 8 [37760/48200 (78%)]\tLoss: 0.127776\n",
      "Train Epoch: 8 [38400/48200 (80%)]\tLoss: 0.149542\n",
      "Train Epoch: 8 [39040/48200 (81%)]\tLoss: 0.049944\n",
      "Train Epoch: 8 [39680/48200 (82%)]\tLoss: 0.122182\n",
      "Train Epoch: 8 [40320/48200 (84%)]\tLoss: 0.176117\n",
      "Train Epoch: 8 [40960/48200 (85%)]\tLoss: 0.194658\n",
      "Train Epoch: 8 [41600/48200 (86%)]\tLoss: 0.087268\n",
      "Train Epoch: 8 [42240/48200 (88%)]\tLoss: 0.245347\n",
      "Train Epoch: 8 [42880/48200 (89%)]\tLoss: 0.189454\n",
      "Train Epoch: 8 [43520/48200 (90%)]\tLoss: 0.094248\n",
      "Train Epoch: 8 [44160/48200 (92%)]\tLoss: 0.064910\n",
      "Train Epoch: 8 [44800/48200 (93%)]\tLoss: 0.306987\n",
      "Train Epoch: 8 [45440/48200 (94%)]\tLoss: 0.108627\n",
      "Train Epoch: 8 [46080/48200 (95%)]\tLoss: 0.075387\n",
      "Train Epoch: 8 [46720/48200 (97%)]\tLoss: 0.144678\n",
      "Train Epoch: 8 [47360/48200 (98%)]\tLoss: 0.101150\n",
      "Train Epoch: 8 [48000/48200 (99%)]\tLoss: 0.026396\n",
      "\n",
      "Test set: Avg. loss: 0.0475, Accuracy: 7902/8017 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/48200 (0%)]\tLoss: 0.121283\n",
      "Train Epoch: 9 [640/48200 (1%)]\tLoss: 0.154893\n",
      "Train Epoch: 9 [1280/48200 (3%)]\tLoss: 0.153727\n",
      "Train Epoch: 9 [1920/48200 (4%)]\tLoss: 0.109356\n",
      "Train Epoch: 9 [2560/48200 (5%)]\tLoss: 0.080958\n",
      "Train Epoch: 9 [3200/48200 (7%)]\tLoss: 0.115717\n",
      "Train Epoch: 9 [3840/48200 (8%)]\tLoss: 0.043683\n",
      "Train Epoch: 9 [4480/48200 (9%)]\tLoss: 0.068365\n",
      "Train Epoch: 9 [5120/48200 (11%)]\tLoss: 0.019906\n",
      "Train Epoch: 9 [5760/48200 (12%)]\tLoss: 0.186582\n",
      "Train Epoch: 9 [6400/48200 (13%)]\tLoss: 0.167679\n",
      "Train Epoch: 9 [7040/48200 (15%)]\tLoss: 0.033079\n",
      "Train Epoch: 9 [7680/48200 (16%)]\tLoss: 0.057972\n",
      "Train Epoch: 9 [8320/48200 (17%)]\tLoss: 0.251730\n",
      "Train Epoch: 9 [8960/48200 (19%)]\tLoss: 0.083830\n",
      "Train Epoch: 9 [9600/48200 (20%)]\tLoss: 0.166611\n",
      "Train Epoch: 9 [10240/48200 (21%)]\tLoss: 0.118204\n",
      "Train Epoch: 9 [10880/48200 (23%)]\tLoss: 0.044250\n",
      "Train Epoch: 9 [11520/48200 (24%)]\tLoss: 0.362694\n",
      "Train Epoch: 9 [12160/48200 (25%)]\tLoss: 0.247644\n",
      "Train Epoch: 9 [12800/48200 (27%)]\tLoss: 0.309233\n",
      "Train Epoch: 9 [13440/48200 (28%)]\tLoss: 0.095092\n",
      "Train Epoch: 9 [14080/48200 (29%)]\tLoss: 0.130964\n",
      "Train Epoch: 9 [14720/48200 (31%)]\tLoss: 0.137179\n",
      "Train Epoch: 9 [15360/48200 (32%)]\tLoss: 0.209255\n",
      "Train Epoch: 9 [16000/48200 (33%)]\tLoss: 0.095217\n",
      "Train Epoch: 9 [16640/48200 (34%)]\tLoss: 0.195468\n",
      "Train Epoch: 9 [17280/48200 (36%)]\tLoss: 0.133416\n",
      "Train Epoch: 9 [17920/48200 (37%)]\tLoss: 0.059929\n",
      "Train Epoch: 9 [18560/48200 (38%)]\tLoss: 0.272553\n",
      "Train Epoch: 9 [19200/48200 (40%)]\tLoss: 0.085575\n",
      "Train Epoch: 9 [19840/48200 (41%)]\tLoss: 0.202050\n",
      "Train Epoch: 9 [20480/48200 (42%)]\tLoss: 0.120396\n",
      "Train Epoch: 9 [21120/48200 (44%)]\tLoss: 0.040328\n",
      "Train Epoch: 9 [21760/48200 (45%)]\tLoss: 0.120288\n",
      "Train Epoch: 9 [22400/48200 (46%)]\tLoss: 0.048371\n",
      "Train Epoch: 9 [23040/48200 (48%)]\tLoss: 0.057922\n",
      "Train Epoch: 9 [23680/48200 (49%)]\tLoss: 0.134118\n",
      "Train Epoch: 9 [24320/48200 (50%)]\tLoss: 0.085292\n",
      "Train Epoch: 9 [24960/48200 (52%)]\tLoss: 0.148130\n",
      "Train Epoch: 9 [25600/48200 (53%)]\tLoss: 0.186601\n",
      "Train Epoch: 9 [26240/48200 (54%)]\tLoss: 0.086885\n",
      "Train Epoch: 9 [26880/48200 (56%)]\tLoss: 0.161303\n",
      "Train Epoch: 9 [27520/48200 (57%)]\tLoss: 0.189675\n",
      "Train Epoch: 9 [28160/48200 (58%)]\tLoss: 0.342631\n",
      "Train Epoch: 9 [28800/48200 (60%)]\tLoss: 0.083417\n",
      "Train Epoch: 9 [29440/48200 (61%)]\tLoss: 0.123517\n",
      "Train Epoch: 9 [30080/48200 (62%)]\tLoss: 0.133558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [30720/48200 (64%)]\tLoss: 0.111690\n",
      "Train Epoch: 9 [31360/48200 (65%)]\tLoss: 0.044955\n",
      "Train Epoch: 9 [32000/48200 (66%)]\tLoss: 0.263454\n",
      "Train Epoch: 9 [32640/48200 (68%)]\tLoss: 0.073768\n",
      "Train Epoch: 9 [33280/48200 (69%)]\tLoss: 0.128747\n",
      "Train Epoch: 9 [33920/48200 (70%)]\tLoss: 0.089640\n",
      "Train Epoch: 9 [34560/48200 (72%)]\tLoss: 0.579141\n",
      "Train Epoch: 9 [35200/48200 (73%)]\tLoss: 0.162319\n",
      "Train Epoch: 9 [35840/48200 (74%)]\tLoss: 0.213115\n",
      "Train Epoch: 9 [36480/48200 (76%)]\tLoss: 0.061674\n",
      "Train Epoch: 9 [37120/48200 (77%)]\tLoss: 0.081147\n",
      "Train Epoch: 9 [37760/48200 (78%)]\tLoss: 0.044464\n",
      "Train Epoch: 9 [38400/48200 (80%)]\tLoss: 0.104212\n",
      "Train Epoch: 9 [39040/48200 (81%)]\tLoss: 0.129581\n",
      "Train Epoch: 9 [39680/48200 (82%)]\tLoss: 0.106596\n",
      "Train Epoch: 9 [40320/48200 (84%)]\tLoss: 0.047931\n",
      "Train Epoch: 9 [40960/48200 (85%)]\tLoss: 0.074869\n",
      "Train Epoch: 9 [41600/48200 (86%)]\tLoss: 0.035494\n",
      "Train Epoch: 9 [42240/48200 (88%)]\tLoss: 0.052306\n",
      "Train Epoch: 9 [42880/48200 (89%)]\tLoss: 0.124216\n",
      "Train Epoch: 9 [43520/48200 (90%)]\tLoss: 0.060908\n",
      "Train Epoch: 9 [44160/48200 (92%)]\tLoss: 0.117909\n",
      "Train Epoch: 9 [44800/48200 (93%)]\tLoss: 0.174113\n",
      "Train Epoch: 9 [45440/48200 (94%)]\tLoss: 0.058604\n",
      "Train Epoch: 9 [46080/48200 (95%)]\tLoss: 0.203883\n",
      "Train Epoch: 9 [46720/48200 (97%)]\tLoss: 0.074199\n",
      "Train Epoch: 9 [47360/48200 (98%)]\tLoss: 0.149097\n",
      "Train Epoch: 9 [48000/48200 (99%)]\tLoss: 0.075623\n",
      "\n",
      "Test set: Avg. loss: 0.0439, Accuracy: 7906/8017 (98%)\n",
      "\n",
      "Train Epoch: 10 [0/48200 (0%)]\tLoss: 0.111935\n",
      "Train Epoch: 10 [640/48200 (1%)]\tLoss: 0.180461\n",
      "Train Epoch: 10 [1280/48200 (3%)]\tLoss: 0.187565\n",
      "Train Epoch: 10 [1920/48200 (4%)]\tLoss: 0.203701\n",
      "Train Epoch: 10 [2560/48200 (5%)]\tLoss: 0.084508\n",
      "Train Epoch: 10 [3200/48200 (7%)]\tLoss: 0.126757\n",
      "Train Epoch: 10 [3840/48200 (8%)]\tLoss: 0.069765\n",
      "Train Epoch: 10 [4480/48200 (9%)]\tLoss: 0.080528\n",
      "Train Epoch: 10 [5120/48200 (11%)]\tLoss: 0.058373\n",
      "Train Epoch: 10 [5760/48200 (12%)]\tLoss: 0.040314\n",
      "Train Epoch: 10 [6400/48200 (13%)]\tLoss: 0.158507\n",
      "Train Epoch: 10 [7040/48200 (15%)]\tLoss: 0.080071\n",
      "Train Epoch: 10 [7680/48200 (16%)]\tLoss: 0.055616\n",
      "Train Epoch: 10 [8320/48200 (17%)]\tLoss: 0.181940\n",
      "Train Epoch: 10 [8960/48200 (19%)]\tLoss: 0.062371\n",
      "Train Epoch: 10 [9600/48200 (20%)]\tLoss: 0.156104\n",
      "Train Epoch: 10 [10240/48200 (21%)]\tLoss: 0.099517\n",
      "Train Epoch: 10 [10880/48200 (23%)]\tLoss: 0.042272\n",
      "Train Epoch: 10 [11520/48200 (24%)]\tLoss: 0.116458\n",
      "Train Epoch: 10 [12160/48200 (25%)]\tLoss: 0.137423\n",
      "Train Epoch: 10 [12800/48200 (27%)]\tLoss: 0.038759\n",
      "Train Epoch: 10 [13440/48200 (28%)]\tLoss: 0.096938\n",
      "Train Epoch: 10 [14080/48200 (29%)]\tLoss: 0.226721\n",
      "Train Epoch: 10 [14720/48200 (31%)]\tLoss: 0.265814\n",
      "Train Epoch: 10 [15360/48200 (32%)]\tLoss: 0.040305\n",
      "Train Epoch: 10 [16000/48200 (33%)]\tLoss: 0.342428\n",
      "Train Epoch: 10 [16640/48200 (34%)]\tLoss: 0.260709\n",
      "Train Epoch: 10 [17280/48200 (36%)]\tLoss: 0.113911\n",
      "Train Epoch: 10 [17920/48200 (37%)]\tLoss: 0.046669\n",
      "Train Epoch: 10 [18560/48200 (38%)]\tLoss: 0.046771\n",
      "Train Epoch: 10 [19200/48200 (40%)]\tLoss: 0.120545\n",
      "Train Epoch: 10 [19840/48200 (41%)]\tLoss: 0.057522\n",
      "Train Epoch: 10 [20480/48200 (42%)]\tLoss: 0.086169\n",
      "Train Epoch: 10 [21120/48200 (44%)]\tLoss: 0.084615\n",
      "Train Epoch: 10 [21760/48200 (45%)]\tLoss: 0.175391\n",
      "Train Epoch: 10 [22400/48200 (46%)]\tLoss: 0.100355\n",
      "Train Epoch: 10 [23040/48200 (48%)]\tLoss: 0.096386\n",
      "Train Epoch: 10 [23680/48200 (49%)]\tLoss: 0.030552\n",
      "Train Epoch: 10 [24320/48200 (50%)]\tLoss: 0.188395\n",
      "Train Epoch: 10 [24960/48200 (52%)]\tLoss: 0.104771\n",
      "Train Epoch: 10 [25600/48200 (53%)]\tLoss: 0.051044\n",
      "Train Epoch: 10 [26240/48200 (54%)]\tLoss: 0.093244\n",
      "Train Epoch: 10 [26880/48200 (56%)]\tLoss: 0.198946\n",
      "Train Epoch: 10 [27520/48200 (57%)]\tLoss: 0.099015\n",
      "Train Epoch: 10 [28160/48200 (58%)]\tLoss: 0.052080\n",
      "Train Epoch: 10 [28800/48200 (60%)]\tLoss: 0.064058\n",
      "Train Epoch: 10 [29440/48200 (61%)]\tLoss: 0.045097\n",
      "Train Epoch: 10 [30080/48200 (62%)]\tLoss: 0.246440\n",
      "Train Epoch: 10 [30720/48200 (64%)]\tLoss: 0.232736\n",
      "Train Epoch: 10 [31360/48200 (65%)]\tLoss: 0.088140\n",
      "Train Epoch: 10 [32000/48200 (66%)]\tLoss: 0.295989\n",
      "Train Epoch: 10 [32640/48200 (68%)]\tLoss: 0.074365\n",
      "Train Epoch: 10 [33280/48200 (69%)]\tLoss: 0.140727\n",
      "Train Epoch: 10 [33920/48200 (70%)]\tLoss: 0.060969\n",
      "Train Epoch: 10 [34560/48200 (72%)]\tLoss: 0.118065\n",
      "Train Epoch: 10 [35200/48200 (73%)]\tLoss: 0.022774\n",
      "Train Epoch: 10 [35840/48200 (74%)]\tLoss: 0.035563\n",
      "Train Epoch: 10 [36480/48200 (76%)]\tLoss: 0.199828\n",
      "Train Epoch: 10 [37120/48200 (77%)]\tLoss: 0.133822\n",
      "Train Epoch: 10 [37760/48200 (78%)]\tLoss: 0.132255\n",
      "Train Epoch: 10 [38400/48200 (80%)]\tLoss: 0.082939\n",
      "Train Epoch: 10 [39040/48200 (81%)]\tLoss: 0.050354\n",
      "Train Epoch: 10 [39680/48200 (82%)]\tLoss: 0.061130\n",
      "Train Epoch: 10 [40320/48200 (84%)]\tLoss: 0.053676\n",
      "Train Epoch: 10 [40960/48200 (85%)]\tLoss: 0.216496\n",
      "Train Epoch: 10 [41600/48200 (86%)]\tLoss: 0.086198\n",
      "Train Epoch: 10 [42240/48200 (88%)]\tLoss: 0.174028\n",
      "Train Epoch: 10 [42880/48200 (89%)]\tLoss: 0.162355\n",
      "Train Epoch: 10 [43520/48200 (90%)]\tLoss: 0.114042\n",
      "Train Epoch: 10 [44160/48200 (92%)]\tLoss: 0.124129\n",
      "Train Epoch: 10 [44800/48200 (93%)]\tLoss: 0.053903\n",
      "Train Epoch: 10 [45440/48200 (94%)]\tLoss: 0.135826\n",
      "Train Epoch: 10 [46080/48200 (95%)]\tLoss: 0.119422\n",
      "Train Epoch: 10 [46720/48200 (97%)]\tLoss: 0.088617\n",
      "Train Epoch: 10 [47360/48200 (98%)]\tLoss: 0.117675\n",
      "Train Epoch: 10 [48000/48200 (99%)]\tLoss: 0.074009\n",
      "\n",
      "Test set: Avg. loss: 0.0422, Accuracy: 7911/8017 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net_c8 = Net_c8().cuda()\n",
    "n_epochs = 10\n",
    "optimizer = optim.SGD(net_c8.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader8.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "test(net_c8, test_loader8)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(net_c8, train_loader8, epoch)\n",
    "    test(net_c8, test_loader8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naiqi/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.00704178, 0.00352089, 0.01408356, 0.02464624, 0.01056267,\n",
       "        0.01408356, 0.03168802, 0.03520891, 0.05281336, 0.07393871,\n",
       "        0.08450138, 0.09858495, 0.1161894 , 0.13027296, 0.11266851,\n",
       "        0.13027296, 0.22181613, 0.21829524, 0.21125346, 0.20773256,\n",
       "        0.24646236, 0.20069078, 0.22885791, 0.22885791, 0.17956544,\n",
       "        0.18308633, 0.25350415, 0.20421167, 0.25350415, 0.21829524,\n",
       "        0.21477435, 0.23589969, 0.23589969, 0.22885791, 0.13731475,\n",
       "        0.193649  , 0.15139831, 0.10210584, 0.16548187, 0.12323118,\n",
       "        0.17252366, 0.06689693, 0.10562673, 0.10210584, 0.05281336,\n",
       "        0.08450138, 0.05985515, 0.05281336, 0.07393871, 0.06689693,\n",
       "        0.04929247, 0.02464624, 0.02112535, 0.01760445, 0.02112535,\n",
       "        0.03520891, 0.02464624, 0.01056267, 0.01056267, 0.02464624,\n",
       "        0.02816713, 0.02112535, 0.01408356, 0.01408356, 0.01056267,\n",
       "        0.01056267, 0.00704178, 0.00352089, 0.00704178, 0.01056267,\n",
       "        0.        , 0.        , 0.01056267, 0.00352089, 0.00352089,\n",
       "        0.00352089, 0.00352089, 0.00704178, 0.        , 0.        ,\n",
       "        0.00352089, 0.00352089, 0.        , 0.        , 0.00704178,\n",
       "        0.00352089, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00352089]),\n",
       " array([ 5.45461125,  5.59662076,  5.73863027,  5.88063978,  6.0226493 ,\n",
       "         6.16465881,  6.30666832,  6.44867783,  6.59068734,  6.73269685,\n",
       "         6.87470636,  7.01671588,  7.15872539,  7.3007349 ,  7.44274441,\n",
       "         7.58475392,  7.72676343,  7.86877295,  8.01078246,  8.15279197,\n",
       "         8.29480148,  8.43681099,  8.5788205 ,  8.72083002,  8.86283953,\n",
       "         9.00484904,  9.14685855,  9.28886806,  9.43087757,  9.57288708,\n",
       "         9.7148966 ,  9.85690611,  9.99891562, 10.14092513, 10.28293464,\n",
       "        10.42494415, 10.56695367, 10.70896318, 10.85097269, 10.9929822 ,\n",
       "        11.13499171, 11.27700122, 11.41901074, 11.56102025, 11.70302976,\n",
       "        11.84503927, 11.98704878, 12.12905829, 12.2710678 , 12.41307732,\n",
       "        12.55508683, 12.69709634, 12.83910585, 12.98111536, 13.12312487,\n",
       "        13.26513439, 13.4071439 , 13.54915341, 13.69116292, 13.83317243,\n",
       "        13.97518194, 14.11719146, 14.25920097, 14.40121048, 14.54321999,\n",
       "        14.6852295 , 14.82723901, 14.96924852, 15.11125804, 15.25326755,\n",
       "        15.39527706, 15.53728657, 15.67929608, 15.82130559, 15.96331511,\n",
       "        16.10532462, 16.24733413, 16.38934364, 16.53135315, 16.67336266,\n",
       "        16.81537218, 16.95738169, 17.0993912 , 17.24140071, 17.38341022,\n",
       "        17.52541973, 17.66742924, 17.80943876, 17.95144827, 18.09345778,\n",
       "        18.23546729, 18.3774768 , 18.51948631, 18.66149583, 18.80350534,\n",
       "        18.94551485, 19.08752436, 19.22953387, 19.37154338, 19.5135529 ,\n",
       "        19.65556241]),\n",
       " <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = net_c8(noise.cuda())\n",
    "ll = net_c8.last\n",
    "print(ll.shape)\n",
    "ll = ll.cpu().detach().numpy() \n",
    "lid_net1_c8 = LID(ll, ll, k=50)\n",
    "plt.hist(lid_net1_c8, bins=100, normed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2_c8(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2_c8, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 100)\n",
    "        self.fc2 = nn.Linear(100, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        self.last = x\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naiqi/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.0767, Accuracy: 1093/8017 (13%)\n",
      "\n",
      "Train Epoch: 1 [0/48200 (0%)]\tLoss: 2.124532\n",
      "Train Epoch: 1 [640/48200 (1%)]\tLoss: 2.080121\n",
      "Train Epoch: 1 [1280/48200 (3%)]\tLoss: 2.001012\n",
      "Train Epoch: 1 [1920/48200 (4%)]\tLoss: 2.000285\n",
      "Train Epoch: 1 [2560/48200 (5%)]\tLoss: 1.937588\n",
      "Train Epoch: 1 [3200/48200 (7%)]\tLoss: 1.935493\n",
      "Train Epoch: 1 [3840/48200 (8%)]\tLoss: 1.729519\n",
      "Train Epoch: 1 [4480/48200 (9%)]\tLoss: 1.619585\n",
      "Train Epoch: 1 [5120/48200 (11%)]\tLoss: 1.392503\n",
      "Train Epoch: 1 [5760/48200 (12%)]\tLoss: 1.315869\n",
      "Train Epoch: 1 [6400/48200 (13%)]\tLoss: 1.125140\n",
      "Train Epoch: 1 [7040/48200 (15%)]\tLoss: 1.302177\n",
      "Train Epoch: 1 [7680/48200 (16%)]\tLoss: 0.987594\n",
      "Train Epoch: 1 [8320/48200 (17%)]\tLoss: 0.975525\n",
      "Train Epoch: 1 [8960/48200 (19%)]\tLoss: 0.838939\n",
      "Train Epoch: 1 [9600/48200 (20%)]\tLoss: 0.667607\n",
      "Train Epoch: 1 [10240/48200 (21%)]\tLoss: 0.614115\n",
      "Train Epoch: 1 [10880/48200 (23%)]\tLoss: 0.755726\n",
      "Train Epoch: 1 [11520/48200 (24%)]\tLoss: 0.767577\n",
      "Train Epoch: 1 [12160/48200 (25%)]\tLoss: 0.550053\n",
      "Train Epoch: 1 [12800/48200 (27%)]\tLoss: 0.647598\n",
      "Train Epoch: 1 [13440/48200 (28%)]\tLoss: 0.349948\n",
      "Train Epoch: 1 [14080/48200 (29%)]\tLoss: 0.656696\n",
      "Train Epoch: 1 [14720/48200 (31%)]\tLoss: 0.630770\n",
      "Train Epoch: 1 [15360/48200 (32%)]\tLoss: 0.842787\n",
      "Train Epoch: 1 [16000/48200 (33%)]\tLoss: 0.674240\n",
      "Train Epoch: 1 [16640/48200 (34%)]\tLoss: 0.492652\n",
      "Train Epoch: 1 [17280/48200 (36%)]\tLoss: 0.445517\n",
      "Train Epoch: 1 [17920/48200 (37%)]\tLoss: 0.402180\n",
      "Train Epoch: 1 [18560/48200 (38%)]\tLoss: 0.594522\n",
      "Train Epoch: 1 [19200/48200 (40%)]\tLoss: 0.462573\n",
      "Train Epoch: 1 [19840/48200 (41%)]\tLoss: 0.487662\n",
      "Train Epoch: 1 [20480/48200 (42%)]\tLoss: 0.367314\n",
      "Train Epoch: 1 [21120/48200 (44%)]\tLoss: 0.695602\n",
      "Train Epoch: 1 [21760/48200 (45%)]\tLoss: 0.311472\n",
      "Train Epoch: 1 [22400/48200 (46%)]\tLoss: 0.378950\n",
      "Train Epoch: 1 [23040/48200 (48%)]\tLoss: 0.394401\n",
      "Train Epoch: 1 [23680/48200 (49%)]\tLoss: 0.429446\n",
      "Train Epoch: 1 [24320/48200 (50%)]\tLoss: 0.364957\n",
      "Train Epoch: 1 [24960/48200 (52%)]\tLoss: 0.291565\n",
      "Train Epoch: 1 [25600/48200 (53%)]\tLoss: 0.257606\n",
      "Train Epoch: 1 [26240/48200 (54%)]\tLoss: 0.544920\n",
      "Train Epoch: 1 [26880/48200 (56%)]\tLoss: 0.296871\n",
      "Train Epoch: 1 [27520/48200 (57%)]\tLoss: 0.391908\n",
      "Train Epoch: 1 [28160/48200 (58%)]\tLoss: 0.331755\n",
      "Train Epoch: 1 [28800/48200 (60%)]\tLoss: 0.356028\n",
      "Train Epoch: 1 [29440/48200 (61%)]\tLoss: 0.421067\n",
      "Train Epoch: 1 [30080/48200 (62%)]\tLoss: 0.272145\n",
      "Train Epoch: 1 [30720/48200 (64%)]\tLoss: 0.288383\n",
      "Train Epoch: 1 [31360/48200 (65%)]\tLoss: 0.369477\n",
      "Train Epoch: 1 [32000/48200 (66%)]\tLoss: 0.470187\n",
      "Train Epoch: 1 [32640/48200 (68%)]\tLoss: 0.540029\n",
      "Train Epoch: 1 [33280/48200 (69%)]\tLoss: 0.279893\n",
      "Train Epoch: 1 [33920/48200 (70%)]\tLoss: 0.442606\n",
      "Train Epoch: 1 [34560/48200 (72%)]\tLoss: 0.330463\n",
      "Train Epoch: 1 [35200/48200 (73%)]\tLoss: 0.368529\n",
      "Train Epoch: 1 [35840/48200 (74%)]\tLoss: 0.156131\n",
      "Train Epoch: 1 [36480/48200 (76%)]\tLoss: 0.352231\n",
      "Train Epoch: 1 [37120/48200 (77%)]\tLoss: 0.326367\n",
      "Train Epoch: 1 [37760/48200 (78%)]\tLoss: 0.331793\n",
      "Train Epoch: 1 [38400/48200 (80%)]\tLoss: 0.332641\n",
      "Train Epoch: 1 [39040/48200 (81%)]\tLoss: 0.432590\n",
      "Train Epoch: 1 [39680/48200 (82%)]\tLoss: 0.286784\n",
      "Train Epoch: 1 [40320/48200 (84%)]\tLoss: 0.180888\n",
      "Train Epoch: 1 [40960/48200 (85%)]\tLoss: 0.401534\n",
      "Train Epoch: 1 [41600/48200 (86%)]\tLoss: 0.298622\n",
      "Train Epoch: 1 [42240/48200 (88%)]\tLoss: 0.411037\n",
      "Train Epoch: 1 [42880/48200 (89%)]\tLoss: 0.371138\n",
      "Train Epoch: 1 [43520/48200 (90%)]\tLoss: 0.184646\n",
      "Train Epoch: 1 [44160/48200 (92%)]\tLoss: 0.341204\n",
      "Train Epoch: 1 [44800/48200 (93%)]\tLoss: 0.306605\n",
      "Train Epoch: 1 [45440/48200 (94%)]\tLoss: 0.189648\n",
      "Train Epoch: 1 [46080/48200 (95%)]\tLoss: 0.296901\n",
      "Train Epoch: 1 [46720/48200 (97%)]\tLoss: 0.190077\n",
      "Train Epoch: 1 [47360/48200 (98%)]\tLoss: 0.273539\n",
      "Train Epoch: 1 [48000/48200 (99%)]\tLoss: 0.241510\n",
      "\n",
      "Test set: Avg. loss: 0.1218, Accuracy: 7716/8017 (96%)\n",
      "\n",
      "Train Epoch: 2 [0/48200 (0%)]\tLoss: 0.399077\n",
      "Train Epoch: 2 [640/48200 (1%)]\tLoss: 0.261044\n",
      "Train Epoch: 2 [1280/48200 (3%)]\tLoss: 0.559918\n",
      "Train Epoch: 2 [1920/48200 (4%)]\tLoss: 0.162571\n",
      "Train Epoch: 2 [2560/48200 (5%)]\tLoss: 0.382858\n",
      "Train Epoch: 2 [3200/48200 (7%)]\tLoss: 0.347829\n",
      "Train Epoch: 2 [3840/48200 (8%)]\tLoss: 0.158901\n",
      "Train Epoch: 2 [4480/48200 (9%)]\tLoss: 0.242240\n",
      "Train Epoch: 2 [5120/48200 (11%)]\tLoss: 0.549883\n",
      "Train Epoch: 2 [5760/48200 (12%)]\tLoss: 0.291869\n",
      "Train Epoch: 2 [6400/48200 (13%)]\tLoss: 0.190454\n",
      "Train Epoch: 2 [7040/48200 (15%)]\tLoss: 0.207747\n",
      "Train Epoch: 2 [7680/48200 (16%)]\tLoss: 0.119367\n",
      "Train Epoch: 2 [8320/48200 (17%)]\tLoss: 0.163328\n",
      "Train Epoch: 2 [8960/48200 (19%)]\tLoss: 0.090984\n",
      "Train Epoch: 2 [9600/48200 (20%)]\tLoss: 0.148798\n",
      "Train Epoch: 2 [10240/48200 (21%)]\tLoss: 0.139144\n",
      "Train Epoch: 2 [10880/48200 (23%)]\tLoss: 0.244685\n",
      "Train Epoch: 2 [11520/48200 (24%)]\tLoss: 0.335252\n",
      "Train Epoch: 2 [12160/48200 (25%)]\tLoss: 0.215910\n",
      "Train Epoch: 2 [12800/48200 (27%)]\tLoss: 0.104801\n",
      "Train Epoch: 2 [13440/48200 (28%)]\tLoss: 0.351539\n",
      "Train Epoch: 2 [14080/48200 (29%)]\tLoss: 0.315677\n",
      "Train Epoch: 2 [14720/48200 (31%)]\tLoss: 0.173974\n",
      "Train Epoch: 2 [15360/48200 (32%)]\tLoss: 0.236859\n",
      "Train Epoch: 2 [16000/48200 (33%)]\tLoss: 0.328457\n",
      "Train Epoch: 2 [16640/48200 (34%)]\tLoss: 0.140235\n",
      "Train Epoch: 2 [17280/48200 (36%)]\tLoss: 0.443850\n",
      "Train Epoch: 2 [17920/48200 (37%)]\tLoss: 0.207801\n",
      "Train Epoch: 2 [18560/48200 (38%)]\tLoss: 0.142057\n",
      "Train Epoch: 2 [19200/48200 (40%)]\tLoss: 0.172684\n",
      "Train Epoch: 2 [19840/48200 (41%)]\tLoss: 0.247571\n",
      "Train Epoch: 2 [20480/48200 (42%)]\tLoss: 0.232745\n",
      "Train Epoch: 2 [21120/48200 (44%)]\tLoss: 0.337171\n",
      "Train Epoch: 2 [21760/48200 (45%)]\tLoss: 0.186813\n",
      "Train Epoch: 2 [22400/48200 (46%)]\tLoss: 0.153906\n",
      "Train Epoch: 2 [23040/48200 (48%)]\tLoss: 0.135990\n",
      "Train Epoch: 2 [23680/48200 (49%)]\tLoss: 0.176966\n",
      "Train Epoch: 2 [24320/48200 (50%)]\tLoss: 0.171495\n",
      "Train Epoch: 2 [24960/48200 (52%)]\tLoss: 0.348241\n",
      "Train Epoch: 2 [25600/48200 (53%)]\tLoss: 0.075650\n",
      "Train Epoch: 2 [26240/48200 (54%)]\tLoss: 0.113921\n",
      "Train Epoch: 2 [26880/48200 (56%)]\tLoss: 0.240038\n",
      "Train Epoch: 2 [27520/48200 (57%)]\tLoss: 0.142230\n",
      "Train Epoch: 2 [28160/48200 (58%)]\tLoss: 0.167907\n",
      "Train Epoch: 2 [28800/48200 (60%)]\tLoss: 0.182548\n",
      "Train Epoch: 2 [29440/48200 (61%)]\tLoss: 0.187350\n",
      "Train Epoch: 2 [30080/48200 (62%)]\tLoss: 0.276264\n",
      "Train Epoch: 2 [30720/48200 (64%)]\tLoss: 0.222055\n",
      "Train Epoch: 2 [31360/48200 (65%)]\tLoss: 0.268878\n",
      "Train Epoch: 2 [32000/48200 (66%)]\tLoss: 0.272270\n",
      "Train Epoch: 2 [32640/48200 (68%)]\tLoss: 0.177273\n",
      "Train Epoch: 2 [33280/48200 (69%)]\tLoss: 0.119907\n",
      "Train Epoch: 2 [33920/48200 (70%)]\tLoss: 0.126845\n",
      "Train Epoch: 2 [34560/48200 (72%)]\tLoss: 0.096615\n",
      "Train Epoch: 2 [35200/48200 (73%)]\tLoss: 0.200391\n",
      "Train Epoch: 2 [35840/48200 (74%)]\tLoss: 0.228527\n",
      "Train Epoch: 2 [36480/48200 (76%)]\tLoss: 0.412660\n",
      "Train Epoch: 2 [37120/48200 (77%)]\tLoss: 0.353553\n",
      "Train Epoch: 2 [37760/48200 (78%)]\tLoss: 0.209105\n",
      "Train Epoch: 2 [38400/48200 (80%)]\tLoss: 0.133392\n",
      "Train Epoch: 2 [39040/48200 (81%)]\tLoss: 0.134824\n",
      "Train Epoch: 2 [39680/48200 (82%)]\tLoss: 0.233396\n",
      "Train Epoch: 2 [40320/48200 (84%)]\tLoss: 0.069083\n",
      "Train Epoch: 2 [40960/48200 (85%)]\tLoss: 0.096448\n",
      "Train Epoch: 2 [41600/48200 (86%)]\tLoss: 0.165116\n",
      "Train Epoch: 2 [42240/48200 (88%)]\tLoss: 0.187566\n",
      "Train Epoch: 2 [42880/48200 (89%)]\tLoss: 0.374261\n",
      "Train Epoch: 2 [43520/48200 (90%)]\tLoss: 0.332986\n",
      "Train Epoch: 2 [44160/48200 (92%)]\tLoss: 0.196882\n",
      "Train Epoch: 2 [44800/48200 (93%)]\tLoss: 0.396504\n",
      "Train Epoch: 2 [45440/48200 (94%)]\tLoss: 0.128694\n",
      "Train Epoch: 2 [46080/48200 (95%)]\tLoss: 0.281307\n",
      "Train Epoch: 2 [46720/48200 (97%)]\tLoss: 0.227474\n",
      "Train Epoch: 2 [47360/48200 (98%)]\tLoss: 0.311865\n",
      "Train Epoch: 2 [48000/48200 (99%)]\tLoss: 0.233466\n",
      "\n",
      "Test set: Avg. loss: 0.0832, Accuracy: 7806/8017 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/48200 (0%)]\tLoss: 0.196420\n",
      "Train Epoch: 3 [640/48200 (1%)]\tLoss: 0.161852\n",
      "Train Epoch: 3 [1280/48200 (3%)]\tLoss: 0.109256\n",
      "Train Epoch: 3 [1920/48200 (4%)]\tLoss: 0.179663\n",
      "Train Epoch: 3 [2560/48200 (5%)]\tLoss: 0.080832\n",
      "Train Epoch: 3 [3200/48200 (7%)]\tLoss: 0.236131\n",
      "Train Epoch: 3 [3840/48200 (8%)]\tLoss: 0.180887\n",
      "Train Epoch: 3 [4480/48200 (9%)]\tLoss: 0.290682\n",
      "Train Epoch: 3 [5120/48200 (11%)]\tLoss: 0.125125\n",
      "Train Epoch: 3 [5760/48200 (12%)]\tLoss: 0.157114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [6400/48200 (13%)]\tLoss: 0.146493\n",
      "Train Epoch: 3 [7040/48200 (15%)]\tLoss: 0.307869\n",
      "Train Epoch: 3 [7680/48200 (16%)]\tLoss: 0.094759\n",
      "Train Epoch: 3 [8320/48200 (17%)]\tLoss: 0.119808\n",
      "Train Epoch: 3 [8960/48200 (19%)]\tLoss: 0.499658\n",
      "Train Epoch: 3 [9600/48200 (20%)]\tLoss: 0.067145\n",
      "Train Epoch: 3 [10240/48200 (21%)]\tLoss: 0.214824\n",
      "Train Epoch: 3 [10880/48200 (23%)]\tLoss: 0.225647\n",
      "Train Epoch: 3 [11520/48200 (24%)]\tLoss: 0.101158\n",
      "Train Epoch: 3 [12160/48200 (25%)]\tLoss: 0.095540\n",
      "Train Epoch: 3 [12800/48200 (27%)]\tLoss: 0.165291\n",
      "Train Epoch: 3 [13440/48200 (28%)]\tLoss: 0.147344\n",
      "Train Epoch: 3 [14080/48200 (29%)]\tLoss: 0.183161\n",
      "Train Epoch: 3 [14720/48200 (31%)]\tLoss: 0.220958\n",
      "Train Epoch: 3 [15360/48200 (32%)]\tLoss: 0.221444\n",
      "Train Epoch: 3 [16000/48200 (33%)]\tLoss: 0.114379\n",
      "Train Epoch: 3 [16640/48200 (34%)]\tLoss: 0.153379\n",
      "Train Epoch: 3 [17280/48200 (36%)]\tLoss: 0.154812\n",
      "Train Epoch: 3 [17920/48200 (37%)]\tLoss: 0.258330\n",
      "Train Epoch: 3 [18560/48200 (38%)]\tLoss: 0.137543\n",
      "Train Epoch: 3 [19200/48200 (40%)]\tLoss: 0.115088\n",
      "Train Epoch: 3 [19840/48200 (41%)]\tLoss: 0.241098\n",
      "Train Epoch: 3 [20480/48200 (42%)]\tLoss: 0.046877\n",
      "Train Epoch: 3 [21120/48200 (44%)]\tLoss: 0.099643\n",
      "Train Epoch: 3 [21760/48200 (45%)]\tLoss: 0.360091\n",
      "Train Epoch: 3 [22400/48200 (46%)]\tLoss: 0.325892\n",
      "Train Epoch: 3 [23040/48200 (48%)]\tLoss: 0.287075\n",
      "Train Epoch: 3 [23680/48200 (49%)]\tLoss: 0.118573\n",
      "Train Epoch: 3 [24320/48200 (50%)]\tLoss: 0.148256\n",
      "Train Epoch: 3 [24960/48200 (52%)]\tLoss: 0.042875\n",
      "Train Epoch: 3 [25600/48200 (53%)]\tLoss: 0.102381\n",
      "Train Epoch: 3 [26240/48200 (54%)]\tLoss: 0.106614\n",
      "Train Epoch: 3 [26880/48200 (56%)]\tLoss: 0.233472\n",
      "Train Epoch: 3 [27520/48200 (57%)]\tLoss: 0.155415\n",
      "Train Epoch: 3 [28160/48200 (58%)]\tLoss: 0.369631\n",
      "Train Epoch: 3 [28800/48200 (60%)]\tLoss: 0.146601\n",
      "Train Epoch: 3 [29440/48200 (61%)]\tLoss: 0.136178\n",
      "Train Epoch: 3 [30080/48200 (62%)]\tLoss: 0.171025\n",
      "Train Epoch: 3 [30720/48200 (64%)]\tLoss: 0.269703\n",
      "Train Epoch: 3 [31360/48200 (65%)]\tLoss: 0.187813\n",
      "Train Epoch: 3 [32000/48200 (66%)]\tLoss: 0.125615\n",
      "Train Epoch: 3 [32640/48200 (68%)]\tLoss: 0.315289\n",
      "Train Epoch: 3 [33280/48200 (69%)]\tLoss: 0.194789\n",
      "Train Epoch: 3 [33920/48200 (70%)]\tLoss: 0.112768\n",
      "Train Epoch: 3 [34560/48200 (72%)]\tLoss: 0.112576\n",
      "Train Epoch: 3 [35200/48200 (73%)]\tLoss: 0.199371\n",
      "Train Epoch: 3 [35840/48200 (74%)]\tLoss: 0.120138\n",
      "Train Epoch: 3 [36480/48200 (76%)]\tLoss: 0.101837\n",
      "Train Epoch: 3 [37120/48200 (77%)]\tLoss: 0.214589\n",
      "Train Epoch: 3 [37760/48200 (78%)]\tLoss: 0.147476\n",
      "Train Epoch: 3 [38400/48200 (80%)]\tLoss: 0.126072\n",
      "Train Epoch: 3 [39040/48200 (81%)]\tLoss: 0.190840\n",
      "Train Epoch: 3 [39680/48200 (82%)]\tLoss: 0.240115\n",
      "Train Epoch: 3 [40320/48200 (84%)]\tLoss: 0.277864\n",
      "Train Epoch: 3 [40960/48200 (85%)]\tLoss: 0.182847\n",
      "Train Epoch: 3 [41600/48200 (86%)]\tLoss: 0.129669\n",
      "Train Epoch: 3 [42240/48200 (88%)]\tLoss: 0.142374\n",
      "Train Epoch: 3 [42880/48200 (89%)]\tLoss: 0.162726\n",
      "Train Epoch: 3 [43520/48200 (90%)]\tLoss: 0.132770\n",
      "Train Epoch: 3 [44160/48200 (92%)]\tLoss: 0.089056\n",
      "Train Epoch: 3 [44800/48200 (93%)]\tLoss: 0.167974\n",
      "Train Epoch: 3 [45440/48200 (94%)]\tLoss: 0.155120\n",
      "Train Epoch: 3 [46080/48200 (95%)]\tLoss: 0.113583\n",
      "Train Epoch: 3 [46720/48200 (97%)]\tLoss: 0.219874\n",
      "Train Epoch: 3 [47360/48200 (98%)]\tLoss: 0.128839\n",
      "Train Epoch: 3 [48000/48200 (99%)]\tLoss: 0.122021\n",
      "\n",
      "Test set: Avg. loss: 0.0656, Accuracy: 7834/8017 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/48200 (0%)]\tLoss: 0.213751\n",
      "Train Epoch: 4 [640/48200 (1%)]\tLoss: 0.151665\n",
      "Train Epoch: 4 [1280/48200 (3%)]\tLoss: 0.098121\n",
      "Train Epoch: 4 [1920/48200 (4%)]\tLoss: 0.107778\n",
      "Train Epoch: 4 [2560/48200 (5%)]\tLoss: 0.130981\n",
      "Train Epoch: 4 [3200/48200 (7%)]\tLoss: 0.264359\n",
      "Train Epoch: 4 [3840/48200 (8%)]\tLoss: 0.106865\n",
      "Train Epoch: 4 [4480/48200 (9%)]\tLoss: 0.090706\n",
      "Train Epoch: 4 [5120/48200 (11%)]\tLoss: 0.271452\n",
      "Train Epoch: 4 [5760/48200 (12%)]\tLoss: 0.059753\n",
      "Train Epoch: 4 [6400/48200 (13%)]\tLoss: 0.057052\n",
      "Train Epoch: 4 [7040/48200 (15%)]\tLoss: 0.118804\n",
      "Train Epoch: 4 [7680/48200 (16%)]\tLoss: 0.188162\n",
      "Train Epoch: 4 [8320/48200 (17%)]\tLoss: 0.165188\n",
      "Train Epoch: 4 [8960/48200 (19%)]\tLoss: 0.091425\n",
      "Train Epoch: 4 [9600/48200 (20%)]\tLoss: 0.119657\n",
      "Train Epoch: 4 [10240/48200 (21%)]\tLoss: 0.066947\n",
      "Train Epoch: 4 [10880/48200 (23%)]\tLoss: 0.191707\n",
      "Train Epoch: 4 [11520/48200 (24%)]\tLoss: 0.154094\n",
      "Train Epoch: 4 [12160/48200 (25%)]\tLoss: 0.068558\n",
      "Train Epoch: 4 [12800/48200 (27%)]\tLoss: 0.134658\n",
      "Train Epoch: 4 [13440/48200 (28%)]\tLoss: 0.145197\n",
      "Train Epoch: 4 [14080/48200 (29%)]\tLoss: 0.191575\n",
      "Train Epoch: 4 [14720/48200 (31%)]\tLoss: 0.254158\n",
      "Train Epoch: 4 [15360/48200 (32%)]\tLoss: 0.144413\n",
      "Train Epoch: 4 [16000/48200 (33%)]\tLoss: 0.164424\n",
      "Train Epoch: 4 [16640/48200 (34%)]\tLoss: 0.265968\n",
      "Train Epoch: 4 [17280/48200 (36%)]\tLoss: 0.090887\n",
      "Train Epoch: 4 [17920/48200 (37%)]\tLoss: 0.041984\n",
      "Train Epoch: 4 [18560/48200 (38%)]\tLoss: 0.127190\n",
      "Train Epoch: 4 [19200/48200 (40%)]\tLoss: 0.072368\n",
      "Train Epoch: 4 [19840/48200 (41%)]\tLoss: 0.193153\n",
      "Train Epoch: 4 [20480/48200 (42%)]\tLoss: 0.231739\n",
      "Train Epoch: 4 [21120/48200 (44%)]\tLoss: 0.094589\n",
      "Train Epoch: 4 [21760/48200 (45%)]\tLoss: 0.224909\n",
      "Train Epoch: 4 [22400/48200 (46%)]\tLoss: 0.092335\n",
      "Train Epoch: 4 [23040/48200 (48%)]\tLoss: 0.182017\n",
      "Train Epoch: 4 [23680/48200 (49%)]\tLoss: 0.026821\n",
      "Train Epoch: 4 [24320/48200 (50%)]\tLoss: 0.150895\n",
      "Train Epoch: 4 [24960/48200 (52%)]\tLoss: 0.078721\n",
      "Train Epoch: 4 [25600/48200 (53%)]\tLoss: 0.038501\n",
      "Train Epoch: 4 [26240/48200 (54%)]\tLoss: 0.148735\n",
      "Train Epoch: 4 [26880/48200 (56%)]\tLoss: 0.421487\n",
      "Train Epoch: 4 [27520/48200 (57%)]\tLoss: 0.085066\n",
      "Train Epoch: 4 [28160/48200 (58%)]\tLoss: 0.234967\n",
      "Train Epoch: 4 [28800/48200 (60%)]\tLoss: 0.146603\n",
      "Train Epoch: 4 [29440/48200 (61%)]\tLoss: 0.194177\n",
      "Train Epoch: 4 [30080/48200 (62%)]\tLoss: 0.077077\n",
      "Train Epoch: 4 [30720/48200 (64%)]\tLoss: 0.200937\n",
      "Train Epoch: 4 [31360/48200 (65%)]\tLoss: 0.138489\n",
      "Train Epoch: 4 [32000/48200 (66%)]\tLoss: 0.086407\n",
      "Train Epoch: 4 [32640/48200 (68%)]\tLoss: 0.090856\n",
      "Train Epoch: 4 [33280/48200 (69%)]\tLoss: 0.118144\n",
      "Train Epoch: 4 [33920/48200 (70%)]\tLoss: 0.075374\n",
      "Train Epoch: 4 [34560/48200 (72%)]\tLoss: 0.077110\n",
      "Train Epoch: 4 [35200/48200 (73%)]\tLoss: 0.145302\n",
      "Train Epoch: 4 [35840/48200 (74%)]\tLoss: 0.089530\n",
      "Train Epoch: 4 [36480/48200 (76%)]\tLoss: 0.061934\n",
      "Train Epoch: 4 [37120/48200 (77%)]\tLoss: 0.126490\n",
      "Train Epoch: 4 [37760/48200 (78%)]\tLoss: 0.138794\n",
      "Train Epoch: 4 [38400/48200 (80%)]\tLoss: 0.050762\n",
      "Train Epoch: 4 [39040/48200 (81%)]\tLoss: 0.128491\n",
      "Train Epoch: 4 [39680/48200 (82%)]\tLoss: 0.161814\n",
      "Train Epoch: 4 [40320/48200 (84%)]\tLoss: 0.129727\n",
      "Train Epoch: 4 [40960/48200 (85%)]\tLoss: 0.133237\n",
      "Train Epoch: 4 [41600/48200 (86%)]\tLoss: 0.279297\n",
      "Train Epoch: 4 [42240/48200 (88%)]\tLoss: 0.079243\n",
      "Train Epoch: 4 [42880/48200 (89%)]\tLoss: 0.055287\n",
      "Train Epoch: 4 [43520/48200 (90%)]\tLoss: 0.036936\n",
      "Train Epoch: 4 [44160/48200 (92%)]\tLoss: 0.348603\n",
      "Train Epoch: 4 [44800/48200 (93%)]\tLoss: 0.159235\n",
      "Train Epoch: 4 [45440/48200 (94%)]\tLoss: 0.158091\n",
      "Train Epoch: 4 [46080/48200 (95%)]\tLoss: 0.098350\n",
      "Train Epoch: 4 [46720/48200 (97%)]\tLoss: 0.115936\n",
      "Train Epoch: 4 [47360/48200 (98%)]\tLoss: 0.059576\n",
      "Train Epoch: 4 [48000/48200 (99%)]\tLoss: 0.314466\n",
      "\n",
      "Test set: Avg. loss: 0.0557, Accuracy: 7865/8017 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/48200 (0%)]\tLoss: 0.166389\n",
      "Train Epoch: 5 [640/48200 (1%)]\tLoss: 0.148249\n",
      "Train Epoch: 5 [1280/48200 (3%)]\tLoss: 0.048412\n",
      "Train Epoch: 5 [1920/48200 (4%)]\tLoss: 0.171488\n",
      "Train Epoch: 5 [2560/48200 (5%)]\tLoss: 0.188752\n",
      "Train Epoch: 5 [3200/48200 (7%)]\tLoss: 0.149270\n",
      "Train Epoch: 5 [3840/48200 (8%)]\tLoss: 0.180401\n",
      "Train Epoch: 5 [4480/48200 (9%)]\tLoss: 0.057559\n",
      "Train Epoch: 5 [5120/48200 (11%)]\tLoss: 0.127262\n",
      "Train Epoch: 5 [5760/48200 (12%)]\tLoss: 0.272595\n",
      "Train Epoch: 5 [6400/48200 (13%)]\tLoss: 0.086447\n",
      "Train Epoch: 5 [7040/48200 (15%)]\tLoss: 0.101287\n",
      "Train Epoch: 5 [7680/48200 (16%)]\tLoss: 0.060646\n",
      "Train Epoch: 5 [8320/48200 (17%)]\tLoss: 0.142190\n",
      "Train Epoch: 5 [8960/48200 (19%)]\tLoss: 0.139090\n",
      "Train Epoch: 5 [9600/48200 (20%)]\tLoss: 0.160824\n",
      "Train Epoch: 5 [10240/48200 (21%)]\tLoss: 0.033129\n",
      "Train Epoch: 5 [10880/48200 (23%)]\tLoss: 0.056823\n",
      "Train Epoch: 5 [11520/48200 (24%)]\tLoss: 0.071291\n",
      "Train Epoch: 5 [12160/48200 (25%)]\tLoss: 0.073229\n",
      "Train Epoch: 5 [12800/48200 (27%)]\tLoss: 0.115824\n",
      "Train Epoch: 5 [13440/48200 (28%)]\tLoss: 0.051851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [14080/48200 (29%)]\tLoss: 0.085986\n",
      "Train Epoch: 5 [14720/48200 (31%)]\tLoss: 0.091218\n",
      "Train Epoch: 5 [15360/48200 (32%)]\tLoss: 0.259002\n",
      "Train Epoch: 5 [16000/48200 (33%)]\tLoss: 0.095736\n",
      "Train Epoch: 5 [16640/48200 (34%)]\tLoss: 0.118999\n",
      "Train Epoch: 5 [17280/48200 (36%)]\tLoss: 0.290808\n",
      "Train Epoch: 5 [17920/48200 (37%)]\tLoss: 0.075848\n",
      "Train Epoch: 5 [18560/48200 (38%)]\tLoss: 0.084447\n",
      "Train Epoch: 5 [19200/48200 (40%)]\tLoss: 0.181716\n",
      "Train Epoch: 5 [19840/48200 (41%)]\tLoss: 0.174951\n",
      "Train Epoch: 5 [20480/48200 (42%)]\tLoss: 0.129419\n",
      "Train Epoch: 5 [21120/48200 (44%)]\tLoss: 0.119647\n",
      "Train Epoch: 5 [21760/48200 (45%)]\tLoss: 0.116237\n",
      "Train Epoch: 5 [22400/48200 (46%)]\tLoss: 0.203935\n",
      "Train Epoch: 5 [23040/48200 (48%)]\tLoss: 0.073762\n",
      "Train Epoch: 5 [23680/48200 (49%)]\tLoss: 0.112127\n",
      "Train Epoch: 5 [24320/48200 (50%)]\tLoss: 0.150749\n",
      "Train Epoch: 5 [24960/48200 (52%)]\tLoss: 0.226141\n",
      "Train Epoch: 5 [25600/48200 (53%)]\tLoss: 0.275428\n",
      "Train Epoch: 5 [26240/48200 (54%)]\tLoss: 0.086640\n",
      "Train Epoch: 5 [26880/48200 (56%)]\tLoss: 0.066375\n",
      "Train Epoch: 5 [27520/48200 (57%)]\tLoss: 0.114296\n",
      "Train Epoch: 5 [28160/48200 (58%)]\tLoss: 0.081979\n",
      "Train Epoch: 5 [28800/48200 (60%)]\tLoss: 0.190440\n",
      "Train Epoch: 5 [29440/48200 (61%)]\tLoss: 0.256135\n",
      "Train Epoch: 5 [30080/48200 (62%)]\tLoss: 0.103179\n",
      "Train Epoch: 5 [30720/48200 (64%)]\tLoss: 0.163113\n",
      "Train Epoch: 5 [31360/48200 (65%)]\tLoss: 0.219387\n",
      "Train Epoch: 5 [32000/48200 (66%)]\tLoss: 0.123214\n",
      "Train Epoch: 5 [32640/48200 (68%)]\tLoss: 0.135218\n",
      "Train Epoch: 5 [33280/48200 (69%)]\tLoss: 0.066550\n",
      "Train Epoch: 5 [33920/48200 (70%)]\tLoss: 0.097472\n",
      "Train Epoch: 5 [34560/48200 (72%)]\tLoss: 0.052983\n",
      "Train Epoch: 5 [35200/48200 (73%)]\tLoss: 0.063723\n",
      "Train Epoch: 5 [35840/48200 (74%)]\tLoss: 0.222012\n",
      "Train Epoch: 5 [36480/48200 (76%)]\tLoss: 0.101051\n",
      "Train Epoch: 5 [37120/48200 (77%)]\tLoss: 0.155135\n",
      "Train Epoch: 5 [37760/48200 (78%)]\tLoss: 0.104636\n",
      "Train Epoch: 5 [38400/48200 (80%)]\tLoss: 0.085663\n",
      "Train Epoch: 5 [39040/48200 (81%)]\tLoss: 0.095035\n",
      "Train Epoch: 5 [39680/48200 (82%)]\tLoss: 0.121684\n",
      "Train Epoch: 5 [40320/48200 (84%)]\tLoss: 0.085528\n",
      "Train Epoch: 5 [40960/48200 (85%)]\tLoss: 0.129409\n",
      "Train Epoch: 5 [41600/48200 (86%)]\tLoss: 0.140938\n",
      "Train Epoch: 5 [42240/48200 (88%)]\tLoss: 0.085085\n",
      "Train Epoch: 5 [42880/48200 (89%)]\tLoss: 0.058794\n",
      "Train Epoch: 5 [43520/48200 (90%)]\tLoss: 0.145117\n",
      "Train Epoch: 5 [44160/48200 (92%)]\tLoss: 0.092687\n",
      "Train Epoch: 5 [44800/48200 (93%)]\tLoss: 0.078925\n",
      "Train Epoch: 5 [45440/48200 (94%)]\tLoss: 0.077602\n",
      "Train Epoch: 5 [46080/48200 (95%)]\tLoss: 0.093567\n",
      "Train Epoch: 5 [46720/48200 (97%)]\tLoss: 0.135804\n",
      "Train Epoch: 5 [47360/48200 (98%)]\tLoss: 0.115493\n",
      "Train Epoch: 5 [48000/48200 (99%)]\tLoss: 0.038463\n",
      "\n",
      "Test set: Avg. loss: 0.0503, Accuracy: 7884/8017 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/48200 (0%)]\tLoss: 0.072531\n",
      "Train Epoch: 6 [640/48200 (1%)]\tLoss: 0.079611\n",
      "Train Epoch: 6 [1280/48200 (3%)]\tLoss: 0.066919\n",
      "Train Epoch: 6 [1920/48200 (4%)]\tLoss: 0.438293\n",
      "Train Epoch: 6 [2560/48200 (5%)]\tLoss: 0.254521\n",
      "Train Epoch: 6 [3200/48200 (7%)]\tLoss: 0.303287\n",
      "Train Epoch: 6 [3840/48200 (8%)]\tLoss: 0.095510\n",
      "Train Epoch: 6 [4480/48200 (9%)]\tLoss: 0.084266\n",
      "Train Epoch: 6 [5120/48200 (11%)]\tLoss: 0.073060\n",
      "Train Epoch: 6 [5760/48200 (12%)]\tLoss: 0.095042\n",
      "Train Epoch: 6 [6400/48200 (13%)]\tLoss: 0.076840\n",
      "Train Epoch: 6 [7040/48200 (15%)]\tLoss: 0.048458\n",
      "Train Epoch: 6 [7680/48200 (16%)]\tLoss: 0.207605\n",
      "Train Epoch: 6 [8320/48200 (17%)]\tLoss: 0.042356\n",
      "Train Epoch: 6 [8960/48200 (19%)]\tLoss: 0.299426\n",
      "Train Epoch: 6 [9600/48200 (20%)]\tLoss: 0.048644\n",
      "Train Epoch: 6 [10240/48200 (21%)]\tLoss: 0.093856\n",
      "Train Epoch: 6 [10880/48200 (23%)]\tLoss: 0.180615\n",
      "Train Epoch: 6 [11520/48200 (24%)]\tLoss: 0.075804\n",
      "Train Epoch: 6 [12160/48200 (25%)]\tLoss: 0.278215\n",
      "Train Epoch: 6 [12800/48200 (27%)]\tLoss: 0.161735\n",
      "Train Epoch: 6 [13440/48200 (28%)]\tLoss: 0.111587\n",
      "Train Epoch: 6 [14080/48200 (29%)]\tLoss: 0.045813\n",
      "Train Epoch: 6 [14720/48200 (31%)]\tLoss: 0.044696\n",
      "Train Epoch: 6 [15360/48200 (32%)]\tLoss: 0.234554\n",
      "Train Epoch: 6 [16000/48200 (33%)]\tLoss: 0.137051\n",
      "Train Epoch: 6 [16640/48200 (34%)]\tLoss: 0.129006\n",
      "Train Epoch: 6 [17280/48200 (36%)]\tLoss: 0.095483\n",
      "Train Epoch: 6 [17920/48200 (37%)]\tLoss: 0.219532\n",
      "Train Epoch: 6 [18560/48200 (38%)]\tLoss: 0.033832\n",
      "Train Epoch: 6 [19200/48200 (40%)]\tLoss: 0.184288\n",
      "Train Epoch: 6 [19840/48200 (41%)]\tLoss: 0.114938\n",
      "Train Epoch: 6 [20480/48200 (42%)]\tLoss: 0.074557\n",
      "Train Epoch: 6 [21120/48200 (44%)]\tLoss: 0.258564\n",
      "Train Epoch: 6 [21760/48200 (45%)]\tLoss: 0.105313\n",
      "Train Epoch: 6 [22400/48200 (46%)]\tLoss: 0.057079\n",
      "Train Epoch: 6 [23040/48200 (48%)]\tLoss: 0.193390\n",
      "Train Epoch: 6 [23680/48200 (49%)]\tLoss: 0.155550\n",
      "Train Epoch: 6 [24320/48200 (50%)]\tLoss: 0.196639\n",
      "Train Epoch: 6 [24960/48200 (52%)]\tLoss: 0.038999\n",
      "Train Epoch: 6 [25600/48200 (53%)]\tLoss: 0.066865\n",
      "Train Epoch: 6 [26240/48200 (54%)]\tLoss: 0.085281\n",
      "Train Epoch: 6 [26880/48200 (56%)]\tLoss: 0.120179\n",
      "Train Epoch: 6 [27520/48200 (57%)]\tLoss: 0.250839\n",
      "Train Epoch: 6 [28160/48200 (58%)]\tLoss: 0.106481\n",
      "Train Epoch: 6 [28800/48200 (60%)]\tLoss: 0.146617\n",
      "Train Epoch: 6 [29440/48200 (61%)]\tLoss: 0.030972\n",
      "Train Epoch: 6 [30080/48200 (62%)]\tLoss: 0.064268\n",
      "Train Epoch: 6 [30720/48200 (64%)]\tLoss: 0.423769\n",
      "Train Epoch: 6 [31360/48200 (65%)]\tLoss: 0.056067\n",
      "Train Epoch: 6 [32000/48200 (66%)]\tLoss: 0.040109\n",
      "Train Epoch: 6 [32640/48200 (68%)]\tLoss: 0.113417\n",
      "Train Epoch: 6 [33280/48200 (69%)]\tLoss: 0.126897\n",
      "Train Epoch: 6 [33920/48200 (70%)]\tLoss: 0.064971\n",
      "Train Epoch: 6 [34560/48200 (72%)]\tLoss: 0.191614\n",
      "Train Epoch: 6 [35200/48200 (73%)]\tLoss: 0.134237\n",
      "Train Epoch: 6 [35840/48200 (74%)]\tLoss: 0.104649\n",
      "Train Epoch: 6 [36480/48200 (76%)]\tLoss: 0.151631\n",
      "Train Epoch: 6 [37120/48200 (77%)]\tLoss: 0.058908\n",
      "Train Epoch: 6 [37760/48200 (78%)]\tLoss: 0.052465\n",
      "Train Epoch: 6 [38400/48200 (80%)]\tLoss: 0.028897\n",
      "Train Epoch: 6 [39040/48200 (81%)]\tLoss: 0.044731\n",
      "Train Epoch: 6 [39680/48200 (82%)]\tLoss: 0.106675\n",
      "Train Epoch: 6 [40320/48200 (84%)]\tLoss: 0.120300\n",
      "Train Epoch: 6 [40960/48200 (85%)]\tLoss: 0.073687\n",
      "Train Epoch: 6 [41600/48200 (86%)]\tLoss: 0.016684\n",
      "Train Epoch: 6 [42240/48200 (88%)]\tLoss: 0.114053\n",
      "Train Epoch: 6 [42880/48200 (89%)]\tLoss: 0.098331\n",
      "Train Epoch: 6 [43520/48200 (90%)]\tLoss: 0.078357\n",
      "Train Epoch: 6 [44160/48200 (92%)]\tLoss: 0.058236\n",
      "Train Epoch: 6 [44800/48200 (93%)]\tLoss: 0.089974\n",
      "Train Epoch: 6 [45440/48200 (94%)]\tLoss: 0.127706\n",
      "Train Epoch: 6 [46080/48200 (95%)]\tLoss: 0.082768\n",
      "Train Epoch: 6 [46720/48200 (97%)]\tLoss: 0.057763\n",
      "Train Epoch: 6 [47360/48200 (98%)]\tLoss: 0.159233\n",
      "Train Epoch: 6 [48000/48200 (99%)]\tLoss: 0.106668\n",
      "\n",
      "Test set: Avg. loss: 0.0435, Accuracy: 7909/8017 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/48200 (0%)]\tLoss: 0.188314\n",
      "Train Epoch: 7 [640/48200 (1%)]\tLoss: 0.126101\n",
      "Train Epoch: 7 [1280/48200 (3%)]\tLoss: 0.052753\n",
      "Train Epoch: 7 [1920/48200 (4%)]\tLoss: 0.299506\n",
      "Train Epoch: 7 [2560/48200 (5%)]\tLoss: 0.150627\n",
      "Train Epoch: 7 [3200/48200 (7%)]\tLoss: 0.144279\n",
      "Train Epoch: 7 [3840/48200 (8%)]\tLoss: 0.045606\n",
      "Train Epoch: 7 [4480/48200 (9%)]\tLoss: 0.149145\n",
      "Train Epoch: 7 [5120/48200 (11%)]\tLoss: 0.078998\n",
      "Train Epoch: 7 [5760/48200 (12%)]\tLoss: 0.070430\n",
      "Train Epoch: 7 [6400/48200 (13%)]\tLoss: 0.076293\n",
      "Train Epoch: 7 [7040/48200 (15%)]\tLoss: 0.116578\n",
      "Train Epoch: 7 [7680/48200 (16%)]\tLoss: 0.126962\n",
      "Train Epoch: 7 [8320/48200 (17%)]\tLoss: 0.043952\n",
      "Train Epoch: 7 [8960/48200 (19%)]\tLoss: 0.083119\n",
      "Train Epoch: 7 [9600/48200 (20%)]\tLoss: 0.070314\n",
      "Train Epoch: 7 [10240/48200 (21%)]\tLoss: 0.042338\n",
      "Train Epoch: 7 [10880/48200 (23%)]\tLoss: 0.094988\n",
      "Train Epoch: 7 [11520/48200 (24%)]\tLoss: 0.029344\n",
      "Train Epoch: 7 [12160/48200 (25%)]\tLoss: 0.055048\n",
      "Train Epoch: 7 [12800/48200 (27%)]\tLoss: 0.142428\n",
      "Train Epoch: 7 [13440/48200 (28%)]\tLoss: 0.101472\n",
      "Train Epoch: 7 [14080/48200 (29%)]\tLoss: 0.097700\n",
      "Train Epoch: 7 [14720/48200 (31%)]\tLoss: 0.144238\n",
      "Train Epoch: 7 [15360/48200 (32%)]\tLoss: 0.069676\n",
      "Train Epoch: 7 [16000/48200 (33%)]\tLoss: 0.051182\n",
      "Train Epoch: 7 [16640/48200 (34%)]\tLoss: 0.124255\n",
      "Train Epoch: 7 [17280/48200 (36%)]\tLoss: 0.052865\n",
      "Train Epoch: 7 [17920/48200 (37%)]\tLoss: 0.091200\n",
      "Train Epoch: 7 [18560/48200 (38%)]\tLoss: 0.094417\n",
      "Train Epoch: 7 [19200/48200 (40%)]\tLoss: 0.022186\n",
      "Train Epoch: 7 [19840/48200 (41%)]\tLoss: 0.180385\n",
      "Train Epoch: 7 [20480/48200 (42%)]\tLoss: 0.020810\n",
      "Train Epoch: 7 [21120/48200 (44%)]\tLoss: 0.040024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [21760/48200 (45%)]\tLoss: 0.197354\n",
      "Train Epoch: 7 [22400/48200 (46%)]\tLoss: 0.047364\n",
      "Train Epoch: 7 [23040/48200 (48%)]\tLoss: 0.032852\n",
      "Train Epoch: 7 [23680/48200 (49%)]\tLoss: 0.137532\n",
      "Train Epoch: 7 [24320/48200 (50%)]\tLoss: 0.207822\n",
      "Train Epoch: 7 [24960/48200 (52%)]\tLoss: 0.027329\n",
      "Train Epoch: 7 [25600/48200 (53%)]\tLoss: 0.139072\n",
      "Train Epoch: 7 [26240/48200 (54%)]\tLoss: 0.066644\n",
      "Train Epoch: 7 [26880/48200 (56%)]\tLoss: 0.038952\n",
      "Train Epoch: 7 [27520/48200 (57%)]\tLoss: 0.109432\n",
      "Train Epoch: 7 [28160/48200 (58%)]\tLoss: 0.093271\n",
      "Train Epoch: 7 [28800/48200 (60%)]\tLoss: 0.045407\n",
      "Train Epoch: 7 [29440/48200 (61%)]\tLoss: 0.084699\n",
      "Train Epoch: 7 [30080/48200 (62%)]\tLoss: 0.141938\n",
      "Train Epoch: 7 [30720/48200 (64%)]\tLoss: 0.083401\n",
      "Train Epoch: 7 [31360/48200 (65%)]\tLoss: 0.030332\n",
      "Train Epoch: 7 [32000/48200 (66%)]\tLoss: 0.080836\n",
      "Train Epoch: 7 [32640/48200 (68%)]\tLoss: 0.016513\n",
      "Train Epoch: 7 [33280/48200 (69%)]\tLoss: 0.037777\n",
      "Train Epoch: 7 [33920/48200 (70%)]\tLoss: 0.081834\n",
      "Train Epoch: 7 [34560/48200 (72%)]\tLoss: 0.063227\n",
      "Train Epoch: 7 [35200/48200 (73%)]\tLoss: 0.130207\n",
      "Train Epoch: 7 [35840/48200 (74%)]\tLoss: 0.155149\n",
      "Train Epoch: 7 [36480/48200 (76%)]\tLoss: 0.052672\n",
      "Train Epoch: 7 [37120/48200 (77%)]\tLoss: 0.166787\n",
      "Train Epoch: 7 [37760/48200 (78%)]\tLoss: 0.127096\n",
      "Train Epoch: 7 [38400/48200 (80%)]\tLoss: 0.222251\n",
      "Train Epoch: 7 [39040/48200 (81%)]\tLoss: 0.076272\n",
      "Train Epoch: 7 [39680/48200 (82%)]\tLoss: 0.255763\n",
      "Train Epoch: 7 [40320/48200 (84%)]\tLoss: 0.113714\n",
      "Train Epoch: 7 [40960/48200 (85%)]\tLoss: 0.144947\n",
      "Train Epoch: 7 [41600/48200 (86%)]\tLoss: 0.086660\n",
      "Train Epoch: 7 [42240/48200 (88%)]\tLoss: 0.179914\n",
      "Train Epoch: 7 [42880/48200 (89%)]\tLoss: 0.090166\n",
      "Train Epoch: 7 [43520/48200 (90%)]\tLoss: 0.118695\n",
      "Train Epoch: 7 [44160/48200 (92%)]\tLoss: 0.115286\n",
      "Train Epoch: 7 [44800/48200 (93%)]\tLoss: 0.167979\n",
      "Train Epoch: 7 [45440/48200 (94%)]\tLoss: 0.127269\n",
      "Train Epoch: 7 [46080/48200 (95%)]\tLoss: 0.069729\n",
      "Train Epoch: 7 [46720/48200 (97%)]\tLoss: 0.102317\n",
      "Train Epoch: 7 [47360/48200 (98%)]\tLoss: 0.191068\n",
      "Train Epoch: 7 [48000/48200 (99%)]\tLoss: 0.070392\n",
      "\n",
      "Test set: Avg. loss: 0.0398, Accuracy: 7912/8017 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/48200 (0%)]\tLoss: 0.165749\n",
      "Train Epoch: 8 [640/48200 (1%)]\tLoss: 0.164374\n",
      "Train Epoch: 8 [1280/48200 (3%)]\tLoss: 0.106285\n",
      "Train Epoch: 8 [1920/48200 (4%)]\tLoss: 0.067718\n",
      "Train Epoch: 8 [2560/48200 (5%)]\tLoss: 0.062619\n",
      "Train Epoch: 8 [3200/48200 (7%)]\tLoss: 0.105703\n",
      "Train Epoch: 8 [3840/48200 (8%)]\tLoss: 0.072849\n",
      "Train Epoch: 8 [4480/48200 (9%)]\tLoss: 0.035989\n",
      "Train Epoch: 8 [5120/48200 (11%)]\tLoss: 0.045151\n",
      "Train Epoch: 8 [5760/48200 (12%)]\tLoss: 0.329873\n",
      "Train Epoch: 8 [6400/48200 (13%)]\tLoss: 0.089611\n",
      "Train Epoch: 8 [7040/48200 (15%)]\tLoss: 0.196265\n",
      "Train Epoch: 8 [7680/48200 (16%)]\tLoss: 0.062386\n",
      "Train Epoch: 8 [8320/48200 (17%)]\tLoss: 0.193779\n",
      "Train Epoch: 8 [8960/48200 (19%)]\tLoss: 0.065219\n",
      "Train Epoch: 8 [9600/48200 (20%)]\tLoss: 0.151034\n",
      "Train Epoch: 8 [10240/48200 (21%)]\tLoss: 0.024665\n",
      "Train Epoch: 8 [10880/48200 (23%)]\tLoss: 0.038073\n",
      "Train Epoch: 8 [11520/48200 (24%)]\tLoss: 0.103913\n",
      "Train Epoch: 8 [12160/48200 (25%)]\tLoss: 0.080172\n",
      "Train Epoch: 8 [12800/48200 (27%)]\tLoss: 0.016111\n",
      "Train Epoch: 8 [13440/48200 (28%)]\tLoss: 0.185504\n",
      "Train Epoch: 8 [14080/48200 (29%)]\tLoss: 0.132444\n",
      "Train Epoch: 8 [14720/48200 (31%)]\tLoss: 0.156397\n",
      "Train Epoch: 8 [15360/48200 (32%)]\tLoss: 0.038307\n",
      "Train Epoch: 8 [16000/48200 (33%)]\tLoss: 0.077751\n",
      "Train Epoch: 8 [16640/48200 (34%)]\tLoss: 0.050657\n",
      "Train Epoch: 8 [17280/48200 (36%)]\tLoss: 0.049806\n",
      "Train Epoch: 8 [17920/48200 (37%)]\tLoss: 0.131329\n",
      "Train Epoch: 8 [18560/48200 (38%)]\tLoss: 0.047728\n",
      "Train Epoch: 8 [19200/48200 (40%)]\tLoss: 0.046934\n",
      "Train Epoch: 8 [19840/48200 (41%)]\tLoss: 0.084180\n",
      "Train Epoch: 8 [20480/48200 (42%)]\tLoss: 0.168245\n",
      "Train Epoch: 8 [21120/48200 (44%)]\tLoss: 0.063198\n",
      "Train Epoch: 8 [21760/48200 (45%)]\tLoss: 0.096822\n",
      "Train Epoch: 8 [22400/48200 (46%)]\tLoss: 0.066050\n",
      "Train Epoch: 8 [23040/48200 (48%)]\tLoss: 0.186810\n",
      "Train Epoch: 8 [23680/48200 (49%)]\tLoss: 0.060539\n",
      "Train Epoch: 8 [24320/48200 (50%)]\tLoss: 0.025829\n",
      "Train Epoch: 8 [24960/48200 (52%)]\tLoss: 0.068021\n",
      "Train Epoch: 8 [25600/48200 (53%)]\tLoss: 0.141153\n",
      "Train Epoch: 8 [26240/48200 (54%)]\tLoss: 0.146004\n",
      "Train Epoch: 8 [26880/48200 (56%)]\tLoss: 0.074001\n",
      "Train Epoch: 8 [27520/48200 (57%)]\tLoss: 0.103053\n",
      "Train Epoch: 8 [28160/48200 (58%)]\tLoss: 0.032789\n",
      "Train Epoch: 8 [28800/48200 (60%)]\tLoss: 0.028655\n",
      "Train Epoch: 8 [29440/48200 (61%)]\tLoss: 0.018393\n",
      "Train Epoch: 8 [30080/48200 (62%)]\tLoss: 0.056336\n",
      "Train Epoch: 8 [30720/48200 (64%)]\tLoss: 0.017960\n",
      "Train Epoch: 8 [31360/48200 (65%)]\tLoss: 0.020713\n",
      "Train Epoch: 8 [32000/48200 (66%)]\tLoss: 0.361488\n",
      "Train Epoch: 8 [32640/48200 (68%)]\tLoss: 0.065309\n",
      "Train Epoch: 8 [33280/48200 (69%)]\tLoss: 0.021431\n",
      "Train Epoch: 8 [33920/48200 (70%)]\tLoss: 0.202734\n",
      "Train Epoch: 8 [34560/48200 (72%)]\tLoss: 0.250356\n",
      "Train Epoch: 8 [35200/48200 (73%)]\tLoss: 0.100870\n",
      "Train Epoch: 8 [35840/48200 (74%)]\tLoss: 0.079246\n",
      "Train Epoch: 8 [36480/48200 (76%)]\tLoss: 0.155725\n",
      "Train Epoch: 8 [37120/48200 (77%)]\tLoss: 0.153725\n",
      "Train Epoch: 8 [37760/48200 (78%)]\tLoss: 0.120886\n",
      "Train Epoch: 8 [38400/48200 (80%)]\tLoss: 0.023718\n",
      "Train Epoch: 8 [39040/48200 (81%)]\tLoss: 0.187503\n",
      "Train Epoch: 8 [39680/48200 (82%)]\tLoss: 0.115435\n",
      "Train Epoch: 8 [40320/48200 (84%)]\tLoss: 0.126801\n",
      "Train Epoch: 8 [40960/48200 (85%)]\tLoss: 0.115489\n",
      "Train Epoch: 8 [41600/48200 (86%)]\tLoss: 0.034532\n",
      "Train Epoch: 8 [42240/48200 (88%)]\tLoss: 0.103683\n",
      "Train Epoch: 8 [42880/48200 (89%)]\tLoss: 0.044909\n",
      "Train Epoch: 8 [43520/48200 (90%)]\tLoss: 0.147917\n",
      "Train Epoch: 8 [44160/48200 (92%)]\tLoss: 0.053121\n",
      "Train Epoch: 8 [44800/48200 (93%)]\tLoss: 0.055269\n",
      "Train Epoch: 8 [45440/48200 (94%)]\tLoss: 0.067809\n",
      "Train Epoch: 8 [46080/48200 (95%)]\tLoss: 0.113732\n",
      "Train Epoch: 8 [46720/48200 (97%)]\tLoss: 0.082486\n",
      "Train Epoch: 8 [47360/48200 (98%)]\tLoss: 0.209967\n",
      "Train Epoch: 8 [48000/48200 (99%)]\tLoss: 0.073693\n",
      "\n",
      "Test set: Avg. loss: 0.0362, Accuracy: 7922/8017 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/48200 (0%)]\tLoss: 0.088912\n",
      "Train Epoch: 9 [640/48200 (1%)]\tLoss: 0.103999\n",
      "Train Epoch: 9 [1280/48200 (3%)]\tLoss: 0.060271\n",
      "Train Epoch: 9 [1920/48200 (4%)]\tLoss: 0.120511\n",
      "Train Epoch: 9 [2560/48200 (5%)]\tLoss: 0.041413\n",
      "Train Epoch: 9 [3200/48200 (7%)]\tLoss: 0.093639\n",
      "Train Epoch: 9 [3840/48200 (8%)]\tLoss: 0.089564\n",
      "Train Epoch: 9 [4480/48200 (9%)]\tLoss: 0.108022\n",
      "Train Epoch: 9 [5120/48200 (11%)]\tLoss: 0.083867\n",
      "Train Epoch: 9 [5760/48200 (12%)]\tLoss: 0.056180\n",
      "Train Epoch: 9 [6400/48200 (13%)]\tLoss: 0.046134\n",
      "Train Epoch: 9 [7040/48200 (15%)]\tLoss: 0.216183\n",
      "Train Epoch: 9 [7680/48200 (16%)]\tLoss: 0.048730\n",
      "Train Epoch: 9 [8320/48200 (17%)]\tLoss: 0.025250\n",
      "Train Epoch: 9 [8960/48200 (19%)]\tLoss: 0.069339\n",
      "Train Epoch: 9 [9600/48200 (20%)]\tLoss: 0.043539\n",
      "Train Epoch: 9 [10240/48200 (21%)]\tLoss: 0.106499\n",
      "Train Epoch: 9 [10880/48200 (23%)]\tLoss: 0.082160\n",
      "Train Epoch: 9 [11520/48200 (24%)]\tLoss: 0.074968\n",
      "Train Epoch: 9 [12160/48200 (25%)]\tLoss: 0.086289\n",
      "Train Epoch: 9 [12800/48200 (27%)]\tLoss: 0.094249\n",
      "Train Epoch: 9 [13440/48200 (28%)]\tLoss: 0.027550\n",
      "Train Epoch: 9 [14080/48200 (29%)]\tLoss: 0.148966\n",
      "Train Epoch: 9 [14720/48200 (31%)]\tLoss: 0.198977\n",
      "Train Epoch: 9 [15360/48200 (32%)]\tLoss: 0.248231\n",
      "Train Epoch: 9 [16000/48200 (33%)]\tLoss: 0.090716\n",
      "Train Epoch: 9 [16640/48200 (34%)]\tLoss: 0.055757\n",
      "Train Epoch: 9 [17280/48200 (36%)]\tLoss: 0.023160\n",
      "Train Epoch: 9 [17920/48200 (37%)]\tLoss: 0.069338\n",
      "Train Epoch: 9 [18560/48200 (38%)]\tLoss: 0.041337\n",
      "Train Epoch: 9 [19200/48200 (40%)]\tLoss: 0.083662\n",
      "Train Epoch: 9 [19840/48200 (41%)]\tLoss: 0.025527\n",
      "Train Epoch: 9 [20480/48200 (42%)]\tLoss: 0.054883\n",
      "Train Epoch: 9 [21120/48200 (44%)]\tLoss: 0.030347\n",
      "Train Epoch: 9 [21760/48200 (45%)]\tLoss: 0.117154\n",
      "Train Epoch: 9 [22400/48200 (46%)]\tLoss: 0.104155\n",
      "Train Epoch: 9 [23040/48200 (48%)]\tLoss: 0.106779\n",
      "Train Epoch: 9 [23680/48200 (49%)]\tLoss: 0.062997\n",
      "Train Epoch: 9 [24320/48200 (50%)]\tLoss: 0.112025\n",
      "Train Epoch: 9 [24960/48200 (52%)]\tLoss: 0.064655\n",
      "Train Epoch: 9 [25600/48200 (53%)]\tLoss: 0.041233\n",
      "Train Epoch: 9 [26240/48200 (54%)]\tLoss: 0.031038\n",
      "Train Epoch: 9 [26880/48200 (56%)]\tLoss: 0.131303\n",
      "Train Epoch: 9 [27520/48200 (57%)]\tLoss: 0.060162\n",
      "Train Epoch: 9 [28160/48200 (58%)]\tLoss: 0.080154\n",
      "Train Epoch: 9 [28800/48200 (60%)]\tLoss: 0.039678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [29440/48200 (61%)]\tLoss: 0.189876\n",
      "Train Epoch: 9 [30080/48200 (62%)]\tLoss: 0.132867\n",
      "Train Epoch: 9 [30720/48200 (64%)]\tLoss: 0.058648\n",
      "Train Epoch: 9 [31360/48200 (65%)]\tLoss: 0.236816\n",
      "Train Epoch: 9 [32000/48200 (66%)]\tLoss: 0.030990\n",
      "Train Epoch: 9 [32640/48200 (68%)]\tLoss: 0.032491\n",
      "Train Epoch: 9 [33280/48200 (69%)]\tLoss: 0.190632\n",
      "Train Epoch: 9 [33920/48200 (70%)]\tLoss: 0.072591\n",
      "Train Epoch: 9 [34560/48200 (72%)]\tLoss: 0.203781\n",
      "Train Epoch: 9 [35200/48200 (73%)]\tLoss: 0.035547\n",
      "Train Epoch: 9 [35840/48200 (74%)]\tLoss: 0.032931\n",
      "Train Epoch: 9 [36480/48200 (76%)]\tLoss: 0.233333\n",
      "Train Epoch: 9 [37120/48200 (77%)]\tLoss: 0.085153\n",
      "Train Epoch: 9 [37760/48200 (78%)]\tLoss: 0.030904\n",
      "Train Epoch: 9 [38400/48200 (80%)]\tLoss: 0.034929\n",
      "Train Epoch: 9 [39040/48200 (81%)]\tLoss: 0.096662\n",
      "Train Epoch: 9 [39680/48200 (82%)]\tLoss: 0.164802\n",
      "Train Epoch: 9 [40320/48200 (84%)]\tLoss: 0.099479\n",
      "Train Epoch: 9 [40960/48200 (85%)]\tLoss: 0.063189\n",
      "Train Epoch: 9 [41600/48200 (86%)]\tLoss: 0.013310\n",
      "Train Epoch: 9 [42240/48200 (88%)]\tLoss: 0.066646\n",
      "Train Epoch: 9 [42880/48200 (89%)]\tLoss: 0.145077\n",
      "Train Epoch: 9 [43520/48200 (90%)]\tLoss: 0.038226\n",
      "Train Epoch: 9 [44160/48200 (92%)]\tLoss: 0.121871\n",
      "Train Epoch: 9 [44800/48200 (93%)]\tLoss: 0.127237\n",
      "Train Epoch: 9 [45440/48200 (94%)]\tLoss: 0.240554\n",
      "Train Epoch: 9 [46080/48200 (95%)]\tLoss: 0.172658\n",
      "Train Epoch: 9 [46720/48200 (97%)]\tLoss: 0.046960\n",
      "Train Epoch: 9 [47360/48200 (98%)]\tLoss: 0.092241\n",
      "Train Epoch: 9 [48000/48200 (99%)]\tLoss: 0.210300\n",
      "\n",
      "Test set: Avg. loss: 0.0358, Accuracy: 7917/8017 (98%)\n",
      "\n",
      "Train Epoch: 10 [0/48200 (0%)]\tLoss: 0.033202\n",
      "Train Epoch: 10 [640/48200 (1%)]\tLoss: 0.036244\n",
      "Train Epoch: 10 [1280/48200 (3%)]\tLoss: 0.111580\n",
      "Train Epoch: 10 [1920/48200 (4%)]\tLoss: 0.016421\n",
      "Train Epoch: 10 [2560/48200 (5%)]\tLoss: 0.096840\n",
      "Train Epoch: 10 [3200/48200 (7%)]\tLoss: 0.161580\n",
      "Train Epoch: 10 [3840/48200 (8%)]\tLoss: 0.166977\n",
      "Train Epoch: 10 [4480/48200 (9%)]\tLoss: 0.126473\n",
      "Train Epoch: 10 [5120/48200 (11%)]\tLoss: 0.182136\n",
      "Train Epoch: 10 [5760/48200 (12%)]\tLoss: 0.142870\n",
      "Train Epoch: 10 [6400/48200 (13%)]\tLoss: 0.194525\n",
      "Train Epoch: 10 [7040/48200 (15%)]\tLoss: 0.092773\n",
      "Train Epoch: 10 [7680/48200 (16%)]\tLoss: 0.014792\n",
      "Train Epoch: 10 [8320/48200 (17%)]\tLoss: 0.274553\n",
      "Train Epoch: 10 [8960/48200 (19%)]\tLoss: 0.021665\n",
      "Train Epoch: 10 [9600/48200 (20%)]\tLoss: 0.085809\n",
      "Train Epoch: 10 [10240/48200 (21%)]\tLoss: 0.079803\n",
      "Train Epoch: 10 [10880/48200 (23%)]\tLoss: 0.081674\n",
      "Train Epoch: 10 [11520/48200 (24%)]\tLoss: 0.067576\n",
      "Train Epoch: 10 [12160/48200 (25%)]\tLoss: 0.075772\n",
      "Train Epoch: 10 [12800/48200 (27%)]\tLoss: 0.051176\n",
      "Train Epoch: 10 [13440/48200 (28%)]\tLoss: 0.017329\n",
      "Train Epoch: 10 [14080/48200 (29%)]\tLoss: 0.018178\n",
      "Train Epoch: 10 [14720/48200 (31%)]\tLoss: 0.047210\n",
      "Train Epoch: 10 [15360/48200 (32%)]\tLoss: 0.112511\n",
      "Train Epoch: 10 [16000/48200 (33%)]\tLoss: 0.051284\n",
      "Train Epoch: 10 [16640/48200 (34%)]\tLoss: 0.017210\n",
      "Train Epoch: 10 [17280/48200 (36%)]\tLoss: 0.082680\n",
      "Train Epoch: 10 [17920/48200 (37%)]\tLoss: 0.094697\n",
      "Train Epoch: 10 [18560/48200 (38%)]\tLoss: 0.035932\n",
      "Train Epoch: 10 [19200/48200 (40%)]\tLoss: 0.073549\n",
      "Train Epoch: 10 [19840/48200 (41%)]\tLoss: 0.233264\n",
      "Train Epoch: 10 [20480/48200 (42%)]\tLoss: 0.044707\n",
      "Train Epoch: 10 [21120/48200 (44%)]\tLoss: 0.088090\n",
      "Train Epoch: 10 [21760/48200 (45%)]\tLoss: 0.120011\n",
      "Train Epoch: 10 [22400/48200 (46%)]\tLoss: 0.171006\n",
      "Train Epoch: 10 [23040/48200 (48%)]\tLoss: 0.133135\n",
      "Train Epoch: 10 [23680/48200 (49%)]\tLoss: 0.046202\n",
      "Train Epoch: 10 [24320/48200 (50%)]\tLoss: 0.066947\n",
      "Train Epoch: 10 [24960/48200 (52%)]\tLoss: 0.146723\n",
      "Train Epoch: 10 [25600/48200 (53%)]\tLoss: 0.031726\n",
      "Train Epoch: 10 [26240/48200 (54%)]\tLoss: 0.191726\n",
      "Train Epoch: 10 [26880/48200 (56%)]\tLoss: 0.196831\n",
      "Train Epoch: 10 [27520/48200 (57%)]\tLoss: 0.084825\n",
      "Train Epoch: 10 [28160/48200 (58%)]\tLoss: 0.013585\n",
      "Train Epoch: 10 [28800/48200 (60%)]\tLoss: 0.050167\n",
      "Train Epoch: 10 [29440/48200 (61%)]\tLoss: 0.108929\n",
      "Train Epoch: 10 [30080/48200 (62%)]\tLoss: 0.066674\n",
      "Train Epoch: 10 [30720/48200 (64%)]\tLoss: 0.028768\n",
      "Train Epoch: 10 [31360/48200 (65%)]\tLoss: 0.038414\n",
      "Train Epoch: 10 [32000/48200 (66%)]\tLoss: 0.063772\n",
      "Train Epoch: 10 [32640/48200 (68%)]\tLoss: 0.272656\n",
      "Train Epoch: 10 [33280/48200 (69%)]\tLoss: 0.042307\n",
      "Train Epoch: 10 [33920/48200 (70%)]\tLoss: 0.059630\n",
      "Train Epoch: 10 [34560/48200 (72%)]\tLoss: 0.038600\n",
      "Train Epoch: 10 [35200/48200 (73%)]\tLoss: 0.183556\n",
      "Train Epoch: 10 [35840/48200 (74%)]\tLoss: 0.070899\n",
      "Train Epoch: 10 [36480/48200 (76%)]\tLoss: 0.075144\n",
      "Train Epoch: 10 [37120/48200 (77%)]\tLoss: 0.081028\n",
      "Train Epoch: 10 [37760/48200 (78%)]\tLoss: 0.051733\n",
      "Train Epoch: 10 [38400/48200 (80%)]\tLoss: 0.017393\n",
      "Train Epoch: 10 [39040/48200 (81%)]\tLoss: 0.254735\n",
      "Train Epoch: 10 [39680/48200 (82%)]\tLoss: 0.021014\n",
      "Train Epoch: 10 [40320/48200 (84%)]\tLoss: 0.025346\n",
      "Train Epoch: 10 [40960/48200 (85%)]\tLoss: 0.034032\n",
      "Train Epoch: 10 [41600/48200 (86%)]\tLoss: 0.092987\n",
      "Train Epoch: 10 [42240/48200 (88%)]\tLoss: 0.054272\n",
      "Train Epoch: 10 [42880/48200 (89%)]\tLoss: 0.027992\n",
      "Train Epoch: 10 [43520/48200 (90%)]\tLoss: 0.075179\n",
      "Train Epoch: 10 [44160/48200 (92%)]\tLoss: 0.098713\n",
      "Train Epoch: 10 [44800/48200 (93%)]\tLoss: 0.087286\n",
      "Train Epoch: 10 [45440/48200 (94%)]\tLoss: 0.158231\n",
      "Train Epoch: 10 [46080/48200 (95%)]\tLoss: 0.068092\n",
      "Train Epoch: 10 [46720/48200 (97%)]\tLoss: 0.309926\n",
      "Train Epoch: 10 [47360/48200 (98%)]\tLoss: 0.085340\n",
      "Train Epoch: 10 [48000/48200 (99%)]\tLoss: 0.154545\n",
      "\n",
      "Test set: Avg. loss: 0.0317, Accuracy: 7924/8017 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net2_c8 = Net2_c8().cuda()\n",
    "n_epochs = 10\n",
    "optimizer = optim.SGD(net2_c8.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader8.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "test(net2_c8, test_loader8)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(net2_c8, train_loader8, epoch)\n",
    "    test(net2_c8, test_loader8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naiqi/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.00354959, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.01064877, 0.        , 0.01419837, 0.01419837,\n",
       "        0.02484714, 0.02129755, 0.03549591, 0.02484714, 0.03904551,\n",
       "        0.06389265, 0.04614469, 0.04969428, 0.09583897, 0.07454142,\n",
       "        0.10648774, 0.10648774, 0.13133488, 0.09938856, 0.14908284,\n",
       "        0.17392998, 0.14908284, 0.13843407, 0.19877712, 0.18457875,\n",
       "        0.13843407, 0.19167794, 0.21652508, 0.17392998, 0.22717385,\n",
       "        0.16328121, 0.17038039, 0.24492181, 0.19522753, 0.18102916,\n",
       "        0.19167794, 0.19167794, 0.17747957, 0.16328121, 0.14198366,\n",
       "        0.15973161, 0.14198366, 0.17392998, 0.11003733, 0.16328121,\n",
       "        0.15618202, 0.11358693, 0.13488447, 0.11358693, 0.07454142,\n",
       "        0.08873979, 0.1242357 , 0.06034305, 0.04969428, 0.05324387,\n",
       "        0.07809101, 0.04969428, 0.03904551, 0.06389265, 0.04969428,\n",
       "        0.05679346, 0.03194632, 0.01064877, 0.02839673, 0.02129755,\n",
       "        0.01064877, 0.02839673, 0.01774796, 0.01419837, 0.02484714,\n",
       "        0.02484714, 0.00709918, 0.00354959, 0.00354959, 0.01064877,\n",
       "        0.00354959, 0.01064877, 0.00354959, 0.00709918, 0.00354959,\n",
       "        0.02129755, 0.        , 0.00709918, 0.        , 0.00354959,\n",
       "        0.00354959, 0.        , 0.00354959, 0.00354959, 0.00354959,\n",
       "        0.00354959, 0.        , 0.        , 0.        , 0.00354959]),\n",
       " array([ 6.75232423,  6.89318551,  7.03404679,  7.17490807,  7.31576935,\n",
       "         7.45663063,  7.59749192,  7.7383532 ,  7.87921448,  8.02007576,\n",
       "         8.16093704,  8.30179833,  8.44265961,  8.58352089,  8.72438217,\n",
       "         8.86524345,  9.00610474,  9.14696602,  9.2878273 ,  9.42868858,\n",
       "         9.56954986,  9.71041115,  9.85127243,  9.99213371, 10.13299499,\n",
       "        10.27385627, 10.41471755, 10.55557884, 10.69644012, 10.8373014 ,\n",
       "        10.97816268, 11.11902396, 11.25988525, 11.40074653, 11.54160781,\n",
       "        11.68246909, 11.82333037, 11.96419166, 12.10505294, 12.24591422,\n",
       "        12.3867755 , 12.52763678, 12.66849807, 12.80935935, 12.95022063,\n",
       "        13.09108191, 13.23194319, 13.37280447, 13.51366576, 13.65452704,\n",
       "        13.79538832, 13.9362496 , 14.07711088, 14.21797217, 14.35883345,\n",
       "        14.49969473, 14.64055601, 14.78141729, 14.92227858, 15.06313986,\n",
       "        15.20400114, 15.34486242, 15.4857237 , 15.62658499, 15.76744627,\n",
       "        15.90830755, 16.04916883, 16.19003011, 16.33089139, 16.47175268,\n",
       "        16.61261396, 16.75347524, 16.89433652, 17.0351978 , 17.17605909,\n",
       "        17.31692037, 17.45778165, 17.59864293, 17.73950421, 17.8803655 ,\n",
       "        18.02122678, 18.16208806, 18.30294934, 18.44381062, 18.5846719 ,\n",
       "        18.72553319, 18.86639447, 19.00725575, 19.14811703, 19.28897831,\n",
       "        19.4298396 , 19.57070088, 19.71156216, 19.85242344, 19.99328472,\n",
       "        20.13414601, 20.27500729, 20.41586857, 20.55672985, 20.69759113,\n",
       "        20.83845242]),\n",
       " <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = net2_c8(noise.cuda())\n",
    "ll = net2_c8.last\n",
    "print(ll.shape)\n",
    "ll = ll.cpu().detach().numpy() \n",
    "lid_net2_c8 = LID(ll, ll, k=50)\n",
    "plt.hist(lid_net2_c8, bins=100, normed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.791598638637002\n",
      "13.379641296242637\n",
      "9.588037746069197\n",
      "12.34017129407341\n"
     ]
    }
   ],
   "source": [
    "print(lid_net1_c10.mean())\n",
    "print(lid_net2_c10.mean())\n",
    "\n",
    "print(lid_net1_c8.mean())\n",
    "print(lid_net2_c8.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network using 0-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_c5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_c5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        self.last = x\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naiqi/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 1.6136, Accuracy: 776/5139 (15%)\n",
      "\n",
      "Train Epoch: 1 [0/30596 (0%)]\tLoss: 1.624736\n",
      "Train Epoch: 1 [640/30596 (2%)]\tLoss: 1.574385\n",
      "Train Epoch: 1 [1280/30596 (4%)]\tLoss: 1.527636\n",
      "Train Epoch: 1 [1920/30596 (6%)]\tLoss: 1.447317\n",
      "Train Epoch: 1 [2560/30596 (8%)]\tLoss: 1.273492\n",
      "Train Epoch: 1 [3200/30596 (10%)]\tLoss: 1.208273\n",
      "Train Epoch: 1 [3840/30596 (13%)]\tLoss: 1.018849\n",
      "Train Epoch: 1 [4480/30596 (15%)]\tLoss: 0.882609\n",
      "Train Epoch: 1 [5120/30596 (17%)]\tLoss: 0.804402\n",
      "Train Epoch: 1 [5760/30596 (19%)]\tLoss: 0.896352\n",
      "Train Epoch: 1 [6400/30596 (21%)]\tLoss: 0.567690\n",
      "Train Epoch: 1 [7040/30596 (23%)]\tLoss: 0.417135\n",
      "Train Epoch: 1 [7680/30596 (25%)]\tLoss: 0.492278\n",
      "Train Epoch: 1 [8320/30596 (27%)]\tLoss: 0.576188\n",
      "Train Epoch: 1 [8960/30596 (29%)]\tLoss: 0.281412\n",
      "Train Epoch: 1 [9600/30596 (31%)]\tLoss: 0.420193\n",
      "Train Epoch: 1 [10240/30596 (33%)]\tLoss: 0.406761\n",
      "Train Epoch: 1 [10880/30596 (35%)]\tLoss: 0.440940\n",
      "Train Epoch: 1 [11520/30596 (38%)]\tLoss: 0.268332\n",
      "Train Epoch: 1 [12160/30596 (40%)]\tLoss: 0.358981\n",
      "Train Epoch: 1 [12800/30596 (42%)]\tLoss: 0.323008\n",
      "Train Epoch: 1 [13440/30596 (44%)]\tLoss: 0.307929\n",
      "Train Epoch: 1 [14080/30596 (46%)]\tLoss: 0.234625\n",
      "Train Epoch: 1 [14720/30596 (48%)]\tLoss: 0.154203\n",
      "Train Epoch: 1 [15360/30596 (50%)]\tLoss: 0.267372\n",
      "Train Epoch: 1 [16000/30596 (52%)]\tLoss: 0.242541\n",
      "Train Epoch: 1 [16640/30596 (54%)]\tLoss: 0.200308\n",
      "Train Epoch: 1 [17280/30596 (56%)]\tLoss: 0.121298\n",
      "Train Epoch: 1 [17920/30596 (58%)]\tLoss: 0.174622\n",
      "Train Epoch: 1 [18560/30596 (61%)]\tLoss: 0.163233\n",
      "Train Epoch: 1 [19200/30596 (63%)]\tLoss: 0.311072\n",
      "Train Epoch: 1 [19840/30596 (65%)]\tLoss: 0.270219\n",
      "Train Epoch: 1 [20480/30596 (67%)]\tLoss: 0.273834\n",
      "Train Epoch: 1 [21120/30596 (69%)]\tLoss: 0.193656\n",
      "Train Epoch: 1 [21760/30596 (71%)]\tLoss: 0.219028\n",
      "Train Epoch: 1 [22400/30596 (73%)]\tLoss: 0.205612\n",
      "Train Epoch: 1 [23040/30596 (75%)]\tLoss: 0.364003\n",
      "Train Epoch: 1 [23680/30596 (77%)]\tLoss: 0.298925\n",
      "Train Epoch: 1 [24320/30596 (79%)]\tLoss: 0.152416\n",
      "Train Epoch: 1 [24960/30596 (81%)]\tLoss: 0.199741\n",
      "Train Epoch: 1 [25600/30596 (84%)]\tLoss: 0.203551\n",
      "Train Epoch: 1 [26240/30596 (86%)]\tLoss: 0.105790\n",
      "Train Epoch: 1 [26880/30596 (88%)]\tLoss: 0.224142\n",
      "Train Epoch: 1 [27520/30596 (90%)]\tLoss: 0.280064\n",
      "Train Epoch: 1 [28160/30596 (92%)]\tLoss: 0.389920\n",
      "Train Epoch: 1 [28800/30596 (94%)]\tLoss: 0.182564\n",
      "Train Epoch: 1 [29440/30596 (96%)]\tLoss: 0.255095\n",
      "Train Epoch: 1 [30080/30596 (98%)]\tLoss: 0.156606\n",
      "\n",
      "Test set: Avg. loss: 0.0909, Accuracy: 4987/5139 (97%)\n",
      "\n",
      "Train Epoch: 2 [0/30596 (0%)]\tLoss: 0.137275\n",
      "Train Epoch: 2 [640/30596 (2%)]\tLoss: 0.152972\n",
      "Train Epoch: 2 [1280/30596 (4%)]\tLoss: 0.144199\n",
      "Train Epoch: 2 [1920/30596 (6%)]\tLoss: 0.157030\n",
      "Train Epoch: 2 [2560/30596 (8%)]\tLoss: 0.348486\n",
      "Train Epoch: 2 [3200/30596 (10%)]\tLoss: 0.255165\n",
      "Train Epoch: 2 [3840/30596 (13%)]\tLoss: 0.203121\n",
      "Train Epoch: 2 [4480/30596 (15%)]\tLoss: 0.068761\n",
      "Train Epoch: 2 [5120/30596 (17%)]\tLoss: 0.093180\n",
      "Train Epoch: 2 [5760/30596 (19%)]\tLoss: 0.203261\n",
      "Train Epoch: 2 [6400/30596 (21%)]\tLoss: 0.386961\n",
      "Train Epoch: 2 [7040/30596 (23%)]\tLoss: 0.258468\n",
      "Train Epoch: 2 [7680/30596 (25%)]\tLoss: 0.170882\n",
      "Train Epoch: 2 [8320/30596 (27%)]\tLoss: 0.098310\n",
      "Train Epoch: 2 [8960/30596 (29%)]\tLoss: 0.161021\n",
      "Train Epoch: 2 [9600/30596 (31%)]\tLoss: 0.138076\n",
      "Train Epoch: 2 [10240/30596 (33%)]\tLoss: 0.214746\n",
      "Train Epoch: 2 [10880/30596 (35%)]\tLoss: 0.097835\n",
      "Train Epoch: 2 [11520/30596 (38%)]\tLoss: 0.216776\n",
      "Train Epoch: 2 [12160/30596 (40%)]\tLoss: 0.251119\n",
      "Train Epoch: 2 [12800/30596 (42%)]\tLoss: 0.198698\n",
      "Train Epoch: 2 [13440/30596 (44%)]\tLoss: 0.169587\n",
      "Train Epoch: 2 [14080/30596 (46%)]\tLoss: 0.235979\n",
      "Train Epoch: 2 [14720/30596 (48%)]\tLoss: 0.068575\n",
      "Train Epoch: 2 [15360/30596 (50%)]\tLoss: 0.182437\n",
      "Train Epoch: 2 [16000/30596 (52%)]\tLoss: 0.111788\n",
      "Train Epoch: 2 [16640/30596 (54%)]\tLoss: 0.148913\n",
      "Train Epoch: 2 [17280/30596 (56%)]\tLoss: 0.209666\n",
      "Train Epoch: 2 [17920/30596 (58%)]\tLoss: 0.093654\n",
      "Train Epoch: 2 [18560/30596 (61%)]\tLoss: 0.254856\n",
      "Train Epoch: 2 [19200/30596 (63%)]\tLoss: 0.123135\n",
      "Train Epoch: 2 [19840/30596 (65%)]\tLoss: 0.218364\n",
      "Train Epoch: 2 [20480/30596 (67%)]\tLoss: 0.115538\n",
      "Train Epoch: 2 [21120/30596 (69%)]\tLoss: 0.102910\n",
      "Train Epoch: 2 [21760/30596 (71%)]\tLoss: 0.388598\n",
      "Train Epoch: 2 [22400/30596 (73%)]\tLoss: 0.133294\n",
      "Train Epoch: 2 [23040/30596 (75%)]\tLoss: 0.136829\n",
      "Train Epoch: 2 [23680/30596 (77%)]\tLoss: 0.057187\n",
      "Train Epoch: 2 [24320/30596 (79%)]\tLoss: 0.104446\n",
      "Train Epoch: 2 [24960/30596 (81%)]\tLoss: 0.098329\n",
      "Train Epoch: 2 [25600/30596 (84%)]\tLoss: 0.175760\n",
      "Train Epoch: 2 [26240/30596 (86%)]\tLoss: 0.132970\n",
      "Train Epoch: 2 [26880/30596 (88%)]\tLoss: 0.122705\n",
      "Train Epoch: 2 [27520/30596 (90%)]\tLoss: 0.099494\n",
      "Train Epoch: 2 [28160/30596 (92%)]\tLoss: 0.132958\n",
      "Train Epoch: 2 [28800/30596 (94%)]\tLoss: 0.105343\n",
      "Train Epoch: 2 [29440/30596 (96%)]\tLoss: 0.334792\n",
      "Train Epoch: 2 [30080/30596 (98%)]\tLoss: 0.124023\n",
      "\n",
      "Test set: Avg. loss: 0.0436, Accuracy: 5064/5139 (98%)\n",
      "\n",
      "Train Epoch: 3 [0/30596 (0%)]\tLoss: 0.091953\n",
      "Train Epoch: 3 [640/30596 (2%)]\tLoss: 0.063611\n",
      "Train Epoch: 3 [1280/30596 (4%)]\tLoss: 0.173455\n",
      "Train Epoch: 3 [1920/30596 (6%)]\tLoss: 0.126004\n",
      "Train Epoch: 3 [2560/30596 (8%)]\tLoss: 0.056758\n",
      "Train Epoch: 3 [3200/30596 (10%)]\tLoss: 0.380061\n",
      "Train Epoch: 3 [3840/30596 (13%)]\tLoss: 0.044645\n",
      "Train Epoch: 3 [4480/30596 (15%)]\tLoss: 0.134727\n",
      "Train Epoch: 3 [5120/30596 (17%)]\tLoss: 0.163265\n",
      "Train Epoch: 3 [5760/30596 (19%)]\tLoss: 0.063852\n",
      "Train Epoch: 3 [6400/30596 (21%)]\tLoss: 0.232300\n",
      "Train Epoch: 3 [7040/30596 (23%)]\tLoss: 0.152093\n",
      "Train Epoch: 3 [7680/30596 (25%)]\tLoss: 0.200223\n",
      "Train Epoch: 3 [8320/30596 (27%)]\tLoss: 0.094148\n",
      "Train Epoch: 3 [8960/30596 (29%)]\tLoss: 0.093046\n",
      "Train Epoch: 3 [9600/30596 (31%)]\tLoss: 0.127484\n",
      "Train Epoch: 3 [10240/30596 (33%)]\tLoss: 0.103578\n",
      "Train Epoch: 3 [10880/30596 (35%)]\tLoss: 0.183276\n",
      "Train Epoch: 3 [11520/30596 (38%)]\tLoss: 0.182592\n",
      "Train Epoch: 3 [12160/30596 (40%)]\tLoss: 0.157766\n",
      "Train Epoch: 3 [12800/30596 (42%)]\tLoss: 0.130195\n",
      "Train Epoch: 3 [13440/30596 (44%)]\tLoss: 0.082038\n",
      "Train Epoch: 3 [14080/30596 (46%)]\tLoss: 0.076515\n",
      "Train Epoch: 3 [14720/30596 (48%)]\tLoss: 0.220154\n",
      "Train Epoch: 3 [15360/30596 (50%)]\tLoss: 0.111114\n",
      "Train Epoch: 3 [16000/30596 (52%)]\tLoss: 0.053586\n",
      "Train Epoch: 3 [16640/30596 (54%)]\tLoss: 0.167119\n",
      "Train Epoch: 3 [17280/30596 (56%)]\tLoss: 0.210926\n",
      "Train Epoch: 3 [17920/30596 (58%)]\tLoss: 0.097422\n",
      "Train Epoch: 3 [18560/30596 (61%)]\tLoss: 0.045570\n",
      "Train Epoch: 3 [19200/30596 (63%)]\tLoss: 0.225772\n",
      "Train Epoch: 3 [19840/30596 (65%)]\tLoss: 0.170177\n",
      "Train Epoch: 3 [20480/30596 (67%)]\tLoss: 0.055881\n",
      "Train Epoch: 3 [21120/30596 (69%)]\tLoss: 0.111733\n",
      "Train Epoch: 3 [21760/30596 (71%)]\tLoss: 0.080948\n",
      "Train Epoch: 3 [22400/30596 (73%)]\tLoss: 0.207030\n",
      "Train Epoch: 3 [23040/30596 (75%)]\tLoss: 0.041917\n",
      "Train Epoch: 3 [23680/30596 (77%)]\tLoss: 0.100142\n",
      "Train Epoch: 3 [24320/30596 (79%)]\tLoss: 0.097253\n",
      "Train Epoch: 3 [24960/30596 (81%)]\tLoss: 0.088844\n",
      "Train Epoch: 3 [25600/30596 (84%)]\tLoss: 0.166068\n",
      "Train Epoch: 3 [26240/30596 (86%)]\tLoss: 0.142585\n",
      "Train Epoch: 3 [26880/30596 (88%)]\tLoss: 0.443452\n",
      "Train Epoch: 3 [27520/30596 (90%)]\tLoss: 0.063585\n",
      "Train Epoch: 3 [28160/30596 (92%)]\tLoss: 0.059206\n",
      "Train Epoch: 3 [28800/30596 (94%)]\tLoss: 0.058355\n",
      "Train Epoch: 3 [29440/30596 (96%)]\tLoss: 0.126496\n",
      "Train Epoch: 3 [30080/30596 (98%)]\tLoss: 0.033947\n",
      "\n",
      "Test set: Avg. loss: 0.0325, Accuracy: 5079/5139 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/30596 (0%)]\tLoss: 0.148190\n",
      "Train Epoch: 4 [640/30596 (2%)]\tLoss: 0.090826\n",
      "Train Epoch: 4 [1280/30596 (4%)]\tLoss: 0.041932\n",
      "Train Epoch: 4 [1920/30596 (6%)]\tLoss: 0.070079\n",
      "Train Epoch: 4 [2560/30596 (8%)]\tLoss: 0.141138\n",
      "Train Epoch: 4 [3200/30596 (10%)]\tLoss: 0.174680\n",
      "Train Epoch: 4 [3840/30596 (13%)]\tLoss: 0.078737\n",
      "Train Epoch: 4 [4480/30596 (15%)]\tLoss: 0.096725\n",
      "Train Epoch: 4 [5120/30596 (17%)]\tLoss: 0.044022\n",
      "Train Epoch: 4 [5760/30596 (19%)]\tLoss: 0.053859\n",
      "Train Epoch: 4 [6400/30596 (21%)]\tLoss: 0.116002\n",
      "Train Epoch: 4 [7040/30596 (23%)]\tLoss: 0.085566\n",
      "Train Epoch: 4 [7680/30596 (25%)]\tLoss: 0.037648\n",
      "Train Epoch: 4 [8320/30596 (27%)]\tLoss: 0.122832\n",
      "Train Epoch: 4 [8960/30596 (29%)]\tLoss: 0.083985\n",
      "Train Epoch: 4 [9600/30596 (31%)]\tLoss: 0.044732\n",
      "Train Epoch: 4 [10240/30596 (33%)]\tLoss: 0.144432\n",
      "Train Epoch: 4 [10880/30596 (35%)]\tLoss: 0.111515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [11520/30596 (38%)]\tLoss: 0.117020\n",
      "Train Epoch: 4 [12160/30596 (40%)]\tLoss: 0.153038\n",
      "Train Epoch: 4 [12800/30596 (42%)]\tLoss: 0.075280\n",
      "Train Epoch: 4 [13440/30596 (44%)]\tLoss: 0.154982\n",
      "Train Epoch: 4 [14080/30596 (46%)]\tLoss: 0.097636\n",
      "Train Epoch: 4 [14720/30596 (48%)]\tLoss: 0.156357\n",
      "Train Epoch: 4 [15360/30596 (50%)]\tLoss: 0.386735\n",
      "Train Epoch: 4 [16000/30596 (52%)]\tLoss: 0.066318\n",
      "Train Epoch: 4 [16640/30596 (54%)]\tLoss: 0.036759\n",
      "Train Epoch: 4 [17280/30596 (56%)]\tLoss: 0.059006\n",
      "Train Epoch: 4 [17920/30596 (58%)]\tLoss: 0.166732\n",
      "Train Epoch: 4 [18560/30596 (61%)]\tLoss: 0.059458\n",
      "Train Epoch: 4 [19200/30596 (63%)]\tLoss: 0.168574\n",
      "Train Epoch: 4 [19840/30596 (65%)]\tLoss: 0.178940\n",
      "Train Epoch: 4 [20480/30596 (67%)]\tLoss: 0.061106\n",
      "Train Epoch: 4 [21120/30596 (69%)]\tLoss: 0.086548\n",
      "Train Epoch: 4 [21760/30596 (71%)]\tLoss: 0.061440\n",
      "Train Epoch: 4 [22400/30596 (73%)]\tLoss: 0.145853\n",
      "Train Epoch: 4 [23040/30596 (75%)]\tLoss: 0.062898\n",
      "Train Epoch: 4 [23680/30596 (77%)]\tLoss: 0.136555\n",
      "Train Epoch: 4 [24320/30596 (79%)]\tLoss: 0.045974\n",
      "Train Epoch: 4 [24960/30596 (81%)]\tLoss: 0.069408\n",
      "Train Epoch: 4 [25600/30596 (84%)]\tLoss: 0.106820\n",
      "Train Epoch: 4 [26240/30596 (86%)]\tLoss: 0.113017\n",
      "Train Epoch: 4 [26880/30596 (88%)]\tLoss: 0.156226\n",
      "Train Epoch: 4 [27520/30596 (90%)]\tLoss: 0.037110\n",
      "Train Epoch: 4 [28160/30596 (92%)]\tLoss: 0.061544\n",
      "Train Epoch: 4 [28800/30596 (94%)]\tLoss: 0.139873\n",
      "Train Epoch: 4 [29440/30596 (96%)]\tLoss: 0.135664\n",
      "Train Epoch: 4 [30080/30596 (98%)]\tLoss: 0.179450\n",
      "\n",
      "Test set: Avg. loss: 0.0291, Accuracy: 5084/5139 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/30596 (0%)]\tLoss: 0.158403\n",
      "Train Epoch: 5 [640/30596 (2%)]\tLoss: 0.010958\n",
      "Train Epoch: 5 [1280/30596 (4%)]\tLoss: 0.103923\n",
      "Train Epoch: 5 [1920/30596 (6%)]\tLoss: 0.285814\n",
      "Train Epoch: 5 [2560/30596 (8%)]\tLoss: 0.077576\n",
      "Train Epoch: 5 [3200/30596 (10%)]\tLoss: 0.097358\n",
      "Train Epoch: 5 [3840/30596 (13%)]\tLoss: 0.060698\n",
      "Train Epoch: 5 [4480/30596 (15%)]\tLoss: 0.149457\n",
      "Train Epoch: 5 [5120/30596 (17%)]\tLoss: 0.134067\n",
      "Train Epoch: 5 [5760/30596 (19%)]\tLoss: 0.079320\n",
      "Train Epoch: 5 [6400/30596 (21%)]\tLoss: 0.080298\n",
      "Train Epoch: 5 [7040/30596 (23%)]\tLoss: 0.067678\n",
      "Train Epoch: 5 [7680/30596 (25%)]\tLoss: 0.088526\n",
      "Train Epoch: 5 [8320/30596 (27%)]\tLoss: 0.075352\n",
      "Train Epoch: 5 [8960/30596 (29%)]\tLoss: 0.104542\n",
      "Train Epoch: 5 [9600/30596 (31%)]\tLoss: 0.283680\n",
      "Train Epoch: 5 [10240/30596 (33%)]\tLoss: 0.079432\n",
      "Train Epoch: 5 [10880/30596 (35%)]\tLoss: 0.032293\n",
      "Train Epoch: 5 [11520/30596 (38%)]\tLoss: 0.151100\n",
      "Train Epoch: 5 [12160/30596 (40%)]\tLoss: 0.043365\n",
      "Train Epoch: 5 [12800/30596 (42%)]\tLoss: 0.023069\n",
      "Train Epoch: 5 [13440/30596 (44%)]\tLoss: 0.047486\n",
      "Train Epoch: 5 [14080/30596 (46%)]\tLoss: 0.191821\n",
      "Train Epoch: 5 [14720/30596 (48%)]\tLoss: 0.086004\n",
      "Train Epoch: 5 [15360/30596 (50%)]\tLoss: 0.036613\n",
      "Train Epoch: 5 [16000/30596 (52%)]\tLoss: 0.090507\n",
      "Train Epoch: 5 [16640/30596 (54%)]\tLoss: 0.051218\n",
      "Train Epoch: 5 [17280/30596 (56%)]\tLoss: 0.110946\n",
      "Train Epoch: 5 [17920/30596 (58%)]\tLoss: 0.024860\n",
      "Train Epoch: 5 [18560/30596 (61%)]\tLoss: 0.186735\n",
      "Train Epoch: 5 [19200/30596 (63%)]\tLoss: 0.016848\n",
      "Train Epoch: 5 [19840/30596 (65%)]\tLoss: 0.084729\n",
      "Train Epoch: 5 [20480/30596 (67%)]\tLoss: 0.171805\n",
      "Train Epoch: 5 [21120/30596 (69%)]\tLoss: 0.064530\n",
      "Train Epoch: 5 [21760/30596 (71%)]\tLoss: 0.063704\n",
      "Train Epoch: 5 [22400/30596 (73%)]\tLoss: 0.057115\n",
      "Train Epoch: 5 [23040/30596 (75%)]\tLoss: 0.051002\n",
      "Train Epoch: 5 [23680/30596 (77%)]\tLoss: 0.139234\n",
      "Train Epoch: 5 [24320/30596 (79%)]\tLoss: 0.145671\n",
      "Train Epoch: 5 [24960/30596 (81%)]\tLoss: 0.062495\n",
      "Train Epoch: 5 [25600/30596 (84%)]\tLoss: 0.051537\n",
      "Train Epoch: 5 [26240/30596 (86%)]\tLoss: 0.117926\n",
      "Train Epoch: 5 [26880/30596 (88%)]\tLoss: 0.134638\n",
      "Train Epoch: 5 [27520/30596 (90%)]\tLoss: 0.211271\n",
      "Train Epoch: 5 [28160/30596 (92%)]\tLoss: 0.096693\n",
      "Train Epoch: 5 [28800/30596 (94%)]\tLoss: 0.041268\n",
      "Train Epoch: 5 [29440/30596 (96%)]\tLoss: 0.050333\n",
      "Train Epoch: 5 [30080/30596 (98%)]\tLoss: 0.072064\n",
      "\n",
      "Test set: Avg. loss: 0.0228, Accuracy: 5099/5139 (99%)\n",
      "\n",
      "Train Epoch: 6 [0/30596 (0%)]\tLoss: 0.217072\n",
      "Train Epoch: 6 [640/30596 (2%)]\tLoss: 0.162112\n",
      "Train Epoch: 6 [1280/30596 (4%)]\tLoss: 0.202571\n",
      "Train Epoch: 6 [1920/30596 (6%)]\tLoss: 0.029493\n",
      "Train Epoch: 6 [2560/30596 (8%)]\tLoss: 0.116537\n",
      "Train Epoch: 6 [3200/30596 (10%)]\tLoss: 0.146041\n",
      "Train Epoch: 6 [3840/30596 (13%)]\tLoss: 0.089712\n",
      "Train Epoch: 6 [4480/30596 (15%)]\tLoss: 0.040155\n",
      "Train Epoch: 6 [5120/30596 (17%)]\tLoss: 0.099945\n",
      "Train Epoch: 6 [5760/30596 (19%)]\tLoss: 0.039584\n",
      "Train Epoch: 6 [6400/30596 (21%)]\tLoss: 0.027806\n",
      "Train Epoch: 6 [7040/30596 (23%)]\tLoss: 0.039912\n",
      "Train Epoch: 6 [7680/30596 (25%)]\tLoss: 0.163345\n",
      "Train Epoch: 6 [8320/30596 (27%)]\tLoss: 0.129774\n",
      "Train Epoch: 6 [8960/30596 (29%)]\tLoss: 0.113695\n",
      "Train Epoch: 6 [9600/30596 (31%)]\tLoss: 0.016504\n",
      "Train Epoch: 6 [10240/30596 (33%)]\tLoss: 0.096533\n",
      "Train Epoch: 6 [10880/30596 (35%)]\tLoss: 0.047168\n",
      "Train Epoch: 6 [11520/30596 (38%)]\tLoss: 0.165296\n",
      "Train Epoch: 6 [12160/30596 (40%)]\tLoss: 0.120831\n",
      "Train Epoch: 6 [12800/30596 (42%)]\tLoss: 0.034771\n",
      "Train Epoch: 6 [13440/30596 (44%)]\tLoss: 0.097592\n",
      "Train Epoch: 6 [14080/30596 (46%)]\tLoss: 0.081350\n",
      "Train Epoch: 6 [14720/30596 (48%)]\tLoss: 0.081811\n",
      "Train Epoch: 6 [15360/30596 (50%)]\tLoss: 0.082236\n",
      "Train Epoch: 6 [16000/30596 (52%)]\tLoss: 0.080531\n",
      "Train Epoch: 6 [16640/30596 (54%)]\tLoss: 0.139053\n",
      "Train Epoch: 6 [17280/30596 (56%)]\tLoss: 0.037586\n",
      "Train Epoch: 6 [17920/30596 (58%)]\tLoss: 0.053018\n",
      "Train Epoch: 6 [18560/30596 (61%)]\tLoss: 0.060120\n",
      "Train Epoch: 6 [19200/30596 (63%)]\tLoss: 0.108746\n",
      "Train Epoch: 6 [19840/30596 (65%)]\tLoss: 0.080490\n",
      "Train Epoch: 6 [20480/30596 (67%)]\tLoss: 0.026667\n",
      "Train Epoch: 6 [21120/30596 (69%)]\tLoss: 0.053635\n",
      "Train Epoch: 6 [21760/30596 (71%)]\tLoss: 0.039187\n",
      "Train Epoch: 6 [22400/30596 (73%)]\tLoss: 0.010998\n",
      "Train Epoch: 6 [23040/30596 (75%)]\tLoss: 0.057161\n",
      "Train Epoch: 6 [23680/30596 (77%)]\tLoss: 0.034209\n",
      "Train Epoch: 6 [24320/30596 (79%)]\tLoss: 0.036853\n",
      "Train Epoch: 6 [24960/30596 (81%)]\tLoss: 0.069464\n",
      "Train Epoch: 6 [25600/30596 (84%)]\tLoss: 0.157970\n",
      "Train Epoch: 6 [26240/30596 (86%)]\tLoss: 0.072594\n",
      "Train Epoch: 6 [26880/30596 (88%)]\tLoss: 0.099767\n",
      "Train Epoch: 6 [27520/30596 (90%)]\tLoss: 0.066629\n",
      "Train Epoch: 6 [28160/30596 (92%)]\tLoss: 0.120312\n",
      "Train Epoch: 6 [28800/30596 (94%)]\tLoss: 0.036032\n",
      "Train Epoch: 6 [29440/30596 (96%)]\tLoss: 0.136227\n",
      "Train Epoch: 6 [30080/30596 (98%)]\tLoss: 0.117903\n",
      "\n",
      "Test set: Avg. loss: 0.0215, Accuracy: 5104/5139 (99%)\n",
      "\n",
      "Train Epoch: 7 [0/30596 (0%)]\tLoss: 0.093476\n",
      "Train Epoch: 7 [640/30596 (2%)]\tLoss: 0.075910\n",
      "Train Epoch: 7 [1280/30596 (4%)]\tLoss: 0.075726\n",
      "Train Epoch: 7 [1920/30596 (6%)]\tLoss: 0.018666\n",
      "Train Epoch: 7 [2560/30596 (8%)]\tLoss: 0.230518\n",
      "Train Epoch: 7 [3200/30596 (10%)]\tLoss: 0.098756\n",
      "Train Epoch: 7 [3840/30596 (13%)]\tLoss: 0.131749\n",
      "Train Epoch: 7 [4480/30596 (15%)]\tLoss: 0.008221\n",
      "Train Epoch: 7 [5120/30596 (17%)]\tLoss: 0.127952\n",
      "Train Epoch: 7 [5760/30596 (19%)]\tLoss: 0.111388\n",
      "Train Epoch: 7 [6400/30596 (21%)]\tLoss: 0.045654\n",
      "Train Epoch: 7 [7040/30596 (23%)]\tLoss: 0.177969\n",
      "Train Epoch: 7 [7680/30596 (25%)]\tLoss: 0.047769\n",
      "Train Epoch: 7 [8320/30596 (27%)]\tLoss: 0.132889\n",
      "Train Epoch: 7 [8960/30596 (29%)]\tLoss: 0.135777\n",
      "Train Epoch: 7 [9600/30596 (31%)]\tLoss: 0.016744\n",
      "Train Epoch: 7 [10240/30596 (33%)]\tLoss: 0.028757\n",
      "Train Epoch: 7 [10880/30596 (35%)]\tLoss: 0.146573\n",
      "Train Epoch: 7 [11520/30596 (38%)]\tLoss: 0.069782\n",
      "Train Epoch: 7 [12160/30596 (40%)]\tLoss: 0.107984\n",
      "Train Epoch: 7 [12800/30596 (42%)]\tLoss: 0.171001\n",
      "Train Epoch: 7 [13440/30596 (44%)]\tLoss: 0.063728\n",
      "Train Epoch: 7 [14080/30596 (46%)]\tLoss: 0.029703\n",
      "Train Epoch: 7 [14720/30596 (48%)]\tLoss: 0.185569\n",
      "Train Epoch: 7 [15360/30596 (50%)]\tLoss: 0.076817\n",
      "Train Epoch: 7 [16000/30596 (52%)]\tLoss: 0.027372\n",
      "Train Epoch: 7 [16640/30596 (54%)]\tLoss: 0.046622\n",
      "Train Epoch: 7 [17280/30596 (56%)]\tLoss: 0.163207\n",
      "Train Epoch: 7 [17920/30596 (58%)]\tLoss: 0.057623\n",
      "Train Epoch: 7 [18560/30596 (61%)]\tLoss: 0.058212\n",
      "Train Epoch: 7 [19200/30596 (63%)]\tLoss: 0.024363\n",
      "Train Epoch: 7 [19840/30596 (65%)]\tLoss: 0.072964\n",
      "Train Epoch: 7 [20480/30596 (67%)]\tLoss: 0.112444\n",
      "Train Epoch: 7 [21120/30596 (69%)]\tLoss: 0.170665\n",
      "Train Epoch: 7 [21760/30596 (71%)]\tLoss: 0.072911\n",
      "Train Epoch: 7 [22400/30596 (73%)]\tLoss: 0.071631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [23040/30596 (75%)]\tLoss: 0.148550\n",
      "Train Epoch: 7 [23680/30596 (77%)]\tLoss: 0.030178\n",
      "Train Epoch: 7 [24320/30596 (79%)]\tLoss: 0.016463\n",
      "Train Epoch: 7 [24960/30596 (81%)]\tLoss: 0.022412\n",
      "Train Epoch: 7 [25600/30596 (84%)]\tLoss: 0.020551\n",
      "Train Epoch: 7 [26240/30596 (86%)]\tLoss: 0.109369\n",
      "Train Epoch: 7 [26880/30596 (88%)]\tLoss: 0.140623\n",
      "Train Epoch: 7 [27520/30596 (90%)]\tLoss: 0.075738\n",
      "Train Epoch: 7 [28160/30596 (92%)]\tLoss: 0.112684\n",
      "Train Epoch: 7 [28800/30596 (94%)]\tLoss: 0.071483\n",
      "Train Epoch: 7 [29440/30596 (96%)]\tLoss: 0.026024\n",
      "Train Epoch: 7 [30080/30596 (98%)]\tLoss: 0.019757\n",
      "\n",
      "Test set: Avg. loss: 0.0210, Accuracy: 5100/5139 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/30596 (0%)]\tLoss: 0.099283\n",
      "Train Epoch: 8 [640/30596 (2%)]\tLoss: 0.138834\n",
      "Train Epoch: 8 [1280/30596 (4%)]\tLoss: 0.097541\n",
      "Train Epoch: 8 [1920/30596 (6%)]\tLoss: 0.312480\n",
      "Train Epoch: 8 [2560/30596 (8%)]\tLoss: 0.119726\n",
      "Train Epoch: 8 [3200/30596 (10%)]\tLoss: 0.033646\n",
      "Train Epoch: 8 [3840/30596 (13%)]\tLoss: 0.050083\n",
      "Train Epoch: 8 [4480/30596 (15%)]\tLoss: 0.065760\n",
      "Train Epoch: 8 [5120/30596 (17%)]\tLoss: 0.031928\n",
      "Train Epoch: 8 [5760/30596 (19%)]\tLoss: 0.042529\n",
      "Train Epoch: 8 [6400/30596 (21%)]\tLoss: 0.163436\n",
      "Train Epoch: 8 [7040/30596 (23%)]\tLoss: 0.065433\n",
      "Train Epoch: 8 [7680/30596 (25%)]\tLoss: 0.049375\n",
      "Train Epoch: 8 [8320/30596 (27%)]\tLoss: 0.055558\n",
      "Train Epoch: 8 [8960/30596 (29%)]\tLoss: 0.078980\n",
      "Train Epoch: 8 [9600/30596 (31%)]\tLoss: 0.035227\n",
      "Train Epoch: 8 [10240/30596 (33%)]\tLoss: 0.064542\n",
      "Train Epoch: 8 [10880/30596 (35%)]\tLoss: 0.079235\n",
      "Train Epoch: 8 [11520/30596 (38%)]\tLoss: 0.131306\n",
      "Train Epoch: 8 [12160/30596 (40%)]\tLoss: 0.005930\n",
      "Train Epoch: 8 [12800/30596 (42%)]\tLoss: 0.213466\n",
      "Train Epoch: 8 [13440/30596 (44%)]\tLoss: 0.033089\n",
      "Train Epoch: 8 [14080/30596 (46%)]\tLoss: 0.050064\n",
      "Train Epoch: 8 [14720/30596 (48%)]\tLoss: 0.024260\n",
      "Train Epoch: 8 [15360/30596 (50%)]\tLoss: 0.017458\n",
      "Train Epoch: 8 [16000/30596 (52%)]\tLoss: 0.103816\n",
      "Train Epoch: 8 [16640/30596 (54%)]\tLoss: 0.013899\n",
      "Train Epoch: 8 [17280/30596 (56%)]\tLoss: 0.116428\n",
      "Train Epoch: 8 [17920/30596 (58%)]\tLoss: 0.054084\n",
      "Train Epoch: 8 [18560/30596 (61%)]\tLoss: 0.080126\n",
      "Train Epoch: 8 [19200/30596 (63%)]\tLoss: 0.072345\n",
      "Train Epoch: 8 [19840/30596 (65%)]\tLoss: 0.025312\n",
      "Train Epoch: 8 [20480/30596 (67%)]\tLoss: 0.020012\n",
      "Train Epoch: 8 [21120/30596 (69%)]\tLoss: 0.080209\n",
      "Train Epoch: 8 [21760/30596 (71%)]\tLoss: 0.138465\n",
      "Train Epoch: 8 [22400/30596 (73%)]\tLoss: 0.053651\n",
      "Train Epoch: 8 [23040/30596 (75%)]\tLoss: 0.051844\n",
      "Train Epoch: 8 [23680/30596 (77%)]\tLoss: 0.088959\n",
      "Train Epoch: 8 [24320/30596 (79%)]\tLoss: 0.009891\n",
      "Train Epoch: 8 [24960/30596 (81%)]\tLoss: 0.026286\n",
      "Train Epoch: 8 [25600/30596 (84%)]\tLoss: 0.011914\n",
      "Train Epoch: 8 [26240/30596 (86%)]\tLoss: 0.123345\n",
      "Train Epoch: 8 [26880/30596 (88%)]\tLoss: 0.111306\n",
      "Train Epoch: 8 [27520/30596 (90%)]\tLoss: 0.109881\n",
      "Train Epoch: 8 [28160/30596 (92%)]\tLoss: 0.031361\n",
      "Train Epoch: 8 [28800/30596 (94%)]\tLoss: 0.092497\n",
      "Train Epoch: 8 [29440/30596 (96%)]\tLoss: 0.052921\n",
      "Train Epoch: 8 [30080/30596 (98%)]\tLoss: 0.073546\n",
      "\n",
      "Test set: Avg. loss: 0.0158, Accuracy: 5108/5139 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/30596 (0%)]\tLoss: 0.012602\n",
      "Train Epoch: 9 [640/30596 (2%)]\tLoss: 0.040582\n",
      "Train Epoch: 9 [1280/30596 (4%)]\tLoss: 0.014581\n",
      "Train Epoch: 9 [1920/30596 (6%)]\tLoss: 0.013634\n",
      "Train Epoch: 9 [2560/30596 (8%)]\tLoss: 0.097113\n",
      "Train Epoch: 9 [3200/30596 (10%)]\tLoss: 0.211201\n",
      "Train Epoch: 9 [3840/30596 (13%)]\tLoss: 0.103273\n",
      "Train Epoch: 9 [4480/30596 (15%)]\tLoss: 0.104023\n",
      "Train Epoch: 9 [5120/30596 (17%)]\tLoss: 0.088987\n",
      "Train Epoch: 9 [5760/30596 (19%)]\tLoss: 0.043517\n",
      "Train Epoch: 9 [6400/30596 (21%)]\tLoss: 0.077361\n",
      "Train Epoch: 9 [7040/30596 (23%)]\tLoss: 0.032807\n",
      "Train Epoch: 9 [7680/30596 (25%)]\tLoss: 0.119796\n",
      "Train Epoch: 9 [8320/30596 (27%)]\tLoss: 0.028175\n",
      "Train Epoch: 9 [8960/30596 (29%)]\tLoss: 0.467088\n",
      "Train Epoch: 9 [9600/30596 (31%)]\tLoss: 0.025822\n",
      "Train Epoch: 9 [10240/30596 (33%)]\tLoss: 0.110505\n",
      "Train Epoch: 9 [10880/30596 (35%)]\tLoss: 0.080129\n",
      "Train Epoch: 9 [11520/30596 (38%)]\tLoss: 0.040479\n",
      "Train Epoch: 9 [12160/30596 (40%)]\tLoss: 0.036603\n",
      "Train Epoch: 9 [12800/30596 (42%)]\tLoss: 0.065096\n",
      "Train Epoch: 9 [13440/30596 (44%)]\tLoss: 0.047132\n",
      "Train Epoch: 9 [14080/30596 (46%)]\tLoss: 0.094926\n",
      "Train Epoch: 9 [14720/30596 (48%)]\tLoss: 0.042148\n",
      "Train Epoch: 9 [15360/30596 (50%)]\tLoss: 0.100076\n",
      "Train Epoch: 9 [16000/30596 (52%)]\tLoss: 0.078566\n",
      "Train Epoch: 9 [16640/30596 (54%)]\tLoss: 0.022315\n",
      "Train Epoch: 9 [17280/30596 (56%)]\tLoss: 0.063372\n",
      "Train Epoch: 9 [17920/30596 (58%)]\tLoss: 0.020307\n",
      "Train Epoch: 9 [18560/30596 (61%)]\tLoss: 0.037746\n",
      "Train Epoch: 9 [19200/30596 (63%)]\tLoss: 0.016025\n",
      "Train Epoch: 9 [19840/30596 (65%)]\tLoss: 0.044300\n",
      "Train Epoch: 9 [20480/30596 (67%)]\tLoss: 0.071687\n",
      "Train Epoch: 9 [21120/30596 (69%)]\tLoss: 0.035400\n",
      "Train Epoch: 9 [21760/30596 (71%)]\tLoss: 0.145755\n",
      "Train Epoch: 9 [22400/30596 (73%)]\tLoss: 0.018015\n",
      "Train Epoch: 9 [23040/30596 (75%)]\tLoss: 0.049086\n",
      "Train Epoch: 9 [23680/30596 (77%)]\tLoss: 0.039231\n",
      "Train Epoch: 9 [24320/30596 (79%)]\tLoss: 0.103107\n",
      "Train Epoch: 9 [24960/30596 (81%)]\tLoss: 0.017328\n",
      "Train Epoch: 9 [25600/30596 (84%)]\tLoss: 0.023750\n",
      "Train Epoch: 9 [26240/30596 (86%)]\tLoss: 0.044197\n",
      "Train Epoch: 9 [26880/30596 (88%)]\tLoss: 0.045847\n",
      "Train Epoch: 9 [27520/30596 (90%)]\tLoss: 0.089070\n",
      "Train Epoch: 9 [28160/30596 (92%)]\tLoss: 0.099173\n",
      "Train Epoch: 9 [28800/30596 (94%)]\tLoss: 0.070866\n",
      "Train Epoch: 9 [29440/30596 (96%)]\tLoss: 0.072400\n",
      "Train Epoch: 9 [30080/30596 (98%)]\tLoss: 0.025967\n",
      "\n",
      "Test set: Avg. loss: 0.0154, Accuracy: 5111/5139 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/30596 (0%)]\tLoss: 0.105714\n",
      "Train Epoch: 10 [640/30596 (2%)]\tLoss: 0.049394\n",
      "Train Epoch: 10 [1280/30596 (4%)]\tLoss: 0.037601\n",
      "Train Epoch: 10 [1920/30596 (6%)]\tLoss: 0.016576\n",
      "Train Epoch: 10 [2560/30596 (8%)]\tLoss: 0.011128\n",
      "Train Epoch: 10 [3200/30596 (10%)]\tLoss: 0.020415\n",
      "Train Epoch: 10 [3840/30596 (13%)]\tLoss: 0.034317\n",
      "Train Epoch: 10 [4480/30596 (15%)]\tLoss: 0.044336\n",
      "Train Epoch: 10 [5120/30596 (17%)]\tLoss: 0.066062\n",
      "Train Epoch: 10 [5760/30596 (19%)]\tLoss: 0.065808\n",
      "Train Epoch: 10 [6400/30596 (21%)]\tLoss: 0.018589\n",
      "Train Epoch: 10 [7040/30596 (23%)]\tLoss: 0.245379\n",
      "Train Epoch: 10 [7680/30596 (25%)]\tLoss: 0.033357\n",
      "Train Epoch: 10 [8320/30596 (27%)]\tLoss: 0.013369\n",
      "Train Epoch: 10 [8960/30596 (29%)]\tLoss: 0.050188\n",
      "Train Epoch: 10 [9600/30596 (31%)]\tLoss: 0.005644\n",
      "Train Epoch: 10 [10240/30596 (33%)]\tLoss: 0.095327\n",
      "Train Epoch: 10 [10880/30596 (35%)]\tLoss: 0.021695\n",
      "Train Epoch: 10 [11520/30596 (38%)]\tLoss: 0.027904\n",
      "Train Epoch: 10 [12160/30596 (40%)]\tLoss: 0.046029\n",
      "Train Epoch: 10 [12800/30596 (42%)]\tLoss: 0.072524\n",
      "Train Epoch: 10 [13440/30596 (44%)]\tLoss: 0.071996\n",
      "Train Epoch: 10 [14080/30596 (46%)]\tLoss: 0.046824\n",
      "Train Epoch: 10 [14720/30596 (48%)]\tLoss: 0.025748\n",
      "Train Epoch: 10 [15360/30596 (50%)]\tLoss: 0.028218\n",
      "Train Epoch: 10 [16000/30596 (52%)]\tLoss: 0.023430\n",
      "Train Epoch: 10 [16640/30596 (54%)]\tLoss: 0.048311\n",
      "Train Epoch: 10 [17280/30596 (56%)]\tLoss: 0.004840\n",
      "Train Epoch: 10 [17920/30596 (58%)]\tLoss: 0.088242\n",
      "Train Epoch: 10 [18560/30596 (61%)]\tLoss: 0.014929\n",
      "Train Epoch: 10 [19200/30596 (63%)]\tLoss: 0.102706\n",
      "Train Epoch: 10 [19840/30596 (65%)]\tLoss: 0.095100\n",
      "Train Epoch: 10 [20480/30596 (67%)]\tLoss: 0.041726\n",
      "Train Epoch: 10 [21120/30596 (69%)]\tLoss: 0.146808\n",
      "Train Epoch: 10 [21760/30596 (71%)]\tLoss: 0.097694\n",
      "Train Epoch: 10 [22400/30596 (73%)]\tLoss: 0.032606\n",
      "Train Epoch: 10 [23040/30596 (75%)]\tLoss: 0.208248\n",
      "Train Epoch: 10 [23680/30596 (77%)]\tLoss: 0.084383\n",
      "Train Epoch: 10 [24320/30596 (79%)]\tLoss: 0.080343\n",
      "Train Epoch: 10 [24960/30596 (81%)]\tLoss: 0.009702\n",
      "Train Epoch: 10 [25600/30596 (84%)]\tLoss: 0.186214\n",
      "Train Epoch: 10 [26240/30596 (86%)]\tLoss: 0.045976\n",
      "Train Epoch: 10 [26880/30596 (88%)]\tLoss: 0.071715\n",
      "Train Epoch: 10 [27520/30596 (90%)]\tLoss: 0.184132\n",
      "Train Epoch: 10 [28160/30596 (92%)]\tLoss: 0.018305\n",
      "Train Epoch: 10 [28800/30596 (94%)]\tLoss: 0.099712\n",
      "Train Epoch: 10 [29440/30596 (96%)]\tLoss: 0.041990\n",
      "Train Epoch: 10 [30080/30596 (98%)]\tLoss: 0.034673\n",
      "\n",
      "Test set: Avg. loss: 0.0158, Accuracy: 5110/5139 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net_c5 = Net_c5().cuda()\n",
    "n_epochs = 10\n",
    "optimizer = optim.SGD(net_c5.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader5.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "test(net_c5, test_loader5)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(net_c5, train_loader5, epoch)\n",
    "    test(net_c5, test_loader5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naiqi/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.00249761, 0.00249761, 0.00249761, 0.00749284, 0.00999045,\n",
       "        0.02747373, 0.0399618 , 0.04745463, 0.04745463, 0.07243075,\n",
       "        0.05744508, 0.12488061, 0.09990449, 0.11239255, 0.13986628,\n",
       "        0.14985673, 0.15235434, 0.16234479, 0.2048042 , 0.19481375,\n",
       "        0.16234479, 0.19231614, 0.19481375, 0.24226838, 0.20730181,\n",
       "        0.21978987, 0.1848233 , 0.16983763, 0.1848233 , 0.17233524,\n",
       "        0.13487106, 0.1024021 , 0.12737822, 0.09740688, 0.10489971,\n",
       "        0.07243075, 0.10739732, 0.06493792, 0.07492837, 0.08491881,\n",
       "        0.05244986, 0.05494747, 0.02997135, 0.02997135, 0.03246896,\n",
       "        0.03246896, 0.0199809 , 0.02747373, 0.01748329, 0.00749284,\n",
       "        0.02747373, 0.01498567, 0.00749284, 0.00999045, 0.01498567,\n",
       "        0.00749284, 0.00999045, 0.00249761, 0.00499522, 0.00249761,\n",
       "        0.00749284, 0.        , 0.        , 0.        , 0.00249761,\n",
       "        0.00499522, 0.        , 0.00249761, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00249761, 0.00249761, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00249761, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00249761]),\n",
       " array([ 5.05932518,  5.25951639,  5.4597076 ,  5.6598988 ,  5.86009001,\n",
       "         6.06028122,  6.26047243,  6.46066363,  6.66085484,  6.86104605,\n",
       "         7.06123726,  7.26142846,  7.46161967,  7.66181088,  7.86200209,\n",
       "         8.06219329,  8.2623845 ,  8.46257571,  8.66276692,  8.86295812,\n",
       "         9.06314933,  9.26334054,  9.46353175,  9.66372295,  9.86391416,\n",
       "        10.06410537, 10.26429658, 10.46448778, 10.66467899, 10.8648702 ,\n",
       "        11.06506141, 11.26525261, 11.46544382, 11.66563503, 11.86582624,\n",
       "        12.06601744, 12.26620865, 12.46639986, 12.66659107, 12.86678227,\n",
       "        13.06697348, 13.26716469, 13.4673559 , 13.6675471 , 13.86773831,\n",
       "        14.06792952, 14.26812073, 14.46831193, 14.66850314, 14.86869435,\n",
       "        15.06888555, 15.26907676, 15.46926797, 15.66945918, 15.86965038,\n",
       "        16.06984159, 16.2700328 , 16.47022401, 16.67041521, 16.87060642,\n",
       "        17.07079763, 17.27098884, 17.47118004, 17.67137125, 17.87156246,\n",
       "        18.07175367, 18.27194487, 18.47213608, 18.67232729, 18.8725185 ,\n",
       "        19.0727097 , 19.27290091, 19.47309212, 19.67328333, 19.87347453,\n",
       "        20.07366574, 20.27385695, 20.47404816, 20.67423936, 20.87443057,\n",
       "        21.07462178, 21.27481299, 21.47500419, 21.6751954 , 21.87538661,\n",
       "        22.07557782, 22.27576902, 22.47596023, 22.67615144, 22.87634265,\n",
       "        23.07653385, 23.27672506, 23.47691627, 23.67710748, 23.87729868,\n",
       "        24.07748989, 24.2776811 , 24.47787231, 24.67806351, 24.87825472,\n",
       "        25.07844593]),\n",
       " <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = net_c5(noise.cuda())\n",
    "ll = net_c5.last\n",
    "print(ll.shape)\n",
    "ll = ll.cpu().detach().numpy() \n",
    "lid_net1_c5 = LID(ll, ll, k=50)\n",
    "plt.hist(lid_net1_c5, bins=100, normed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.128637566955044\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(lid_net1_c5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network using 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_c2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_c2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        self.last = x\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naiqi/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.6630, Accuracy: 980/2115 (46%)\n",
      "\n",
      "Train Epoch: 1 [0/12665 (0%)]\tLoss: 0.730942\n",
      "Train Epoch: 1 [640/12665 (5%)]\tLoss: 0.530172\n",
      "Train Epoch: 1 [1280/12665 (10%)]\tLoss: 0.331076\n",
      "Train Epoch: 1 [1920/12665 (15%)]\tLoss: 0.219369\n",
      "Train Epoch: 1 [2560/12665 (20%)]\tLoss: 0.121802\n",
      "Train Epoch: 1 [3200/12665 (25%)]\tLoss: 0.086594\n",
      "Train Epoch: 1 [3840/12665 (30%)]\tLoss: 0.068915\n",
      "Train Epoch: 1 [4480/12665 (35%)]\tLoss: 0.029769\n",
      "Train Epoch: 1 [5120/12665 (40%)]\tLoss: 0.081206\n",
      "Train Epoch: 1 [5760/12665 (45%)]\tLoss: 0.135490\n",
      "Train Epoch: 1 [6400/12665 (51%)]\tLoss: 0.055422\n",
      "Train Epoch: 1 [7040/12665 (56%)]\tLoss: 0.073580\n",
      "Train Epoch: 1 [7680/12665 (61%)]\tLoss: 0.031799\n",
      "Train Epoch: 1 [8320/12665 (66%)]\tLoss: 0.040979\n",
      "Train Epoch: 1 [8960/12665 (71%)]\tLoss: 0.010712\n",
      "Train Epoch: 1 [9600/12665 (76%)]\tLoss: 0.036680\n",
      "Train Epoch: 1 [10240/12665 (81%)]\tLoss: 0.017262\n",
      "Train Epoch: 1 [10880/12665 (86%)]\tLoss: 0.021737\n",
      "Train Epoch: 1 [11520/12665 (91%)]\tLoss: 0.020572\n",
      "Train Epoch: 1 [12160/12665 (96%)]\tLoss: 0.013283\n",
      "\n",
      "Test set: Avg. loss: 0.0030, Accuracy: 2113/2115 (99%)\n",
      "\n",
      "Train Epoch: 2 [0/12665 (0%)]\tLoss: 0.018302\n",
      "Train Epoch: 2 [640/12665 (5%)]\tLoss: 0.024161\n",
      "Train Epoch: 2 [1280/12665 (10%)]\tLoss: 0.039577\n",
      "Train Epoch: 2 [1920/12665 (15%)]\tLoss: 0.016369\n",
      "Train Epoch: 2 [2560/12665 (20%)]\tLoss: 0.017281\n",
      "Train Epoch: 2 [3200/12665 (25%)]\tLoss: 0.029345\n",
      "Train Epoch: 2 [3840/12665 (30%)]\tLoss: 0.008058\n",
      "Train Epoch: 2 [4480/12665 (35%)]\tLoss: 0.014970\n",
      "Train Epoch: 2 [5120/12665 (40%)]\tLoss: 0.011193\n",
      "Train Epoch: 2 [5760/12665 (45%)]\tLoss: 0.006452\n",
      "Train Epoch: 2 [6400/12665 (51%)]\tLoss: 0.018814\n",
      "Train Epoch: 2 [7040/12665 (56%)]\tLoss: 0.003859\n",
      "Train Epoch: 2 [7680/12665 (61%)]\tLoss: 0.003084\n",
      "Train Epoch: 2 [8320/12665 (66%)]\tLoss: 0.010664\n",
      "Train Epoch: 2 [8960/12665 (71%)]\tLoss: 0.010308\n",
      "Train Epoch: 2 [9600/12665 (76%)]\tLoss: 0.013688\n",
      "Train Epoch: 2 [10240/12665 (81%)]\tLoss: 0.038847\n",
      "Train Epoch: 2 [10880/12665 (86%)]\tLoss: 0.015123\n",
      "Train Epoch: 2 [11520/12665 (91%)]\tLoss: 0.009881\n",
      "Train Epoch: 2 [12160/12665 (96%)]\tLoss: 0.013844\n",
      "\n",
      "Test set: Avg. loss: 0.0022, Accuracy: 2113/2115 (99%)\n",
      "\n",
      "Train Epoch: 3 [0/12665 (0%)]\tLoss: 0.013135\n",
      "Train Epoch: 3 [640/12665 (5%)]\tLoss: 0.011946\n",
      "Train Epoch: 3 [1280/12665 (10%)]\tLoss: 0.002173\n",
      "Train Epoch: 3 [1920/12665 (15%)]\tLoss: 0.016984\n",
      "Train Epoch: 3 [2560/12665 (20%)]\tLoss: 0.005561\n",
      "Train Epoch: 3 [3200/12665 (25%)]\tLoss: 0.082927\n",
      "Train Epoch: 3 [3840/12665 (30%)]\tLoss: 0.003454\n",
      "Train Epoch: 3 [4480/12665 (35%)]\tLoss: 0.013788\n",
      "Train Epoch: 3 [5120/12665 (40%)]\tLoss: 0.057552\n",
      "Train Epoch: 3 [5760/12665 (45%)]\tLoss: 0.007378\n",
      "Train Epoch: 3 [6400/12665 (51%)]\tLoss: 0.003840\n",
      "Train Epoch: 3 [7040/12665 (56%)]\tLoss: 0.004144\n",
      "Train Epoch: 3 [7680/12665 (61%)]\tLoss: 0.017448\n",
      "Train Epoch: 3 [8320/12665 (66%)]\tLoss: 0.006557\n",
      "Train Epoch: 3 [8960/12665 (71%)]\tLoss: 0.006064\n",
      "Train Epoch: 3 [9600/12665 (76%)]\tLoss: 0.002002\n",
      "Train Epoch: 3 [10240/12665 (81%)]\tLoss: 0.013553\n",
      "Train Epoch: 3 [10880/12665 (86%)]\tLoss: 0.002418\n",
      "Train Epoch: 3 [11520/12665 (91%)]\tLoss: 0.001053\n",
      "Train Epoch: 3 [12160/12665 (96%)]\tLoss: 0.004807\n",
      "\n",
      "Test set: Avg. loss: 0.0022, Accuracy: 2114/2115 (99%)\n",
      "\n",
      "Train Epoch: 4 [0/12665 (0%)]\tLoss: 0.015654\n",
      "Train Epoch: 4 [640/12665 (5%)]\tLoss: 0.077157\n",
      "Train Epoch: 4 [1280/12665 (10%)]\tLoss: 0.006984\n",
      "Train Epoch: 4 [1920/12665 (15%)]\tLoss: 0.004048\n",
      "Train Epoch: 4 [2560/12665 (20%)]\tLoss: 0.002843\n",
      "Train Epoch: 4 [3200/12665 (25%)]\tLoss: 0.009194\n",
      "Train Epoch: 4 [3840/12665 (30%)]\tLoss: 0.005312\n",
      "Train Epoch: 4 [4480/12665 (35%)]\tLoss: 0.003487\n",
      "Train Epoch: 4 [5120/12665 (40%)]\tLoss: 0.011452\n",
      "Train Epoch: 4 [5760/12665 (45%)]\tLoss: 0.004283\n",
      "Train Epoch: 4 [6400/12665 (51%)]\tLoss: 0.002189\n",
      "Train Epoch: 4 [7040/12665 (56%)]\tLoss: 0.005093\n",
      "Train Epoch: 4 [7680/12665 (61%)]\tLoss: 0.002537\n",
      "Train Epoch: 4 [8320/12665 (66%)]\tLoss: 0.002638\n",
      "Train Epoch: 4 [8960/12665 (71%)]\tLoss: 0.012583\n",
      "Train Epoch: 4 [9600/12665 (76%)]\tLoss: 0.002391\n",
      "Train Epoch: 4 [10240/12665 (81%)]\tLoss: 0.006206\n",
      "Train Epoch: 4 [10880/12665 (86%)]\tLoss: 0.002807\n",
      "Train Epoch: 4 [11520/12665 (91%)]\tLoss: 0.001066\n",
      "Train Epoch: 4 [12160/12665 (96%)]\tLoss: 0.008645\n",
      "\n",
      "Test set: Avg. loss: 0.0019, Accuracy: 2113/2115 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/12665 (0%)]\tLoss: 0.010373\n",
      "Train Epoch: 5 [640/12665 (5%)]\tLoss: 0.000942\n",
      "Train Epoch: 5 [1280/12665 (10%)]\tLoss: 0.005781\n",
      "Train Epoch: 5 [1920/12665 (15%)]\tLoss: 0.000608\n",
      "Train Epoch: 5 [2560/12665 (20%)]\tLoss: 0.004210\n",
      "Train Epoch: 5 [3200/12665 (25%)]\tLoss: 0.001422\n",
      "Train Epoch: 5 [3840/12665 (30%)]\tLoss: 0.000711\n",
      "Train Epoch: 5 [4480/12665 (35%)]\tLoss: 0.000833\n",
      "Train Epoch: 5 [5120/12665 (40%)]\tLoss: 0.000464\n",
      "Train Epoch: 5 [5760/12665 (45%)]\tLoss: 0.004799\n",
      "Train Epoch: 5 [6400/12665 (51%)]\tLoss: 0.003562\n",
      "Train Epoch: 5 [7040/12665 (56%)]\tLoss: 0.017961\n",
      "Train Epoch: 5 [7680/12665 (61%)]\tLoss: 0.003225\n",
      "Train Epoch: 5 [8320/12665 (66%)]\tLoss: 0.000041\n",
      "Train Epoch: 5 [8960/12665 (71%)]\tLoss: 0.001258\n",
      "Train Epoch: 5 [9600/12665 (76%)]\tLoss: 0.007360\n",
      "Train Epoch: 5 [10240/12665 (81%)]\tLoss: 0.003235\n",
      "Train Epoch: 5 [10880/12665 (86%)]\tLoss: 0.005276\n",
      "Train Epoch: 5 [11520/12665 (91%)]\tLoss: 0.004795\n",
      "Train Epoch: 5 [12160/12665 (96%)]\tLoss: 0.000645\n",
      "\n",
      "Test set: Avg. loss: 0.0019, Accuracy: 2113/2115 (99%)\n",
      "\n",
      "Train Epoch: 6 [0/12665 (0%)]\tLoss: 0.002994\n",
      "Train Epoch: 6 [640/12665 (5%)]\tLoss: 0.016513\n",
      "Train Epoch: 6 [1280/12665 (10%)]\tLoss: 0.003295\n",
      "Train Epoch: 6 [1920/12665 (15%)]\tLoss: 0.002413\n",
      "Train Epoch: 6 [2560/12665 (20%)]\tLoss: 0.001923\n",
      "Train Epoch: 6 [3200/12665 (25%)]\tLoss: 0.075733\n",
      "Train Epoch: 6 [3840/12665 (30%)]\tLoss: 0.002998\n",
      "Train Epoch: 6 [4480/12665 (35%)]\tLoss: 0.002502\n",
      "Train Epoch: 6 [5120/12665 (40%)]\tLoss: 0.001101\n",
      "Train Epoch: 6 [5760/12665 (45%)]\tLoss: 0.003102\n",
      "Train Epoch: 6 [6400/12665 (51%)]\tLoss: 0.000531\n",
      "Train Epoch: 6 [7040/12665 (56%)]\tLoss: 0.001144\n",
      "Train Epoch: 6 [7680/12665 (61%)]\tLoss: 0.000687\n",
      "Train Epoch: 6 [8320/12665 (66%)]\tLoss: 0.000408\n",
      "Train Epoch: 6 [8960/12665 (71%)]\tLoss: 0.001942\n",
      "Train Epoch: 6 [9600/12665 (76%)]\tLoss: 0.014798\n",
      "Train Epoch: 6 [10240/12665 (81%)]\tLoss: 0.011491\n",
      "Train Epoch: 6 [10880/12665 (86%)]\tLoss: 0.001448\n",
      "Train Epoch: 6 [11520/12665 (91%)]\tLoss: 0.025130\n",
      "Train Epoch: 6 [12160/12665 (96%)]\tLoss: 0.000361\n",
      "\n",
      "Test set: Avg. loss: 0.0022, Accuracy: 2114/2115 (99%)\n",
      "\n",
      "Train Epoch: 7 [0/12665 (0%)]\tLoss: 0.000928\n",
      "Train Epoch: 7 [640/12665 (5%)]\tLoss: 0.002494\n",
      "Train Epoch: 7 [1280/12665 (10%)]\tLoss: 0.018189\n",
      "Train Epoch: 7 [1920/12665 (15%)]\tLoss: 0.003211\n",
      "Train Epoch: 7 [2560/12665 (20%)]\tLoss: 0.001742\n",
      "Train Epoch: 7 [3200/12665 (25%)]\tLoss: 0.001339\n",
      "Train Epoch: 7 [3840/12665 (30%)]\tLoss: 0.001521\n",
      "Train Epoch: 7 [4480/12665 (35%)]\tLoss: 0.004706\n",
      "Train Epoch: 7 [5120/12665 (40%)]\tLoss: 0.000975\n",
      "Train Epoch: 7 [5760/12665 (45%)]\tLoss: 0.000991\n",
      "Train Epoch: 7 [6400/12665 (51%)]\tLoss: 0.002313\n",
      "Train Epoch: 7 [7040/12665 (56%)]\tLoss: 0.000301\n",
      "Train Epoch: 7 [7680/12665 (61%)]\tLoss: 0.002353\n",
      "Train Epoch: 7 [8320/12665 (66%)]\tLoss: 0.007634\n",
      "Train Epoch: 7 [8960/12665 (71%)]\tLoss: 0.004034\n",
      "Train Epoch: 7 [9600/12665 (76%)]\tLoss: 0.001492\n",
      "Train Epoch: 7 [10240/12665 (81%)]\tLoss: 0.000855\n",
      "Train Epoch: 7 [10880/12665 (86%)]\tLoss: 0.001635\n",
      "Train Epoch: 7 [11520/12665 (91%)]\tLoss: 0.017995\n",
      "Train Epoch: 7 [12160/12665 (96%)]\tLoss: 0.001780\n",
      "\n",
      "Test set: Avg. loss: 0.0017, Accuracy: 2114/2115 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/12665 (0%)]\tLoss: 0.002800\n",
      "Train Epoch: 8 [640/12665 (5%)]\tLoss: 0.042679\n",
      "Train Epoch: 8 [1280/12665 (10%)]\tLoss: 0.010861\n",
      "Train Epoch: 8 [1920/12665 (15%)]\tLoss: 0.004780\n",
      "Train Epoch: 8 [2560/12665 (20%)]\tLoss: 0.003084\n",
      "Train Epoch: 8 [3200/12665 (25%)]\tLoss: 0.024636\n",
      "Train Epoch: 8 [3840/12665 (30%)]\tLoss: 0.000882\n",
      "Train Epoch: 8 [4480/12665 (35%)]\tLoss: 0.006823\n",
      "Train Epoch: 8 [5120/12665 (40%)]\tLoss: 0.002573\n",
      "Train Epoch: 8 [5760/12665 (45%)]\tLoss: 0.004459\n",
      "Train Epoch: 8 [6400/12665 (51%)]\tLoss: 0.009081\n",
      "Train Epoch: 8 [7040/12665 (56%)]\tLoss: 0.004465\n",
      "Train Epoch: 8 [7680/12665 (61%)]\tLoss: 0.001244\n",
      "Train Epoch: 8 [8320/12665 (66%)]\tLoss: 0.001674\n",
      "Train Epoch: 8 [8960/12665 (71%)]\tLoss: 0.003204\n",
      "Train Epoch: 8 [9600/12665 (76%)]\tLoss: 0.001555\n",
      "Train Epoch: 8 [10240/12665 (81%)]\tLoss: 0.001256\n",
      "Train Epoch: 8 [10880/12665 (86%)]\tLoss: 0.000751\n",
      "Train Epoch: 8 [11520/12665 (91%)]\tLoss: 0.001822\n",
      "Train Epoch: 8 [12160/12665 (96%)]\tLoss: 0.004873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0014, Accuracy: 2114/2115 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/12665 (0%)]\tLoss: 0.002993\n",
      "Train Epoch: 9 [640/12665 (5%)]\tLoss: 0.009096\n",
      "Train Epoch: 9 [1280/12665 (10%)]\tLoss: 0.003589\n",
      "Train Epoch: 9 [1920/12665 (15%)]\tLoss: 0.000592\n",
      "Train Epoch: 9 [2560/12665 (20%)]\tLoss: 0.001687\n",
      "Train Epoch: 9 [3200/12665 (25%)]\tLoss: 0.026976\n",
      "Train Epoch: 9 [3840/12665 (30%)]\tLoss: 0.002124\n",
      "Train Epoch: 9 [4480/12665 (35%)]\tLoss: 0.001033\n",
      "Train Epoch: 9 [5120/12665 (40%)]\tLoss: 0.002208\n",
      "Train Epoch: 9 [5760/12665 (45%)]\tLoss: 0.000430\n",
      "Train Epoch: 9 [6400/12665 (51%)]\tLoss: 0.001817\n",
      "Train Epoch: 9 [7040/12665 (56%)]\tLoss: 0.000974\n",
      "Train Epoch: 9 [7680/12665 (61%)]\tLoss: 0.006053\n",
      "Train Epoch: 9 [8320/12665 (66%)]\tLoss: 0.000621\n",
      "Train Epoch: 9 [8960/12665 (71%)]\tLoss: 0.007114\n",
      "Train Epoch: 9 [9600/12665 (76%)]\tLoss: 0.018094\n",
      "Train Epoch: 9 [10240/12665 (81%)]\tLoss: 0.000424\n",
      "Train Epoch: 9 [10880/12665 (86%)]\tLoss: 0.000322\n",
      "Train Epoch: 9 [11520/12665 (91%)]\tLoss: 0.003464\n",
      "Train Epoch: 9 [12160/12665 (96%)]\tLoss: 0.002436\n",
      "\n",
      "Test set: Avg. loss: 0.0017, Accuracy: 2114/2115 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/12665 (0%)]\tLoss: 0.016972\n",
      "Train Epoch: 10 [640/12665 (5%)]\tLoss: 0.000784\n",
      "Train Epoch: 10 [1280/12665 (10%)]\tLoss: 0.002228\n",
      "Train Epoch: 10 [1920/12665 (15%)]\tLoss: 0.001670\n",
      "Train Epoch: 10 [2560/12665 (20%)]\tLoss: 0.000460\n",
      "Train Epoch: 10 [3200/12665 (25%)]\tLoss: 0.001522\n",
      "Train Epoch: 10 [3840/12665 (30%)]\tLoss: 0.001209\n",
      "Train Epoch: 10 [4480/12665 (35%)]\tLoss: 0.000306\n",
      "Train Epoch: 10 [5120/12665 (40%)]\tLoss: 0.000555\n",
      "Train Epoch: 10 [5760/12665 (45%)]\tLoss: 0.000484\n",
      "Train Epoch: 10 [6400/12665 (51%)]\tLoss: 0.000259\n",
      "Train Epoch: 10 [7040/12665 (56%)]\tLoss: 0.000735\n",
      "Train Epoch: 10 [7680/12665 (61%)]\tLoss: 0.002165\n",
      "Train Epoch: 10 [8320/12665 (66%)]\tLoss: 0.000821\n",
      "Train Epoch: 10 [8960/12665 (71%)]\tLoss: 0.000242\n",
      "Train Epoch: 10 [9600/12665 (76%)]\tLoss: 0.007930\n",
      "Train Epoch: 10 [10240/12665 (81%)]\tLoss: 0.000409\n",
      "Train Epoch: 10 [10880/12665 (86%)]\tLoss: 0.008605\n",
      "Train Epoch: 10 [11520/12665 (91%)]\tLoss: 0.000506\n",
      "Train Epoch: 10 [12160/12665 (96%)]\tLoss: 0.084741\n",
      "\n",
      "Test set: Avg. loss: 0.0010, Accuracy: 2114/2115 (99%)\n",
      "\n",
      "Train Epoch: 11 [0/12665 (0%)]\tLoss: 0.002619\n",
      "Train Epoch: 11 [640/12665 (5%)]\tLoss: 0.001055\n",
      "Train Epoch: 11 [1280/12665 (10%)]\tLoss: 0.001814\n",
      "Train Epoch: 11 [1920/12665 (15%)]\tLoss: 0.002711\n",
      "Train Epoch: 11 [2560/12665 (20%)]\tLoss: 0.001455\n",
      "Train Epoch: 11 [3200/12665 (25%)]\tLoss: 0.003960\n",
      "Train Epoch: 11 [3840/12665 (30%)]\tLoss: 0.000949\n",
      "Train Epoch: 11 [4480/12665 (35%)]\tLoss: 0.001099\n",
      "Train Epoch: 11 [5120/12665 (40%)]\tLoss: 0.000741\n",
      "Train Epoch: 11 [5760/12665 (45%)]\tLoss: 0.001082\n",
      "Train Epoch: 11 [6400/12665 (51%)]\tLoss: 0.002808\n",
      "Train Epoch: 11 [7040/12665 (56%)]\tLoss: 0.001953\n",
      "Train Epoch: 11 [7680/12665 (61%)]\tLoss: 0.000471\n",
      "Train Epoch: 11 [8320/12665 (66%)]\tLoss: 0.004492\n",
      "Train Epoch: 11 [8960/12665 (71%)]\tLoss: 0.004723\n",
      "Train Epoch: 11 [9600/12665 (76%)]\tLoss: 0.001006\n",
      "Train Epoch: 11 [10240/12665 (81%)]\tLoss: 0.002008\n",
      "Train Epoch: 11 [10880/12665 (86%)]\tLoss: 0.000357\n",
      "Train Epoch: 11 [11520/12665 (91%)]\tLoss: 0.000673\n",
      "Train Epoch: 11 [12160/12665 (96%)]\tLoss: 0.000341\n",
      "\n",
      "Test set: Avg. loss: 0.0022, Accuracy: 2114/2115 (99%)\n",
      "\n",
      "Train Epoch: 12 [0/12665 (0%)]\tLoss: 0.003189\n",
      "Train Epoch: 12 [640/12665 (5%)]\tLoss: 0.002852\n",
      "Train Epoch: 12 [1280/12665 (10%)]\tLoss: 0.023967\n",
      "Train Epoch: 12 [1920/12665 (15%)]\tLoss: 0.000802\n",
      "Train Epoch: 12 [2560/12665 (20%)]\tLoss: 0.001390\n",
      "Train Epoch: 12 [3200/12665 (25%)]\tLoss: 0.007477\n",
      "Train Epoch: 12 [3840/12665 (30%)]\tLoss: 0.000412\n",
      "Train Epoch: 12 [4480/12665 (35%)]\tLoss: 0.002116\n",
      "Train Epoch: 12 [5120/12665 (40%)]\tLoss: 0.003146\n",
      "Train Epoch: 12 [5760/12665 (45%)]\tLoss: 0.000306\n",
      "Train Epoch: 12 [6400/12665 (51%)]\tLoss: 0.001146\n",
      "Train Epoch: 12 [7040/12665 (56%)]\tLoss: 0.001676\n",
      "Train Epoch: 12 [7680/12665 (61%)]\tLoss: 0.004194\n",
      "Train Epoch: 12 [8320/12665 (66%)]\tLoss: 0.000299\n",
      "Train Epoch: 12 [8960/12665 (71%)]\tLoss: 0.001822\n",
      "Train Epoch: 12 [9600/12665 (76%)]\tLoss: 0.000141\n",
      "Train Epoch: 12 [10240/12665 (81%)]\tLoss: 0.013127\n",
      "Train Epoch: 12 [10880/12665 (86%)]\tLoss: 0.000111\n",
      "Train Epoch: 12 [11520/12665 (91%)]\tLoss: 0.000592\n",
      "Train Epoch: 12 [12160/12665 (96%)]\tLoss: 0.000249\n",
      "\n",
      "Test set: Avg. loss: 0.0014, Accuracy: 2114/2115 (99%)\n",
      "\n",
      "Train Epoch: 13 [0/12665 (0%)]\tLoss: 0.000842\n",
      "Train Epoch: 13 [640/12665 (5%)]\tLoss: 0.002486\n",
      "Train Epoch: 13 [1280/12665 (10%)]\tLoss: 0.000256\n",
      "Train Epoch: 13 [1920/12665 (15%)]\tLoss: 0.000119\n",
      "Train Epoch: 13 [2560/12665 (20%)]\tLoss: 0.001173\n",
      "Train Epoch: 13 [3200/12665 (25%)]\tLoss: 0.000632\n",
      "Train Epoch: 13 [3840/12665 (30%)]\tLoss: 0.000536\n",
      "Train Epoch: 13 [4480/12665 (35%)]\tLoss: 0.000868\n",
      "Train Epoch: 13 [5120/12665 (40%)]\tLoss: 0.000585\n",
      "Train Epoch: 13 [5760/12665 (45%)]\tLoss: 0.000131\n",
      "Train Epoch: 13 [6400/12665 (51%)]\tLoss: 0.002656\n",
      "Train Epoch: 13 [7040/12665 (56%)]\tLoss: 0.002632\n",
      "Train Epoch: 13 [7680/12665 (61%)]\tLoss: 0.002437\n",
      "Train Epoch: 13 [8320/12665 (66%)]\tLoss: 0.012163\n",
      "Train Epoch: 13 [8960/12665 (71%)]\tLoss: 0.003587\n",
      "Train Epoch: 13 [9600/12665 (76%)]\tLoss: 0.000200\n",
      "Train Epoch: 13 [10240/12665 (81%)]\tLoss: 0.000606\n",
      "Train Epoch: 13 [10880/12665 (86%)]\tLoss: 0.000221\n",
      "Train Epoch: 13 [11520/12665 (91%)]\tLoss: 0.000645\n",
      "Train Epoch: 13 [12160/12665 (96%)]\tLoss: 0.009624\n",
      "\n",
      "Test set: Avg. loss: 0.0014, Accuracy: 2114/2115 (99%)\n",
      "\n",
      "Train Epoch: 14 [0/12665 (0%)]\tLoss: 0.002648\n",
      "Train Epoch: 14 [640/12665 (5%)]\tLoss: 0.001121\n",
      "Train Epoch: 14 [1280/12665 (10%)]\tLoss: 0.005980\n",
      "Train Epoch: 14 [1920/12665 (15%)]\tLoss: 0.000712\n",
      "Train Epoch: 14 [2560/12665 (20%)]\tLoss: 0.000534\n",
      "Train Epoch: 14 [3200/12665 (25%)]\tLoss: 0.000398\n",
      "Train Epoch: 14 [3840/12665 (30%)]\tLoss: 0.003683\n",
      "Train Epoch: 14 [4480/12665 (35%)]\tLoss: 0.008781\n",
      "Train Epoch: 14 [5120/12665 (40%)]\tLoss: 0.013213\n",
      "Train Epoch: 14 [5760/12665 (45%)]\tLoss: 0.008594\n",
      "Train Epoch: 14 [6400/12665 (51%)]\tLoss: 0.001161\n",
      "Train Epoch: 14 [7040/12665 (56%)]\tLoss: 0.000404\n",
      "Train Epoch: 14 [7680/12665 (61%)]\tLoss: 0.001344\n",
      "Train Epoch: 14 [8320/12665 (66%)]\tLoss: 0.000584\n",
      "Train Epoch: 14 [8960/12665 (71%)]\tLoss: 0.011995\n",
      "Train Epoch: 14 [9600/12665 (76%)]\tLoss: 0.001350\n",
      "Train Epoch: 14 [10240/12665 (81%)]\tLoss: 0.000278\n",
      "Train Epoch: 14 [10880/12665 (86%)]\tLoss: 0.000393\n",
      "Train Epoch: 14 [11520/12665 (91%)]\tLoss: 0.023321\n",
      "Train Epoch: 14 [12160/12665 (96%)]\tLoss: 0.000243\n",
      "\n",
      "Test set: Avg. loss: 0.0016, Accuracy: 2114/2115 (99%)\n",
      "\n",
      "Train Epoch: 15 [0/12665 (0%)]\tLoss: 0.007597\n",
      "Train Epoch: 15 [640/12665 (5%)]\tLoss: 0.002387\n",
      "Train Epoch: 15 [1280/12665 (10%)]\tLoss: 0.003129\n",
      "Train Epoch: 15 [1920/12665 (15%)]\tLoss: 0.010393\n",
      "Train Epoch: 15 [2560/12665 (20%)]\tLoss: 0.001268\n",
      "Train Epoch: 15 [3200/12665 (25%)]\tLoss: 0.000170\n",
      "Train Epoch: 15 [3840/12665 (30%)]\tLoss: 0.000180\n",
      "Train Epoch: 15 [4480/12665 (35%)]\tLoss: 0.009252\n",
      "Train Epoch: 15 [5120/12665 (40%)]\tLoss: 0.000088\n",
      "Train Epoch: 15 [5760/12665 (45%)]\tLoss: 0.000106\n",
      "Train Epoch: 15 [6400/12665 (51%)]\tLoss: 0.000379\n",
      "Train Epoch: 15 [7040/12665 (56%)]\tLoss: 0.003548\n",
      "Train Epoch: 15 [7680/12665 (61%)]\tLoss: 0.001013\n",
      "Train Epoch: 15 [8320/12665 (66%)]\tLoss: 0.000960\n",
      "Train Epoch: 15 [8960/12665 (71%)]\tLoss: 0.002373\n",
      "Train Epoch: 15 [9600/12665 (76%)]\tLoss: 0.001080\n",
      "Train Epoch: 15 [10240/12665 (81%)]\tLoss: 0.005990\n",
      "Train Epoch: 15 [10880/12665 (86%)]\tLoss: 0.001858\n",
      "Train Epoch: 15 [11520/12665 (91%)]\tLoss: 0.000190\n",
      "Train Epoch: 15 [12160/12665 (96%)]\tLoss: 0.000807\n",
      "\n",
      "Test set: Avg. loss: 0.0018, Accuracy: 2114/2115 (99%)\n",
      "\n",
      "Train Epoch: 16 [0/12665 (0%)]\tLoss: 0.000280\n",
      "Train Epoch: 16 [640/12665 (5%)]\tLoss: 0.003237\n",
      "Train Epoch: 16 [1280/12665 (10%)]\tLoss: 0.002607\n",
      "Train Epoch: 16 [1920/12665 (15%)]\tLoss: 0.000231\n",
      "Train Epoch: 16 [2560/12665 (20%)]\tLoss: 0.000194\n",
      "Train Epoch: 16 [3200/12665 (25%)]\tLoss: 0.001800\n",
      "Train Epoch: 16 [3840/12665 (30%)]\tLoss: 0.006913\n",
      "Train Epoch: 16 [4480/12665 (35%)]\tLoss: 0.000511\n",
      "Train Epoch: 16 [5120/12665 (40%)]\tLoss: 0.000514\n",
      "Train Epoch: 16 [5760/12665 (45%)]\tLoss: 0.000809\n",
      "Train Epoch: 16 [6400/12665 (51%)]\tLoss: 0.007030\n",
      "Train Epoch: 16 [7040/12665 (56%)]\tLoss: 0.001247\n",
      "Train Epoch: 16 [7680/12665 (61%)]\tLoss: 0.002059\n",
      "Train Epoch: 16 [8320/12665 (66%)]\tLoss: 0.000341\n",
      "Train Epoch: 16 [8960/12665 (71%)]\tLoss: 0.000057\n",
      "Train Epoch: 16 [9600/12665 (76%)]\tLoss: 0.001252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 16 [10240/12665 (81%)]\tLoss: 0.002045\n",
      "Train Epoch: 16 [10880/12665 (86%)]\tLoss: 0.006406\n",
      "Train Epoch: 16 [11520/12665 (91%)]\tLoss: 0.000179\n",
      "Train Epoch: 16 [12160/12665 (96%)]\tLoss: 0.001717\n",
      "\n",
      "Test set: Avg. loss: 0.0017, Accuracy: 2114/2115 (99%)\n",
      "\n",
      "Train Epoch: 17 [0/12665 (0%)]\tLoss: 0.004440\n",
      "Train Epoch: 17 [640/12665 (5%)]\tLoss: 0.025175\n",
      "Train Epoch: 17 [1280/12665 (10%)]\tLoss: 0.000206\n",
      "Train Epoch: 17 [1920/12665 (15%)]\tLoss: 0.000324\n",
      "Train Epoch: 17 [2560/12665 (20%)]\tLoss: 0.000282\n",
      "Train Epoch: 17 [3200/12665 (25%)]\tLoss: 0.001244\n",
      "Train Epoch: 17 [3840/12665 (30%)]\tLoss: 0.008195\n",
      "Train Epoch: 17 [4480/12665 (35%)]\tLoss: 0.004777\n",
      "Train Epoch: 17 [5120/12665 (40%)]\tLoss: 0.001062\n",
      "Train Epoch: 17 [5760/12665 (45%)]\tLoss: 0.000278\n",
      "Train Epoch: 17 [6400/12665 (51%)]\tLoss: 0.000533\n",
      "Train Epoch: 17 [7040/12665 (56%)]\tLoss: 0.009032\n",
      "Train Epoch: 17 [7680/12665 (61%)]\tLoss: 0.002060\n",
      "Train Epoch: 17 [8320/12665 (66%)]\tLoss: 0.001964\n",
      "Train Epoch: 17 [8960/12665 (71%)]\tLoss: 0.000113\n",
      "Train Epoch: 17 [9600/12665 (76%)]\tLoss: 0.000208\n",
      "Train Epoch: 17 [10240/12665 (81%)]\tLoss: 0.000873\n",
      "Train Epoch: 17 [10880/12665 (86%)]\tLoss: 0.002405\n",
      "Train Epoch: 17 [11520/12665 (91%)]\tLoss: 0.000347\n",
      "Train Epoch: 17 [12160/12665 (96%)]\tLoss: 0.000068\n",
      "\n",
      "Test set: Avg. loss: 0.0016, Accuracy: 2114/2115 (99%)\n",
      "\n",
      "Train Epoch: 18 [0/12665 (0%)]\tLoss: 0.000185\n",
      "Train Epoch: 18 [640/12665 (5%)]\tLoss: 0.003361\n",
      "Train Epoch: 18 [1280/12665 (10%)]\tLoss: 0.002803\n",
      "Train Epoch: 18 [1920/12665 (15%)]\tLoss: 0.002900\n",
      "Train Epoch: 18 [2560/12665 (20%)]\tLoss: 0.000961\n",
      "Train Epoch: 18 [3200/12665 (25%)]\tLoss: 0.004994\n",
      "Train Epoch: 18 [3840/12665 (30%)]\tLoss: 0.000667\n",
      "Train Epoch: 18 [4480/12665 (35%)]\tLoss: 0.000166\n",
      "Train Epoch: 18 [5120/12665 (40%)]\tLoss: 0.000905\n",
      "Train Epoch: 18 [5760/12665 (45%)]\tLoss: 0.000155\n",
      "Train Epoch: 18 [6400/12665 (51%)]\tLoss: 0.000683\n",
      "Train Epoch: 18 [7040/12665 (56%)]\tLoss: 0.000474\n",
      "Train Epoch: 18 [7680/12665 (61%)]\tLoss: 0.001811\n",
      "Train Epoch: 18 [8320/12665 (66%)]\tLoss: 0.001602\n",
      "Train Epoch: 18 [8960/12665 (71%)]\tLoss: 0.012130\n",
      "Train Epoch: 18 [9600/12665 (76%)]\tLoss: 0.000779\n",
      "Train Epoch: 18 [10240/12665 (81%)]\tLoss: 0.000081\n",
      "Train Epoch: 18 [10880/12665 (86%)]\tLoss: 0.000169\n",
      "Train Epoch: 18 [11520/12665 (91%)]\tLoss: 0.009742\n",
      "Train Epoch: 18 [12160/12665 (96%)]\tLoss: 0.000123\n",
      "\n",
      "Test set: Avg. loss: 0.0017, Accuracy: 2114/2115 (99%)\n",
      "\n",
      "Train Epoch: 19 [0/12665 (0%)]\tLoss: 0.000024\n",
      "Train Epoch: 19 [640/12665 (5%)]\tLoss: 0.003260\n",
      "Train Epoch: 19 [1280/12665 (10%)]\tLoss: 0.000373\n",
      "Train Epoch: 19 [1920/12665 (15%)]\tLoss: 0.000339\n",
      "Train Epoch: 19 [2560/12665 (20%)]\tLoss: 0.001392\n",
      "Train Epoch: 19 [3200/12665 (25%)]\tLoss: 0.018030\n",
      "Train Epoch: 19 [3840/12665 (30%)]\tLoss: 0.000841\n",
      "Train Epoch: 19 [4480/12665 (35%)]\tLoss: 0.000926\n",
      "Train Epoch: 19 [5120/12665 (40%)]\tLoss: 0.001118\n",
      "Train Epoch: 19 [5760/12665 (45%)]\tLoss: 0.000484\n",
      "Train Epoch: 19 [6400/12665 (51%)]\tLoss: 0.000863\n",
      "Train Epoch: 19 [7040/12665 (56%)]\tLoss: 0.001534\n",
      "Train Epoch: 19 [7680/12665 (61%)]\tLoss: 0.004860\n",
      "Train Epoch: 19 [8320/12665 (66%)]\tLoss: 0.000581\n",
      "Train Epoch: 19 [8960/12665 (71%)]\tLoss: 0.001024\n",
      "Train Epoch: 19 [9600/12665 (76%)]\tLoss: 0.000123\n",
      "Train Epoch: 19 [10240/12665 (81%)]\tLoss: 0.010713\n",
      "Train Epoch: 19 [10880/12665 (86%)]\tLoss: 0.005864\n",
      "Train Epoch: 19 [11520/12665 (91%)]\tLoss: 0.000582\n",
      "Train Epoch: 19 [12160/12665 (96%)]\tLoss: 0.063896\n",
      "\n",
      "Test set: Avg. loss: 0.0017, Accuracy: 2114/2115 (99%)\n",
      "\n",
      "Train Epoch: 20 [0/12665 (0%)]\tLoss: 0.000327\n",
      "Train Epoch: 20 [640/12665 (5%)]\tLoss: 0.000756\n",
      "Train Epoch: 20 [1280/12665 (10%)]\tLoss: 0.000333\n",
      "Train Epoch: 20 [1920/12665 (15%)]\tLoss: 0.002320\n",
      "Train Epoch: 20 [2560/12665 (20%)]\tLoss: 0.000513\n",
      "Train Epoch: 20 [3200/12665 (25%)]\tLoss: 0.000046\n",
      "Train Epoch: 20 [3840/12665 (30%)]\tLoss: 0.000625\n",
      "Train Epoch: 20 [4480/12665 (35%)]\tLoss: 0.002269\n",
      "Train Epoch: 20 [5120/12665 (40%)]\tLoss: 0.000278\n",
      "Train Epoch: 20 [5760/12665 (45%)]\tLoss: 0.000362\n",
      "Train Epoch: 20 [6400/12665 (51%)]\tLoss: 0.000309\n",
      "Train Epoch: 20 [7040/12665 (56%)]\tLoss: 0.001343\n",
      "Train Epoch: 20 [7680/12665 (61%)]\tLoss: 0.002668\n",
      "Train Epoch: 20 [8320/12665 (66%)]\tLoss: 0.000420\n",
      "Train Epoch: 20 [8960/12665 (71%)]\tLoss: 0.000068\n",
      "Train Epoch: 20 [9600/12665 (76%)]\tLoss: 0.001582\n",
      "Train Epoch: 20 [10240/12665 (81%)]\tLoss: 0.019289\n",
      "Train Epoch: 20 [10880/12665 (86%)]\tLoss: 0.001670\n",
      "Train Epoch: 20 [11520/12665 (91%)]\tLoss: 0.005630\n",
      "Train Epoch: 20 [12160/12665 (96%)]\tLoss: 0.000251\n",
      "\n",
      "Test set: Avg. loss: 0.0015, Accuracy: 2114/2115 (99%)\n",
      "\n",
      "Train Epoch: 21 [0/12665 (0%)]\tLoss: 0.002126\n",
      "Train Epoch: 21 [640/12665 (5%)]\tLoss: 0.000147\n",
      "Train Epoch: 21 [1280/12665 (10%)]\tLoss: 0.001283\n",
      "Train Epoch: 21 [1920/12665 (15%)]\tLoss: 0.000518\n",
      "Train Epoch: 21 [2560/12665 (20%)]\tLoss: 0.001250\n",
      "Train Epoch: 21 [3200/12665 (25%)]\tLoss: 0.000389\n",
      "Train Epoch: 21 [3840/12665 (30%)]\tLoss: 0.000231\n",
      "Train Epoch: 21 [4480/12665 (35%)]\tLoss: 0.001083\n",
      "Train Epoch: 21 [5120/12665 (40%)]\tLoss: 0.004885\n",
      "Train Epoch: 21 [5760/12665 (45%)]\tLoss: 0.002804\n",
      "Train Epoch: 21 [6400/12665 (51%)]\tLoss: 0.000397\n",
      "Train Epoch: 21 [7040/12665 (56%)]\tLoss: 0.015821\n",
      "Train Epoch: 21 [7680/12665 (61%)]\tLoss: 0.006540\n",
      "Train Epoch: 21 [8320/12665 (66%)]\tLoss: 0.001276\n",
      "Train Epoch: 21 [8960/12665 (71%)]\tLoss: 0.050533\n",
      "Train Epoch: 21 [9600/12665 (76%)]\tLoss: 0.006413\n",
      "Train Epoch: 21 [10240/12665 (81%)]\tLoss: 0.002862\n",
      "Train Epoch: 21 [10880/12665 (86%)]\tLoss: 0.001827\n",
      "Train Epoch: 21 [11520/12665 (91%)]\tLoss: 0.000809\n",
      "Train Epoch: 21 [12160/12665 (96%)]\tLoss: 0.003464\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 2114/2115 (99%)\n",
      "\n",
      "Train Epoch: 22 [0/12665 (0%)]\tLoss: 0.002192\n",
      "Train Epoch: 22 [640/12665 (5%)]\tLoss: 0.000334\n",
      "Train Epoch: 22 [1280/12665 (10%)]\tLoss: 0.000177\n",
      "Train Epoch: 22 [1920/12665 (15%)]\tLoss: 0.000449\n",
      "Train Epoch: 22 [2560/12665 (20%)]\tLoss: 0.007607\n",
      "Train Epoch: 22 [3200/12665 (25%)]\tLoss: 0.001261\n",
      "Train Epoch: 22 [3840/12665 (30%)]\tLoss: 0.003170\n",
      "Train Epoch: 22 [4480/12665 (35%)]\tLoss: 0.000345\n",
      "Train Epoch: 22 [5120/12665 (40%)]\tLoss: 0.001928\n",
      "Train Epoch: 22 [5760/12665 (45%)]\tLoss: 0.001459\n",
      "Train Epoch: 22 [6400/12665 (51%)]\tLoss: 0.000157\n",
      "Train Epoch: 22 [7040/12665 (56%)]\tLoss: 0.000950\n",
      "Train Epoch: 22 [7680/12665 (61%)]\tLoss: 0.001227\n",
      "Train Epoch: 22 [8320/12665 (66%)]\tLoss: 0.038225\n",
      "Train Epoch: 22 [8960/12665 (71%)]\tLoss: 0.005270\n",
      "Train Epoch: 22 [9600/12665 (76%)]\tLoss: 0.000101\n",
      "Train Epoch: 22 [10240/12665 (81%)]\tLoss: 0.005785\n",
      "Train Epoch: 22 [10880/12665 (86%)]\tLoss: 0.000104\n",
      "Train Epoch: 22 [11520/12665 (91%)]\tLoss: 0.000241\n",
      "Train Epoch: 22 [12160/12665 (96%)]\tLoss: 0.000193\n",
      "\n",
      "Test set: Avg. loss: 0.0020, Accuracy: 2114/2115 (99%)\n",
      "\n",
      "Train Epoch: 23 [0/12665 (0%)]\tLoss: 0.003456\n",
      "Train Epoch: 23 [640/12665 (5%)]\tLoss: 0.000334\n",
      "Train Epoch: 23 [1280/12665 (10%)]\tLoss: 0.000023\n",
      "Train Epoch: 23 [1920/12665 (15%)]\tLoss: 0.001802\n",
      "Train Epoch: 23 [2560/12665 (20%)]\tLoss: 0.000653\n",
      "Train Epoch: 23 [3200/12665 (25%)]\tLoss: 0.000248\n",
      "Train Epoch: 23 [3840/12665 (30%)]\tLoss: 0.000665\n",
      "Train Epoch: 23 [4480/12665 (35%)]\tLoss: 0.000103\n",
      "Train Epoch: 23 [5120/12665 (40%)]\tLoss: 0.000381\n",
      "Train Epoch: 23 [5760/12665 (45%)]\tLoss: 0.001457\n",
      "Train Epoch: 23 [6400/12665 (51%)]\tLoss: 0.001195\n",
      "Train Epoch: 23 [7040/12665 (56%)]\tLoss: 0.014942\n",
      "Train Epoch: 23 [7680/12665 (61%)]\tLoss: 0.000903\n",
      "Train Epoch: 23 [8320/12665 (66%)]\tLoss: 0.000295\n",
      "Train Epoch: 23 [8960/12665 (71%)]\tLoss: 0.000215\n",
      "Train Epoch: 23 [9600/12665 (76%)]\tLoss: 0.000090\n",
      "Train Epoch: 23 [10240/12665 (81%)]\tLoss: 0.000197\n",
      "Train Epoch: 23 [10880/12665 (86%)]\tLoss: 0.005685\n",
      "Train Epoch: 23 [11520/12665 (91%)]\tLoss: 0.000054\n",
      "Train Epoch: 23 [12160/12665 (96%)]\tLoss: 0.000335\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 2114/2115 (99%)\n",
      "\n",
      "Train Epoch: 24 [0/12665 (0%)]\tLoss: 0.000200\n",
      "Train Epoch: 24 [640/12665 (5%)]\tLoss: 0.001215\n",
      "Train Epoch: 24 [1280/12665 (10%)]\tLoss: 0.002105\n",
      "Train Epoch: 24 [1920/12665 (15%)]\tLoss: 0.000033\n",
      "Train Epoch: 24 [2560/12665 (20%)]\tLoss: 0.000232\n",
      "Train Epoch: 24 [3200/12665 (25%)]\tLoss: 0.000739\n",
      "Train Epoch: 24 [3840/12665 (30%)]\tLoss: 0.000451\n",
      "Train Epoch: 24 [4480/12665 (35%)]\tLoss: 0.000158\n",
      "Train Epoch: 24 [5120/12665 (40%)]\tLoss: 0.000035\n",
      "Train Epoch: 24 [5760/12665 (45%)]\tLoss: 0.000922\n",
      "Train Epoch: 24 [6400/12665 (51%)]\tLoss: 0.000006\n",
      "Train Epoch: 24 [7040/12665 (56%)]\tLoss: 0.001934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 24 [7680/12665 (61%)]\tLoss: 0.000312\n",
      "Train Epoch: 24 [8320/12665 (66%)]\tLoss: 0.000460\n",
      "Train Epoch: 24 [8960/12665 (71%)]\tLoss: 0.000706\n",
      "Train Epoch: 24 [9600/12665 (76%)]\tLoss: 0.000757\n",
      "Train Epoch: 24 [10240/12665 (81%)]\tLoss: 0.010789\n",
      "Train Epoch: 24 [10880/12665 (86%)]\tLoss: 0.000090\n",
      "Train Epoch: 24 [11520/12665 (91%)]\tLoss: 0.001752\n",
      "Train Epoch: 24 [12160/12665 (96%)]\tLoss: 0.000059\n",
      "\n",
      "Test set: Avg. loss: 0.0020, Accuracy: 2114/2115 (99%)\n",
      "\n",
      "Train Epoch: 25 [0/12665 (0%)]\tLoss: 0.000023\n",
      "Train Epoch: 25 [640/12665 (5%)]\tLoss: 0.001693\n",
      "Train Epoch: 25 [1280/12665 (10%)]\tLoss: 0.000091\n",
      "Train Epoch: 25 [1920/12665 (15%)]\tLoss: 0.000974\n",
      "Train Epoch: 25 [2560/12665 (20%)]\tLoss: 0.000229\n",
      "Train Epoch: 25 [3200/12665 (25%)]\tLoss: 0.000082\n",
      "Train Epoch: 25 [3840/12665 (30%)]\tLoss: 0.016137\n",
      "Train Epoch: 25 [4480/12665 (35%)]\tLoss: 0.004767\n",
      "Train Epoch: 25 [5120/12665 (40%)]\tLoss: 0.001386\n",
      "Train Epoch: 25 [5760/12665 (45%)]\tLoss: 0.006480\n",
      "Train Epoch: 25 [6400/12665 (51%)]\tLoss: 0.001589\n",
      "Train Epoch: 25 [7040/12665 (56%)]\tLoss: 0.001233\n",
      "Train Epoch: 25 [7680/12665 (61%)]\tLoss: 0.001680\n",
      "Train Epoch: 25 [8320/12665 (66%)]\tLoss: 0.000378\n",
      "Train Epoch: 25 [8960/12665 (71%)]\tLoss: 0.002849\n",
      "Train Epoch: 25 [9600/12665 (76%)]\tLoss: 0.000419\n",
      "Train Epoch: 25 [10240/12665 (81%)]\tLoss: 0.000062\n",
      "Train Epoch: 25 [10880/12665 (86%)]\tLoss: 0.000436\n",
      "Train Epoch: 25 [11520/12665 (91%)]\tLoss: 0.000069\n",
      "Train Epoch: 25 [12160/12665 (96%)]\tLoss: 0.004351\n",
      "\n",
      "Test set: Avg. loss: 0.0017, Accuracy: 2114/2115 (99%)\n",
      "\n",
      "Train Epoch: 26 [0/12665 (0%)]\tLoss: 0.000393\n",
      "Train Epoch: 26 [640/12665 (5%)]\tLoss: 0.006629\n",
      "Train Epoch: 26 [1280/12665 (10%)]\tLoss: 0.004855\n",
      "Train Epoch: 26 [1920/12665 (15%)]\tLoss: 0.001502\n",
      "Train Epoch: 26 [2560/12665 (20%)]\tLoss: 0.000053\n",
      "Train Epoch: 26 [3200/12665 (25%)]\tLoss: 0.000391\n",
      "Train Epoch: 26 [3840/12665 (30%)]\tLoss: 0.000689\n",
      "Train Epoch: 26 [4480/12665 (35%)]\tLoss: 0.117665\n",
      "Train Epoch: 26 [5120/12665 (40%)]\tLoss: 0.000050\n",
      "Train Epoch: 26 [5760/12665 (45%)]\tLoss: 0.000448\n",
      "Train Epoch: 26 [6400/12665 (51%)]\tLoss: 0.001061\n",
      "Train Epoch: 26 [7040/12665 (56%)]\tLoss: 0.000772\n",
      "Train Epoch: 26 [7680/12665 (61%)]\tLoss: 0.000311\n",
      "Train Epoch: 26 [8320/12665 (66%)]\tLoss: 0.000714\n",
      "Train Epoch: 26 [8960/12665 (71%)]\tLoss: 0.000050\n",
      "Train Epoch: 26 [9600/12665 (76%)]\tLoss: 0.000090\n",
      "Train Epoch: 26 [10240/12665 (81%)]\tLoss: 0.000325\n",
      "Train Epoch: 26 [10880/12665 (86%)]\tLoss: 0.001712\n",
      "Train Epoch: 26 [11520/12665 (91%)]\tLoss: 0.000023\n",
      "Train Epoch: 26 [12160/12665 (96%)]\tLoss: 0.000726\n",
      "\n",
      "Test set: Avg. loss: 0.0022, Accuracy: 2114/2115 (99%)\n",
      "\n",
      "Train Epoch: 27 [0/12665 (0%)]\tLoss: 0.014378\n",
      "Train Epoch: 27 [640/12665 (5%)]\tLoss: 0.000062\n",
      "Train Epoch: 27 [1280/12665 (10%)]\tLoss: 0.000102\n",
      "Train Epoch: 27 [1920/12665 (15%)]\tLoss: 0.000081\n",
      "Train Epoch: 27 [2560/12665 (20%)]\tLoss: 0.000070\n",
      "Train Epoch: 27 [3200/12665 (25%)]\tLoss: 0.008500\n",
      "Train Epoch: 27 [3840/12665 (30%)]\tLoss: 0.000010\n",
      "Train Epoch: 27 [4480/12665 (35%)]\tLoss: 0.000151\n",
      "Train Epoch: 27 [5120/12665 (40%)]\tLoss: 0.000143\n",
      "Train Epoch: 27 [5760/12665 (45%)]\tLoss: 0.000564\n",
      "Train Epoch: 27 [6400/12665 (51%)]\tLoss: 0.000272\n",
      "Train Epoch: 27 [7040/12665 (56%)]\tLoss: 0.000265\n",
      "Train Epoch: 27 [7680/12665 (61%)]\tLoss: 0.007216\n",
      "Train Epoch: 27 [8320/12665 (66%)]\tLoss: 0.000269\n",
      "Train Epoch: 27 [8960/12665 (71%)]\tLoss: 0.037327\n",
      "Train Epoch: 27 [9600/12665 (76%)]\tLoss: 0.001411\n",
      "Train Epoch: 27 [10240/12665 (81%)]\tLoss: 0.008833\n",
      "Train Epoch: 27 [10880/12665 (86%)]\tLoss: 0.017286\n",
      "Train Epoch: 27 [11520/12665 (91%)]\tLoss: 0.000258\n",
      "Train Epoch: 27 [12160/12665 (96%)]\tLoss: 0.000603\n",
      "\n",
      "Test set: Avg. loss: 0.0019, Accuracy: 2114/2115 (99%)\n",
      "\n",
      "Train Epoch: 28 [0/12665 (0%)]\tLoss: 0.001162\n",
      "Train Epoch: 28 [640/12665 (5%)]\tLoss: 0.000195\n",
      "Train Epoch: 28 [1280/12665 (10%)]\tLoss: 0.000155\n",
      "Train Epoch: 28 [1920/12665 (15%)]\tLoss: 0.001186\n",
      "Train Epoch: 28 [2560/12665 (20%)]\tLoss: 0.003506\n",
      "Train Epoch: 28 [3200/12665 (25%)]\tLoss: 0.000172\n",
      "Train Epoch: 28 [3840/12665 (30%)]\tLoss: 0.000841\n",
      "Train Epoch: 28 [4480/12665 (35%)]\tLoss: 0.000237\n",
      "Train Epoch: 28 [5120/12665 (40%)]\tLoss: 0.000085\n",
      "Train Epoch: 28 [5760/12665 (45%)]\tLoss: 0.000013\n",
      "Train Epoch: 28 [6400/12665 (51%)]\tLoss: 0.004182\n",
      "Train Epoch: 28 [7040/12665 (56%)]\tLoss: 0.000098\n",
      "Train Epoch: 28 [7680/12665 (61%)]\tLoss: 0.011403\n",
      "Train Epoch: 28 [8320/12665 (66%)]\tLoss: 0.000216\n",
      "Train Epoch: 28 [8960/12665 (71%)]\tLoss: 0.005704\n",
      "Train Epoch: 28 [9600/12665 (76%)]\tLoss: 0.000279\n",
      "Train Epoch: 28 [10240/12665 (81%)]\tLoss: 0.000350\n",
      "Train Epoch: 28 [10880/12665 (86%)]\tLoss: 0.001621\n",
      "Train Epoch: 28 [11520/12665 (91%)]\tLoss: 0.000054\n",
      "Train Epoch: 28 [12160/12665 (96%)]\tLoss: 0.007556\n",
      "\n",
      "Test set: Avg. loss: 0.0019, Accuracy: 2114/2115 (99%)\n",
      "\n",
      "Train Epoch: 29 [0/12665 (0%)]\tLoss: 0.000643\n",
      "Train Epoch: 29 [640/12665 (5%)]\tLoss: 0.000087\n",
      "Train Epoch: 29 [1280/12665 (10%)]\tLoss: 0.000172\n",
      "Train Epoch: 29 [1920/12665 (15%)]\tLoss: 0.000022\n",
      "Train Epoch: 29 [2560/12665 (20%)]\tLoss: 0.001819\n",
      "Train Epoch: 29 [3200/12665 (25%)]\tLoss: 0.000550\n",
      "Train Epoch: 29 [3840/12665 (30%)]\tLoss: 0.000137\n",
      "Train Epoch: 29 [4480/12665 (35%)]\tLoss: 0.001859\n",
      "Train Epoch: 29 [5120/12665 (40%)]\tLoss: 0.000338\n",
      "Train Epoch: 29 [5760/12665 (45%)]\tLoss: 0.001447\n",
      "Train Epoch: 29 [6400/12665 (51%)]\tLoss: 0.020965\n",
      "Train Epoch: 29 [7040/12665 (56%)]\tLoss: 0.001186\n",
      "Train Epoch: 29 [7680/12665 (61%)]\tLoss: 0.000485\n",
      "Train Epoch: 29 [8320/12665 (66%)]\tLoss: 0.000050\n",
      "Train Epoch: 29 [8960/12665 (71%)]\tLoss: 0.000095\n",
      "Train Epoch: 29 [9600/12665 (76%)]\tLoss: 0.000884\n",
      "Train Epoch: 29 [10240/12665 (81%)]\tLoss: 0.000092\n",
      "Train Epoch: 29 [10880/12665 (86%)]\tLoss: 0.000293\n",
      "Train Epoch: 29 [11520/12665 (91%)]\tLoss: 0.000485\n",
      "Train Epoch: 29 [12160/12665 (96%)]\tLoss: 0.003305\n",
      "\n",
      "Test set: Avg. loss: 0.0025, Accuracy: 2114/2115 (99%)\n",
      "\n",
      "Train Epoch: 30 [0/12665 (0%)]\tLoss: 0.000175\n",
      "Train Epoch: 30 [640/12665 (5%)]\tLoss: 0.001307\n",
      "Train Epoch: 30 [1280/12665 (10%)]\tLoss: 0.000078\n",
      "Train Epoch: 30 [1920/12665 (15%)]\tLoss: 0.003196\n",
      "Train Epoch: 30 [2560/12665 (20%)]\tLoss: 0.000024\n",
      "Train Epoch: 30 [3200/12665 (25%)]\tLoss: 0.000260\n",
      "Train Epoch: 30 [3840/12665 (30%)]\tLoss: 0.012834\n",
      "Train Epoch: 30 [4480/12665 (35%)]\tLoss: 0.001131\n",
      "Train Epoch: 30 [5120/12665 (40%)]\tLoss: 0.000119\n",
      "Train Epoch: 30 [5760/12665 (45%)]\tLoss: 0.000047\n",
      "Train Epoch: 30 [6400/12665 (51%)]\tLoss: 0.000007\n",
      "Train Epoch: 30 [7040/12665 (56%)]\tLoss: 0.000189\n",
      "Train Epoch: 30 [7680/12665 (61%)]\tLoss: 0.000252\n",
      "Train Epoch: 30 [8320/12665 (66%)]\tLoss: 0.000838\n",
      "Train Epoch: 30 [8960/12665 (71%)]\tLoss: 0.032268\n",
      "Train Epoch: 30 [9600/12665 (76%)]\tLoss: 0.000026\n",
      "Train Epoch: 30 [10240/12665 (81%)]\tLoss: 0.002282\n",
      "Train Epoch: 30 [10880/12665 (86%)]\tLoss: 0.113293\n",
      "Train Epoch: 30 [11520/12665 (91%)]\tLoss: 0.020421\n",
      "Train Epoch: 30 [12160/12665 (96%)]\tLoss: 0.006504\n",
      "\n",
      "Test set: Avg. loss: 0.0013, Accuracy: 2114/2115 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net_c2 = Net_c2().cuda()\n",
    "n_epochs = 30\n",
    "optimizer = optim.SGD(net_c2.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader2.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "test(net_c2, test_loader2)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(net_c2, train_loader2, epoch)\n",
    "    test(net_c2, test_loader2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naiqi/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.00222785, 0.00222785, 0.        , 0.00222785, 0.00222785,\n",
       "        0.00222785, 0.00222785, 0.00668356, 0.00445571, 0.00222785,\n",
       "        0.00891141, 0.00222785, 0.        , 0.00891141, 0.01336712,\n",
       "        0.01336712, 0.00445571, 0.00668356, 0.02227853, 0.00668356,\n",
       "        0.01559497, 0.01782282, 0.01336712, 0.01782282, 0.03118994,\n",
       "        0.02450638, 0.02673424, 0.05124062, 0.0334178 , 0.05792418,\n",
       "        0.05792418, 0.07351915, 0.075747  , 0.08020271, 0.08465841,\n",
       "        0.09579768, 0.11584836, 0.13367118, 0.12475977, 0.12921548,\n",
       "        0.13589903, 0.14926615, 0.16263327, 0.18268395, 0.14481045,\n",
       "        0.17377254, 0.19382321, 0.13367118, 0.17154468, 0.14035474,\n",
       "        0.12030406, 0.14481045, 0.12030406, 0.09802553, 0.10248124,\n",
       "        0.11139265, 0.09356983, 0.08911412, 0.06906344, 0.05792418,\n",
       "        0.04901277, 0.05569633, 0.04678491, 0.03564565, 0.03118994,\n",
       "        0.05124062, 0.02896209, 0.02005068, 0.02005068, 0.02896209,\n",
       "        0.00668356, 0.02673424, 0.01782282, 0.00891141, 0.00891141,\n",
       "        0.00222785, 0.00222785, 0.00222785, 0.01336712, 0.00222785,\n",
       "        0.00222785, 0.00668356, 0.        , 0.00222785, 0.        ,\n",
       "        0.00222785, 0.00445571, 0.00445571, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00222785, 0.        , 0.00222785]),\n",
       " array([ 4.1256174 ,  4.35004873,  4.57448005,  4.79891137,  5.02334269,\n",
       "         5.24777402,  5.47220534,  5.69663666,  5.92106799,  6.14549931,\n",
       "         6.36993063,  6.59436196,  6.81879328,  7.0432246 ,  7.26765592,\n",
       "         7.49208725,  7.71651857,  7.94094989,  8.16538122,  8.38981254,\n",
       "         8.61424386,  8.83867518,  9.06310651,  9.28753783,  9.51196915,\n",
       "         9.73640048,  9.9608318 , 10.18526312, 10.40969445, 10.63412577,\n",
       "        10.85855709, 11.08298841, 11.30741974, 11.53185106, 11.75628238,\n",
       "        11.98071371, 12.20514503, 12.42957635, 12.65400768, 12.878439  ,\n",
       "        13.10287032, 13.32730164, 13.55173297, 13.77616429, 14.00059561,\n",
       "        14.22502694, 14.44945826, 14.67388958, 14.8983209 , 15.12275223,\n",
       "        15.34718355, 15.57161487, 15.7960462 , 16.02047752, 16.24490884,\n",
       "        16.46934017, 16.69377149, 16.91820281, 17.14263413, 17.36706546,\n",
       "        17.59149678, 17.8159281 , 18.04035943, 18.26479075, 18.48922207,\n",
       "        18.71365339, 18.93808472, 19.16251604, 19.38694736, 19.61137869,\n",
       "        19.83581001, 20.06024133, 20.28467266, 20.50910398, 20.7335353 ,\n",
       "        20.95796662, 21.18239795, 21.40682927, 21.63126059, 21.85569192,\n",
       "        22.08012324, 22.30455456, 22.52898588, 22.75341721, 22.97784853,\n",
       "        23.20227985, 23.42671118, 23.6511425 , 23.87557382, 24.10000515,\n",
       "        24.32443647, 24.54886779, 24.77329911, 24.99773044, 25.22216176,\n",
       "        25.44659308, 25.67102441, 25.89545573, 26.11988705, 26.34431838,\n",
       "        26.5687497 ]),\n",
       " <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = net_c2(noise.cuda())\n",
    "ll = net_c2.last\n",
    "print(ll.shape)\n",
    "ll = ll.cpu().detach().numpy() \n",
    "lid_net1_c2 = LID(ll, ll, k=50)\n",
    "plt.hist(lid_net1_c2, bins=100, normed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.337823982033932\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(lid_net1_c2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
